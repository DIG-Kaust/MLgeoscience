{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29d9d3ea",
   "metadata": {},
   "source": [
    "# Implicit neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8090e46c",
   "metadata": {},
   "source": [
    "This notebook is exctracted from the tutorial [Deep Implicit Layers](http://implicit-layers-tutorial.org) chapters 1 and 4. It covers the basic principles of implicit layers and a Deep Equilibrium Model (DEQ) architecture. The first part covers the implementation of implicit layers and how to link it with Pytorch's backpropagation. The second part covers DEQs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fae349b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.autograd as autograd\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.autograd import gradcheck\n",
    "from epoch import epoch, epoch_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331ff4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812f3f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import set_seed\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = 'cuda:0'\n",
    "print(torch.cuda.get_device_name(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2009f820",
   "metadata": {},
   "source": [
    "### Fixed point iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c144174",
   "metadata": {},
   "source": [
    "As a first example we consider the following fixed point iteration:\n",
    "\n",
    "$$\n",
    "    z_{k+1} = \\tanh{(Wz_k + b)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45611dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhFixedPointLayer(nn.Module):\n",
    "    def __init__(self, out_features, tol = 1e-4, max_iter=50):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(out_features, out_features, bias=False)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # initialize output z to be zero\n",
    "        z = torch.zeros_like(x)\n",
    "        self.iterations = 0\n",
    "\n",
    "        # iterate until convergence\n",
    "        while self.iterations < self.max_iter:\n",
    "            z_next = torch.tanh(self.linear(z) + x)\n",
    "            self.err = torch.norm(z - z_next)\n",
    "            z = z_next\n",
    "            self.iterations += 1\n",
    "            if self.err < self.tol:\n",
    "                break\n",
    "\n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f3b11a",
   "metadata": {},
   "source": [
    "Let's run the fixed point iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2b884a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TanhFixedPointLayer(50)\n",
    "X = torch.randn(10,50)\n",
    "Z = layer(X)\n",
    "print(f\"Terminated after {layer.iterations} iterations with error {layer.err}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f4722",
   "metadata": {},
   "source": [
    "Let's train this simple network on the MNIST dataset to see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47014f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = datasets.MNIST(\".\", train=True, download=False, transform=transforms.ToTensor())\n",
    "mnist_test = datasets.MNIST(\".\", train=False, download=False, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(mnist_train, batch_size = 100, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test, batch_size = 100, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ccba243",
   "metadata": {},
   "source": [
    "Below we add a first and final layer to make sure the dimensions of the input and output are matched. Note that the fixed point layer has a width of 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bedc029",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = nn.Sequential(nn.Flatten(),\n",
    "                      nn.Linear(784, 100),\n",
    "                      TanhFixedPointLayer(100, max_iter=200),\n",
    "                      nn.Linear(100, 10)\n",
    "                      ).to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669e6496",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    if i == 5:\n",
    "        opt.param_groups[0][\"lr\"] = 1e-2\n",
    "\n",
    "    train_err, train_loss, train_fpiter = epoch(train_loader, model, opt, lambda x : x[2].iterations)\n",
    "    test_err, test_loss, test_fpiter = epoch(test_loader, model, monitor = lambda x : x[2].iterations)\n",
    "    print(f\"Train Error: {train_err:.4f}, Loss: {train_loss:.4f}, FP Iters: {train_fpiter:.2f} | \" +\n",
    "          f\"Test Error: {test_err:.4f}, Loss: {test_loss:.4f}, FP Iters: {test_fpiter:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc230cc",
   "metadata": {},
   "source": [
    "Although this simple fixed point iteration works there exist faster root finding methods, like Newton's iteration:\n",
    "\n",
    "$$\n",
    "    z_{k+1} = z_k - \\left(\\frac{\\partial g}{\\partial z}\\right)^{-1}g(z_k).\n",
    "$$\n",
    "\n",
    "The Jacobian is given by\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial g}{\\partial z} = I - \\text{diag}\\left(\\tanh^{'}(Wz+x)\\right)\n",
    "$$\n",
    "\n",
    "We implement this method below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c31882",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhNewtonLayer(nn.Module):\n",
    "    def __init__(self, out_features, tol = 1e-4, max_iter=50):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(out_features, out_features, bias=False)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # initialize output z to be zero\n",
    "        z = torch.tanh(x)\n",
    "        self.iterations = 0\n",
    "    \n",
    "        # iterate until convergence\n",
    "        while self.iterations < self.max_iter:\n",
    "            z_linear = self.linear(z) + x\n",
    "            g = z - torch.tanh(z_linear)\n",
    "            self.err = torch.norm(g)\n",
    "            if self.err < self.tol:\n",
    "                break\n",
    "\n",
    "            # newton step\n",
    "            J = torch.eye(z.shape[1])[None,:,:].to(device) - (1 / torch.cosh(z_linear)**2)[:,:,None]*self.linear.weight[None,:,:]\n",
    "            z = z - torch.solve(g[:,:,None], J)[0][:,:,0]\n",
    "            self.iterations += 1\n",
    "\n",
    "        g = z - torch.tanh(self.linear(z) + x)\n",
    "        z[torch.norm(g,dim=1) > self.tol,:] = 0\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef8a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TanhNewtonLayer(50)\n",
    "X = torch.randn(10,50)\n",
    "Z = layer(X)\n",
    "print(f\"Terminated after {layer.iterations} iterations with error {layer.err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd87bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = nn.Sequential(nn.Flatten(),\n",
    "                      nn.Linear(784, 100),\n",
    "                      TanhNewtonLayer(100, max_iter=40),\n",
    "                      nn.Linear(100, 10)\n",
    "                      ).to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for i in range(8):\n",
    "    if i == 5:\n",
    "        opt.param_groups[0][\"lr\"] = 1e-2\n",
    "\n",
    "    train_err, train_loss, train_fpiter = epoch(train_loader, model, opt, lambda x : x[2].iterations)\n",
    "    test_err, test_loss, test_fpiter = epoch(test_loader, model, monitor = lambda x : x[2].iterations)\n",
    "    print(f\"Train Error: {train_err:.4f}, Loss: {train_loss:.4f}, Newton Iters: {train_fpiter:.2f} | \" +\n",
    "          f\"Test Error: {test_err:.4f}, Loss: {test_loss:.4f}, Newton Iters: {test_fpiter:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b25188b",
   "metadata": {},
   "source": [
    "### Implicit layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445f16bd",
   "metadata": {},
   "source": [
    "Let's now look at how to implement this layer implicitly. We're solving for the root of the equation\n",
    "\n",
    "$$\n",
    "    g(x, z) := z - \\tanh{(Wz + b)} = 0,\n",
    "$$\n",
    "\n",
    "and denote by $z_{\\star}(x)$ the $z$ that satisfies\n",
    "\n",
    "$$\n",
    "    g(x, z_{\\star}(x)) = 0.\n",
    "$$\n",
    "\n",
    "Differentiating this equation on both sides with respect to $x$ yields\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial g(x, z_{\\star}(x))}{\\partial x} = \\frac{\\partial g(x, z_{\\star})}{\\partial x} + \\frac{\\partial g(x, z_{\\star})}{\\partial z_{\\star}}\\frac{\\partial z_{\\star}}{\\partial x} = 0\n",
    "$$\n",
    "\n",
    "Solving for $\\frac{\\partial z_{\\star}}{\\partial x}$ yields\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial z_{\\star}}{\\partial x} = -\\left(\\frac{\\partial g(x, z_{\\star})}{\\partial z_{\\star}}\\right)^{-1}\\frac{\\partial g(x, z_{\\star})}{\\partial x}\n",
    "$$\n",
    "\n",
    "When updating the model we consider some loss function with respect to which we want to differentiate. Denoting the loss function by $l$, we actually compute\n",
    "\n",
    "$$\n",
    "    \\frac{\\partial l}{\\partial x} = \\frac{\\partial l}{\\partial z_{\\star}}\\frac{\\partial z_{\\star}}{\\partial x} = -\\frac{\\partial l}{\\partial z_{\\star}}\\left(\\frac{\\partial g(x, z_{\\star})}{\\partial z_{\\star}}\\right)^{-1}\\frac{\\partial g(x, z_{\\star})}{\\partial x}\n",
    "$$\n",
    "\n",
    "Automatic differentiation typically works on the gradient instead of the Jacobian, and for a scalar valued function the gradient is just the transpose of the Jacobian, i.e.\n",
    "\n",
    "$$\n",
    "    \\nabla_{z_{\\star}} l = \\left(\\frac{\\partial l}{\\partial z_{\\star}}\\right)^T.\n",
    "$$\n",
    "\n",
    "Hence\n",
    "\n",
    "$$\n",
    "    \\nabla_x l = -\\left(\\frac{\\partial g(x, z_{\\star})}{\\partial x}\\right)^T\\left(\\frac{\\partial g(x, z_{\\star})}{\\partial z_{\\star}}\\right)^{-T}\\nabla_{z_{\\star}}l\n",
    "$$\n",
    "\n",
    "It's important to realize that the function that is implemented is not standard. The function first finds a fixed point that is not needed within the automatic differentiation, and then a backward pass that acts accordingly. To implement this function we take the following steps:\n",
    "\n",
    "1. Solve $(g(x, z_{\\star})) = 0$. This part of the forward is *outside* of the automatic differentiation.\n",
    "\n",
    "2. Activate the automatic differentiation using the following assignment:\n",
    "\n",
    "$$\n",
    "    z := z_{\\star} - g(x, z_{\\star}).\n",
    "$$\n",
    "\n",
    "3. Multiply by $\\left(\\frac{\\partial g}{\\partial z_{\\star}}\\right)^{-T}$ so the backward pass correctly implements the gradient according to the implicit function theorem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02f82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanhNewtonImplicitLayer(nn.Module):\n",
    "    def __init__(self, out_features, tol = 1e-4, max_iter=50):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(out_features, out_features, bias=False)\n",
    "        self.tol = tol\n",
    "        self.max_iter = max_iter\n",
    "  \n",
    "    def forward(self, x):\n",
    "        # Run Newton's method outside of the autograd framework\n",
    "        with torch.no_grad():\n",
    "            z = torch.tanh(x)\n",
    "            self.iterations = 0\n",
    "            while self.iterations < self.max_iter:\n",
    "                z_linear = self.linear(z) + x\n",
    "                g = z - torch.tanh(z_linear)\n",
    "                self.err = torch.norm(g)\n",
    "                if self.err < self.tol:\n",
    "                    break\n",
    "\n",
    "                # newton step\n",
    "                J = torch.eye(z.shape[1])[None,:,:].to(device) - (1 / torch.cosh(z_linear)**2)[:,:,None]*self.linear.weight[None,:,:]\n",
    "                z = z - torch.solve(g[:,:,None], J)[0][:,:,0]\n",
    "                self.iterations += 1\n",
    "    \n",
    "        # reengage autograd and add the gradient hook\n",
    "        z = torch.tanh(self.linear(z) + x)\n",
    "        z.register_hook(lambda grad : torch.solve(grad[:,:,None], J.transpose(1,2))[0][:,:,0])\n",
    "        return z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6f8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = TanhNewtonImplicitLayer(5, tol=1e-10).double()\n",
    "gradcheck(layer, torch.randn(3, 5, requires_grad=True, dtype=torch.double), check_undefined_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc24ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "model = nn.Sequential(nn.Flatten(),\n",
    "                      nn.Linear(784, 100),\n",
    "                      TanhNewtonImplicitLayer(100, max_iter=40),\n",
    "                      nn.Linear(100, 10)\n",
    "                      ).to(device)\n",
    "opt = optim.SGD(model.parameters(), lr=1e-1)\n",
    "\n",
    "for i in range(10):\n",
    "    if i == 5:\n",
    "        opt.param_groups[0][\"lr\"] = 1e-2\n",
    "\n",
    "    train_err, train_loss, train_fpiter = epoch(train_loader, model, opt, lambda x : x[2].iterations)\n",
    "    test_err, test_loss, test_fpiter = epoch(test_loader, model, monitor = lambda x : x[2].iterations)\n",
    "    print(f\"Train Error: {train_err:.4f}, Loss: {train_loss:.4f}, Newton Iters: {train_fpiter:.2f} | \" +\n",
    "          f\"Test Error: {test_err:.4f}, Loss: {test_loss:.4f}, Newton Iters: {test_fpiter:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e5dcf7",
   "metadata": {},
   "source": [
    "### Deep Equilibrium Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1f881e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetLayer(nn.Module):\n",
    "    def __init__(self, n_channels, n_inner_channels, kernel_size=3, num_groups=8):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(n_channels, n_inner_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.conv2 = nn.Conv2d(n_inner_channels, n_channels, kernel_size, padding=kernel_size//2, bias=False)\n",
    "        self.norm1 = nn.GroupNorm(num_groups, n_inner_channels)\n",
    "        self.norm2 = nn.GroupNorm(num_groups, n_channels)\n",
    "        self.norm3 = nn.GroupNorm(num_groups, n_channels)\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        \n",
    "    def forward(self, z, x):\n",
    "        y = self.norm1(F.relu(self.conv1(z)))\n",
    "        return self.norm3(F.relu(z + self.norm2(x + self.conv2(y))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94802f78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def anderson(f, x0, m=5, lam=1e-4, max_iter=50, tol=1e-2, beta = 1.0):\n",
    "    \"\"\" Anderson acceleration for fixed point iteration. \"\"\"\n",
    "    bsz, d, H, W = x0.shape\n",
    "    X = torch.zeros(bsz, m, d*H*W, dtype=x0.dtype, device=x0.device)\n",
    "    F = torch.zeros(bsz, m, d*H*W, dtype=x0.dtype, device=x0.device)\n",
    "    X[:,0], F[:,0] = x0.view(bsz, -1), f(x0).view(bsz, -1)\n",
    "    X[:,1], F[:,1] = F[:,0], f(F[:,0].view_as(x0)).view(bsz, -1)\n",
    "    \n",
    "    H = torch.zeros(bsz, m+1, m+1, dtype=x0.dtype, device=x0.device)\n",
    "    H[:,0,1:] = H[:,1:,0] = 1\n",
    "    y = torch.zeros(bsz, m+1, 1, dtype=x0.dtype, device=x0.device)\n",
    "    y[:,0] = 1\n",
    "    \n",
    "    res = []\n",
    "    for k in range(2, max_iter):\n",
    "        n = min(k, m)\n",
    "        G = F[:,:n]-X[:,:n]\n",
    "        H[:,1:n+1,1:n+1] = torch.bmm(G,G.transpose(1,2)) + lam*torch.eye(n, dtype=x0.dtype,device=x0.device)[None]\n",
    "        alpha = torch.solve(y[:,:n+1], H[:,:n+1,:n+1])[0][:, 1:n+1, 0]   # (bsz x n)\n",
    "        \n",
    "        X[:,k%m] = beta * (alpha[:,None] @ F[:,:n])[:,0] + (1-beta)*(alpha[:,None] @ X[:,:n])[:,0]\n",
    "        F[:,k%m] = f(X[:,k%m].view_as(x0)).view(bsz, -1)\n",
    "        res.append((F[:,k%m] - X[:,k%m]).norm().item()/(1e-5 + F[:,k%m].norm().item()))\n",
    "        if (res[-1] < tol):\n",
    "            break\n",
    "    return X[:,k%m].view_as(x0), res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5aff79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(10,64,32,32)\n",
    "f = ResNetLayer(64,128)\n",
    "Z, res = anderson(lambda Z : f(Z,X), torch.zeros_like(X), tol=1e-4, beta=1.0)\n",
    "plt.figure(dpi=150)\n",
    "plt.semilogy(res)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Relative residual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d04ec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_iteration(f, x0, max_iter=50, tol=1e-2):\n",
    "    f0 = f(x0)\n",
    "    res = []\n",
    "    for k in range(max_iter):\n",
    "        x = f0\n",
    "        f0 = f(x)\n",
    "        res.append((f0 - x).norm().item() / (1e-5 + f0.norm().item()))\n",
    "        if (res[-1] < tol):\n",
    "            break\n",
    "    return f0, res\n",
    "\n",
    "Z, res = forward_iteration(lambda Z : f(Z,X), torch.zeros_like(X), tol=1e-4)\n",
    "plt.figure(dpi=150)\n",
    "plt.semilogy(res)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Relative residual\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d309a748",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DEQFixedPoint(nn.Module):\n",
    "    def __init__(self, f, solver, **kwargs):\n",
    "        super().__init__()\n",
    "        self.f = f\n",
    "        self.solver = solver\n",
    "        self.kwargs = kwargs\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # compute forward pass and re-engage autograd tape\n",
    "        with torch.no_grad():\n",
    "            z, self.forward_res = self.solver(lambda z : self.f(z, x), torch.zeros_like(x), **self.kwargs)\n",
    "        z = self.f(z,x)\n",
    "        \n",
    "        # set up Jacobian vector product (without additional forward calls)\n",
    "        z0 = z.clone().detach().requires_grad_()\n",
    "        f0 = self.f(z0,x)\n",
    "        def backward_hook(grad):\n",
    "            g, self.backward_res = self.solver(lambda y : autograd.grad(f0, z0, y, retain_graph=True)[0] + grad,\n",
    "                                               grad, **self.kwargs)\n",
    "            return g\n",
    "                \n",
    "        z.register_hook(backward_hook)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30869c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run a very small network with double precision, iterating to high precision\n",
    "f = ResNetLayer(2,2, num_groups=2).double()\n",
    "deq = DEQFixedPoint(f, anderson, tol=1e-10, max_iter=500).double()\n",
    "gradcheck(deq, torch.randn(1,2,3,3).double().requires_grad_(), eps=1e-5, atol=1e-3, check_undefined_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a915a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = ResNetLayer(64,128)\n",
    "deq = DEQFixedPoint(f, anderson, tol=1e-4, max_iter=100, beta=2.0)\n",
    "X = torch.randn(10,64,32,32)\n",
    "out = deq(X)\n",
    "(out*torch.randn_like(out)).sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717eb7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(dpi=150)\n",
    "plt.semilogy(deq.forward_res)\n",
    "plt.semilogy(deq.backward_res)\n",
    "plt.legend(['Forward', 'Backward'])\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Residual\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bd1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "torch.manual_seed(0)\n",
    "chan = 48\n",
    "f = ResNetLayer(chan, 64, kernel_size=3)\n",
    "model = nn.Sequential(nn.Conv2d(3,chan, kernel_size=3, bias=True, padding=1),\n",
    "                      nn.BatchNorm2d(chan),\n",
    "                      DEQFixedPoint(f, anderson, tol=1e-2, max_iter=25, m=5),\n",
    "                      nn.BatchNorm2d(chan),\n",
    "                      nn.AvgPool2d(8,8),\n",
    "                      nn.Flatten(),\n",
    "                      nn.Linear(chan*4*4,10)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e68ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_train = datasets.CIFAR10(\".\", train=True, download=False, transform=transforms.ToTensor())\n",
    "cifar10_test = datasets.CIFAR10(\".\", train=False, download=False, transform=transforms.ToTensor())\n",
    "train_loader = DataLoader(cifar10_train, batch_size = 100, shuffle=True, num_workers=8)\n",
    "test_loader = DataLoader(cifar10_test, batch_size = 100, shuffle=False, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cf7a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch(loader, model, opt=None, lr_scheduler=None):\n",
    "    total_loss, total_err = 0.,0.\n",
    "    model.eval() if opt is None else model.train()\n",
    "    for X,y in loader:\n",
    "        X,y = X.to(device), y.to(device)\n",
    "        yp = model(X)\n",
    "        loss = nn.CrossEntropyLoss()(yp,y)\n",
    "        if opt:\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            lr_scheduler.step()\n",
    "                \n",
    "        total_err += (yp.max(dim=1)[1] != y).sum().item()\n",
    "        total_loss += loss.item() * X.shape[0]\n",
    "\n",
    "    return total_err / len(loader.dataset), total_loss / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6701f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = optim.Adam(model.parameters(), lr=1e-3)\n",
    "print(\"# Parameters: \", sum(a.numel() for a in model.parameters()))\n",
    "\n",
    "max_epochs = 50\n",
    "scheduler = optim.lr_scheduler.CosineAnnealingLR(opt, max_epochs*len(train_loader), eta_min=1e-6)\n",
    "\n",
    "for i in range(50):\n",
    "    print(epoch_cifar(train_loader, model, opt, scheduler))\n",
    "    print(epoch_cifar(test_loader, model)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9aba5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
