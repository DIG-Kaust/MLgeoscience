{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Homepage This course covers the fundamentals of machine learning, its applications to geoscientific problems, and it provides basic best practices for the rigorous development and evaluation of machine learning results. The main focus of the course is on describing the fundamental theory of linear regression , logistic regression , neural networks , convolutional neural networks , sequence modelling , dimensionality reduction , generative modelling , and physics-inspired neural networks . Students will also be introduced to practical applications in geoscience for each of the presented method lab sessions will be held using the PyTorch computational framework in the Python programming language. Lectures X, and X, XX:XXam - XX:XXam Teaching Staff Instructor: Matteo Ravasi - Office Hours: Tuesday 4pm to 5pm (by Appointment: Zoom or Office - BI-1432) Textbook Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville \u2013 MIT Press. Pre-requisites Knowledge of calculus, linear algebra ad statistics is required. Basic Python knowledge is preferred. Course Requirements ErSE 213 - Inverse problems","title":"Homepage"},{"location":"#homepage","text":"This course covers the fundamentals of machine learning, its applications to geoscientific problems, and it provides basic best practices for the rigorous development and evaluation of machine learning results. The main focus of the course is on describing the fundamental theory of linear regression , logistic regression , neural networks , convolutional neural networks , sequence modelling , dimensionality reduction , generative modelling , and physics-inspired neural networks . Students will also be introduced to practical applications in geoscience for each of the presented method lab sessions will be held using the PyTorch computational framework in the Python programming language.","title":"Homepage"},{"location":"#lectures","text":"X, and X, XX:XXam - XX:XXam","title":"Lectures"},{"location":"#teaching-staff","text":"Instructor: Matteo Ravasi - Office Hours: Tuesday 4pm to 5pm (by Appointment: Zoom or Office - BI-1432)","title":"Teaching Staff"},{"location":"#textbook","text":"Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville \u2013 MIT Press.","title":"Textbook"},{"location":"#pre-requisites","text":"Knowledge of calculus, linear algebra ad statistics is required. Basic Python knowledge is preferred.","title":"Pre-requisites"},{"location":"#course-requirements","text":"ErSE 213 - Inverse problems","title":"Course Requirements"},{"location":"gradind/","text":"Grading system The final grade will be obtained as the combination of the following: 30.00% - Course Project 30.00% - Final exam 20.00% - Homeworks 20.00% - Midterm exam Homeworks Homeworks will be assigned at the end of each topic. They consist of both pen and paper questions and programming exercises. The submitted codes must be properly commented and implementation choices must be justified (this is as important as the code itself and counts towards the final mark). Project The project should cover one of the topics learned in this course. It could be focused on implementing a novel machine learning algorithm to a geoscientific problem or on performing a systematic comparison of different machine learning algorithms to a geoscientific dataset. Students are encouraged to start the project early. The best way is to define a problem statement at the beginning of the term and learn how to use machine learning to solve such a problem during the course. Collaboration Most homeworks involve programming assigments. Students are encouraged to collaborate and consult with each other, but an individual assignments (and code) must be handed in. Acknowledge explicitly in your submitted assignment if you have collaborated with someone else while working on the assigment. Late submissions Each student has access to one late submission wildcard of no more than 2 days from the submission deadline. Apart from using this wildcard, late submissions will be penalized with a loss of 40% of the achieved score.","title":"Grading system"},{"location":"gradind/#grading-system","text":"The final grade will be obtained as the combination of the following: 30.00% - Course Project 30.00% - Final exam 20.00% - Homeworks 20.00% - Midterm exam","title":"Grading system"},{"location":"gradind/#homeworks","text":"Homeworks will be assigned at the end of each topic. They consist of both pen and paper questions and programming exercises. The submitted codes must be properly commented and implementation choices must be justified (this is as important as the code itself and counts towards the final mark).","title":"Homeworks"},{"location":"gradind/#project","text":"The project should cover one of the topics learned in this course. It could be focused on implementing a novel machine learning algorithm to a geoscientific problem or on performing a systematic comparison of different machine learning algorithms to a geoscientific dataset. Students are encouraged to start the project early. The best way is to define a problem statement at the beginning of the term and learn how to use machine learning to solve such a problem during the course.","title":"Project"},{"location":"gradind/#collaboration","text":"Most homeworks involve programming assigments. Students are encouraged to collaborate and consult with each other, but an individual assignments (and code) must be handed in. Acknowledge explicitly in your submitted assignment if you have collaborated with someone else while working on the assigment.","title":"Collaboration"},{"location":"gradind/#late-submissions","text":"Each student has access to one late submission wildcard of no more than 2 days from the submission deadline. Apart from using this wildcard, late submissions will be penalized with a loss of 40% of the achieved score.","title":"Late submissions"},{"location":"schedule/","text":"Schedule Lecture Date Topic Exercise 1 XX Course overview and introduction to Machine Learning - 2 XX Linear algebra refresher - 2 XX Probability refresher - 3 XX Gradient-based optimization link 4 XX Linear and Logistic regression link 10 XX More on gradient-based optimization - 11 XX Autoencoders -","title":"Schedule"},{"location":"schedule/#schedule","text":"Lecture Date Topic Exercise 1 XX Course overview and introduction to Machine Learning - 2 XX Linear algebra refresher - 2 XX Probability refresher - 3 XX Gradient-based optimization link 4 XX Linear and Logistic regression link 10 XX More on gradient-based optimization - 11 XX Autoencoders -","title":"Schedule"},{"location":"lectures/10_gradopt1/","text":"More on gradient-based optimization XX","title":"More on gradient-based optimization"},{"location":"lectures/10_gradopt1/#more-on-gradient-based-optimization","text":"XX","title":"More on gradient-based optimization"},{"location":"lectures/1_intro/","text":"Introduction to Machine Learning Humans have long dreamed of creating machines that can think and act independently . For many years this has been the aim of Artificial Intelligence (AI) . In the early days of AI, many problems that are difficult to solve by humans (e.g., large summations or multiplications, solution of systems of equations) turn out to be easier for computers as long as humans could define a list of tasks that machines could perform at faster speed and higher precisions than humans can do themselves. On the other hand, tasks that are very easily solved by adult humans and even kids (e.g., recognizing animals in pictures or singing a song) turned out to be very difficult for computers. The main reason of such difficulties lies in the fact that humans cannot explain in words (and with a simple set of instructions) how they have learned to accomplish these tasks. This is where instead the second era of AI solutions, belonging to the field of Machine Learning (ML) , have shown astonishing results in the last decade. Instead of relying on hard-coded rules, these algorithms operate in a similar fashion to human beings as they learn from experience . In other words, given enough training data in the form of inputs (e.g., photos) and outputs (e.g., label of the animal present in the photo), ML algorithms can learn a complex nonlinear mapping between them such that they can infer the output from the input when provided with unseen inputs. A large variety of ML algorithms have been developed by the scientific community, ranging from the basic linear and logistic regression that we will see in our third lecture , decision tree-based statistical methods such as random forrest or gradient boosting , all the way to deep neural networks , which have recently shown to outperform previously developed algorithms in many fields (e.g., computer science, text analysis and speech recognition, seismic interpretation). This subfield has grown exponentially in the last few years and it is now referred to as Deep Learning and will be subject of most of our course. In short, Deep learning is a particular kind of machine learning that represent the world as a nested hierarchy of increasingly complicated concepts the more we move away from the input and towards the output of the associated computational graph. Whilst sharing the same underlying principle of learning from experience in the form of a training data , different algorithms presents their own strenght and limitations and a machine learning practitioner must make a careful judgment at any time depending on the problem to be solved. {: align=left } Terminology Machine Learning is divided into 3 main categories: Supervised Learning : learn a function that maps an input to an output ( \\(X \\rightarrow Y\\) ). Inputs are also referred to as features and outputs are called targets. In practice we have access to a number of training pairs \\(\\{ \\textbf{x}_i, \\textbf{y}_i \\} \\; i=1,..,N\\) and we learn \\(\\textbf{y}_i=f_\\theta(\\textbf{x}_i)\\) where \\(f_\\theta\\) is for example parametrized via a neural network. Two main applications of supervised learning are Classification : the target is discrete Regression : the target is continuous Unsupervised Learning : learn patterns from unlabelled data. These methods have been shown to be able to find compact internal representation of the manifold the input data belongs to. Such compact representations can become valuable input features for subsequent tasks of supervised learning. In the context of deep learning, unsupervised models may even attempt to estimate the entire probability distribution of the dataset or how to generate new, indipendent samples from such distribution. We will get into the mathematical details of these families of models in the second part of our course. Semi-supervised Learning : it lies in between the other other learning paradigms as it learns from some examples that include a target and some that do not. Input data can also come in 2 different types: Structured data : tables (e.g., databases) Unstructured data : images, audio, text, ... Examples of applications in geoscience are displayed in the figure below. A number of available data types in various geoscientific contexts is also displayed. History Finally, we take a brief look at the history of Deep Learning. This field has so far experienced three main waves of major development (and periods of success) interspersed by winters (or periods of disbelief): '40 - '50 : first learning algorithms heavily influenced by our understanding of the inner working of the human brain. Mostly linear models such as the McCulloch-Pitts neuron, the perceptron by Rosenblatt, and the adaptive linear element (ADALINE). The latter was trained on an algorithm very similar to Stochastic Gradient Descent (SGD). These models showed poor performance in learning complex functions (e.g., XOR) and led to a drop in popularity of the field. '80 - '90 : these years so the creation of the Multi Layer Perceptron (MLP), the neocognitron (the ancestor of the convolutional layer), the first deep neural networks (e.g., LeNet for MNIST classification), the first sequence-to-sequence networks and the LSTM layer. from 2010 till now : a major moment for the history of this field can be traced back to 2012, when a deep convolution neural network developed by Krizhevsky and co-authors won the ImageNet competition lowering the top-5 error rate from 26.1 percent (previous winning solution not based on a neural network) to 15.3 percent. Since then the field has exploded with advances both in terms of model architectures (AlexNet, VGG, ResNet, GoogleLeNet, ...) optimization algorithms (AdaGrad, RMSProp, Adam, ...), applications (computer vision, text analysis, speech recognition, ...). Moreover, recente developments in the area of unsupervised learning have led to the creation of dimensionality reduction and generative algorithms that can now outperform any state-of-the-art method that is not based on neural networks. If want to dig deeper into the history of this field, an interesting read can be found here .","title":"Introduction to Machine Learning"},{"location":"lectures/1_intro/#introduction-to-machine-learning","text":"Humans have long dreamed of creating machines that can think and act independently . For many years this has been the aim of Artificial Intelligence (AI) . In the early days of AI, many problems that are difficult to solve by humans (e.g., large summations or multiplications, solution of systems of equations) turn out to be easier for computers as long as humans could define a list of tasks that machines could perform at faster speed and higher precisions than humans can do themselves. On the other hand, tasks that are very easily solved by adult humans and even kids (e.g., recognizing animals in pictures or singing a song) turned out to be very difficult for computers. The main reason of such difficulties lies in the fact that humans cannot explain in words (and with a simple set of instructions) how they have learned to accomplish these tasks. This is where instead the second era of AI solutions, belonging to the field of Machine Learning (ML) , have shown astonishing results in the last decade. Instead of relying on hard-coded rules, these algorithms operate in a similar fashion to human beings as they learn from experience . In other words, given enough training data in the form of inputs (e.g., photos) and outputs (e.g., label of the animal present in the photo), ML algorithms can learn a complex nonlinear mapping between them such that they can infer the output from the input when provided with unseen inputs. A large variety of ML algorithms have been developed by the scientific community, ranging from the basic linear and logistic regression that we will see in our third lecture , decision tree-based statistical methods such as random forrest or gradient boosting , all the way to deep neural networks , which have recently shown to outperform previously developed algorithms in many fields (e.g., computer science, text analysis and speech recognition, seismic interpretation). This subfield has grown exponentially in the last few years and it is now referred to as Deep Learning and will be subject of most of our course. In short, Deep learning is a particular kind of machine learning that represent the world as a nested hierarchy of increasingly complicated concepts the more we move away from the input and towards the output of the associated computational graph. Whilst sharing the same underlying principle of learning from experience in the form of a training data , different algorithms presents their own strenght and limitations and a machine learning practitioner must make a careful judgment at any time depending on the problem to be solved. {: align=left }","title":"Introduction to Machine Learning"},{"location":"lectures/1_intro/#terminology","text":"Machine Learning is divided into 3 main categories: Supervised Learning : learn a function that maps an input to an output ( \\(X \\rightarrow Y\\) ). Inputs are also referred to as features and outputs are called targets. In practice we have access to a number of training pairs \\(\\{ \\textbf{x}_i, \\textbf{y}_i \\} \\; i=1,..,N\\) and we learn \\(\\textbf{y}_i=f_\\theta(\\textbf{x}_i)\\) where \\(f_\\theta\\) is for example parametrized via a neural network. Two main applications of supervised learning are Classification : the target is discrete Regression : the target is continuous Unsupervised Learning : learn patterns from unlabelled data. These methods have been shown to be able to find compact internal representation of the manifold the input data belongs to. Such compact representations can become valuable input features for subsequent tasks of supervised learning. In the context of deep learning, unsupervised models may even attempt to estimate the entire probability distribution of the dataset or how to generate new, indipendent samples from such distribution. We will get into the mathematical details of these families of models in the second part of our course. Semi-supervised Learning : it lies in between the other other learning paradigms as it learns from some examples that include a target and some that do not. Input data can also come in 2 different types: Structured data : tables (e.g., databases) Unstructured data : images, audio, text, ... Examples of applications in geoscience are displayed in the figure below. A number of available data types in various geoscientific contexts is also displayed.","title":"Terminology"},{"location":"lectures/1_intro/#history","text":"Finally, we take a brief look at the history of Deep Learning. This field has so far experienced three main waves of major development (and periods of success) interspersed by winters (or periods of disbelief): '40 - '50 : first learning algorithms heavily influenced by our understanding of the inner working of the human brain. Mostly linear models such as the McCulloch-Pitts neuron, the perceptron by Rosenblatt, and the adaptive linear element (ADALINE). The latter was trained on an algorithm very similar to Stochastic Gradient Descent (SGD). These models showed poor performance in learning complex functions (e.g., XOR) and led to a drop in popularity of the field. '80 - '90 : these years so the creation of the Multi Layer Perceptron (MLP), the neocognitron (the ancestor of the convolutional layer), the first deep neural networks (e.g., LeNet for MNIST classification), the first sequence-to-sequence networks and the LSTM layer. from 2010 till now : a major moment for the history of this field can be traced back to 2012, when a deep convolution neural network developed by Krizhevsky and co-authors won the ImageNet competition lowering the top-5 error rate from 26.1 percent (previous winning solution not based on a neural network) to 15.3 percent. Since then the field has exploded with advances both in terms of model architectures (AlexNet, VGG, ResNet, GoogleLeNet, ...) optimization algorithms (AdaGrad, RMSProp, Adam, ...), applications (computer vision, text analysis, speech recognition, ...). Moreover, recente developments in the area of unsupervised learning have led to the creation of dimensionality reduction and generative algorithms that can now outperform any state-of-the-art method that is not based on neural networks. If want to dig deeper into the history of this field, an interesting read can be found here .","title":"History"},{"location":"lectures/2_linalg/","text":"Linear Algebra refresher In this lecture we will go through some of the key concepts of linear algebra and inverse problem theory that are required to develop the theories of the different machine learning algorithm presented in this course. This is not meant to be an exhaustive treatise and students are strongly advised to take the ErSE 213 - Inverse Problems prior to this course. Three key mathematical objects arise in the study of linear algebra: Scalars : \\(a \\in \\mathbb{R}\\) , a single number represented by a lower case italic letter; Vectors : \\(\\mathbf{x} = [x_1, x_2, ..., x_N]^T \\in \\mathbb{R}^N\\) , ordered collection of \\(N\\) numbers represented by a lower case bold letter; it is sometimes useful to extract a subset of elements by defining a set \\(\\mathbb{S}\\) and add it to as a superscript, \\(\\mathbf{x}_\\mathbb{S}\\) . As an example, given \\(\\mathbb{S} = {1, 3, N}\\) we can define the vector \\(\\mathbf{x}_\\mathbb{S} = [x_1, x_3, ..., x_N]\\) and its complementary vector \\(\\mathbf{x}_{-\\mathbb{S}} = [x_2, x_4, ..., x_{N-1}]\\) Matrices : \\(\\mathbf{X} \\in \\mathbb{R}^{[N \\times M]}\\) , two dimensional collection of numbers represented by an upper case bold letter where \\(N\\) and \\(M\\) are referred to as the height and width of the matrix. More specifically a matrix can be written as \\[\\mathbf{X} = \\begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,M} \\\\ ... & ... & ... \\\\ x_{N,1} & x_{N,2} & x_{N,M} \\end{bmatrix} \\] A matrix can be indexed by rows \\(\\mathbf{X}_{i, :}\\) (i-th row), by columns \\(\\mathbf{X}_{:, j}\\) (j-th column), and by element \\(\\mathbf{X}_{i, j}\\) (i-th row, j-th column). A number of useful operations that are commonly applied on vectors and matrices are now described: Transpose: \\(\\mathbf{Y} = \\mathbf{X}^T\\) , where \\(Y_{i, j} = X_{j, i}\\) Matrix plus vector: \\(\\mathbf{Y}_{[N \\times M]} = \\mathbf{X}_{[N \\times M]} + \\mathbf{z}_{[1 \\times M]}\\) , where \\(Y_{i, j} = X_{i, j} + z_{j}\\) ( \\(\\mathbf{z}\\) is added to each row of the matrix \\(\\mathbf{X}\\) ) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-matrix product: \\(\\mathbf{C}_{[N \\times K]} = \\mathbf{A}_{[N \\times M]} \\mathbf{B}_{[M \\times K]}\\) , where \\(C_{i,k} = \\sum_{j=1}^M A_{i, j} B_{j, k}\\) Hadamart product (i.e., element-wise product): \\(\\mathbf{C}_{[N \\times M]} = \\mathbf{A}_{[N \\times M]} \\odot \\mathbf{B}_{[N \\times M]}\\) , where \\(C_{i,j} = A_{i, j} B_{i, j}\\) Dot product: \\(a = \\mathbf{x}_{[N \\times 1]}^T \\mathbf{y}_{[N \\times 1]} = \\sum_{i=1}^N x_i y_i\\) Identity matrix: \\(\\mathbf{I}_N = diag\\{\\mathbf{1}_N\\}\\) . Based on its definition, we have that \\(\\mathbf{I}_N \\mathbf{x} = \\mathbf{x}\\) and \\(\\mathbf{I}_N \\mathbf{X} = \\mathbf{X}\\) Inverse matrix: given \\(\\mathbf{y} = \\mathbf{A} \\mathbf{x}\\) , the inverse matrix of \\(\\mathbf{A}\\) is a matrix that satisfies the following equality \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_N\\) . We can finally write \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{y}\\) Orthogonal vectors and matrices: given two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) , they are said to be orthogonal if \\(\\mathbf{y}^T \\mathbf{x} = 0\\) . Given two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) , they are said to be orthogonal if \\(\\mathbf{Y}^T \\mathbf{X} = \\mathbf{I}_N\\) . Orthogonal matrices are especially interesting because their inverse is very cheap \\(\\mathbf{X}^{-1} = \\mathbf{X}^T\\) Matrix decomposition: like any scalar number can be decomposed into a product of prime numbers, a matrix \\(\\mathbf{A}\\) can also be decomposed into a combination of vectors (i.e., eigenvectors) and scalars (i.e., eigenvalues). Eigendecomposition: real-valued, square, symmetric matrices can be written as \\(\\mathbf{A} = \\mathbf{V} \\Lambda \\mathbf{V}^T = \\sum_i \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\) where \\(\\lambda_i\\) and \\(\\mathbf{v}_i\\) are the eigenvalues and eigenvectors of the matrix \\(\\mathbf{A}\\) , respectively. Eigenvectors are placed along the columns of the matrix \\(\\mathbf{V}\\) , which is an orthogonal matrix (i.e., \\(\\mathbf{V}^T=\\mathbf{V}^{-1}\\) ). Eigenvalues are placed along the diagonal of the matrix \\(\\Lambda=diag\\{\\lambda\\}\\) and tell us about the rank of the matrix, \\(rank(\\mathbf{A}) = \\# \\lambda \\neq 0\\) . A full rank matrix is matrix whose eigenvalues are all non-zero and can be inverted. In this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\Lambda^{-1}\\mathbf{V}^T\\) Singular value decomposition (SVD): this is a more general decomposition which can be applied to real-valued, non-square, non-symmetric matrices. Singular vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) and singular values \\(\\lambda\\) generalized the concept of eigenvectors and and eigenvalues. The matrix \\(\\mathbf{A}\\) can be decomposed as \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T\\) where \\(\\mathbf{D} = \\Lambda\\) for square matrices, \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]^T\\) for \\(N>M\\) and \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]\\) for \\(M>N\\) . Similar to the eigendecomposition, in this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\mathbf{D}^{-1}\\mathbf{U}^T\\) Conditioning: in general, it refers to how fast a function \\(f(x)\\) changes given a small change in its input \\(x\\) . Similarly for a matrix, conditioning is linked to the curvature of its associated quadratic form \\(f(\\mathbf{A}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) and it generally indicates how rapidly this function changes as function of \\(\\mathbf{x}\\) . It is defined as \\(cond(\\mathbf{A})=\\frac{|\\lambda_{max}|}{|\\lambda_{min}|}\\) . Norms : another important object that we will be using when defining cost functions for ML models are norms. A norm is a function that maps a vector \\(\\mathbf{x} \\in \\mathbb{R}^N\\) to a scalar \\(d \\in \\mathbb{R}\\) and it can be loosely seen as measure of the lenght of the vector (i.e., distance from the origin). In general, the \\(L^p\\) norm is defined as: \\[ ||\\mathbf{x}||_p = \\left( \\sum_i |x_i|^p \\right) ^{1/p} \\; p \\ge 0 \\] Popular norms are: Euclidean norm ( \\(L_2\\) ): \\(||\\mathbf{x}||_2 = \\sqrt{\\sum_i x_i^2}\\) , is a real distance of a vector from the origin of the N-d Euclidean space. Note that \\(||\\mathbf{x}||_2^2 = \\mathbf{x}^T \\mathbf{x}\\) and that \\(||\\mathbf{x}||_2=1\\) for a unit vector; \\(L_1\\) norm: \\(||\\mathbf{x}||_1 = \\sum_i |x_i|\\) \\(L_0\\) norm: number of non-zero elements in the vector \\(\\mathbf{x}\\) \\(L_\\inf\\) norm: \\(||\\mathbf{x}||_2 = max |x_i|\\) Frobenious norm (for matrices): \\(||\\mathbf{A}||_F = \\sqrt{\\sum_{i,j} A_{i,j}^2}\\) ,","title":"Linear Algebra refresher"},{"location":"lectures/2_linalg/#linear-algebra-refresher","text":"In this lecture we will go through some of the key concepts of linear algebra and inverse problem theory that are required to develop the theories of the different machine learning algorithm presented in this course. This is not meant to be an exhaustive treatise and students are strongly advised to take the ErSE 213 - Inverse Problems prior to this course. Three key mathematical objects arise in the study of linear algebra: Scalars : \\(a \\in \\mathbb{R}\\) , a single number represented by a lower case italic letter; Vectors : \\(\\mathbf{x} = [x_1, x_2, ..., x_N]^T \\in \\mathbb{R}^N\\) , ordered collection of \\(N\\) numbers represented by a lower case bold letter; it is sometimes useful to extract a subset of elements by defining a set \\(\\mathbb{S}\\) and add it to as a superscript, \\(\\mathbf{x}_\\mathbb{S}\\) . As an example, given \\(\\mathbb{S} = {1, 3, N}\\) we can define the vector \\(\\mathbf{x}_\\mathbb{S} = [x_1, x_3, ..., x_N]\\) and its complementary vector \\(\\mathbf{x}_{-\\mathbb{S}} = [x_2, x_4, ..., x_{N-1}]\\) Matrices : \\(\\mathbf{X} \\in \\mathbb{R}^{[N \\times M]}\\) , two dimensional collection of numbers represented by an upper case bold letter where \\(N\\) and \\(M\\) are referred to as the height and width of the matrix. More specifically a matrix can be written as \\[\\mathbf{X} = \\begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,M} \\\\ ... & ... & ... \\\\ x_{N,1} & x_{N,2} & x_{N,M} \\end{bmatrix} \\] A matrix can be indexed by rows \\(\\mathbf{X}_{i, :}\\) (i-th row), by columns \\(\\mathbf{X}_{:, j}\\) (j-th column), and by element \\(\\mathbf{X}_{i, j}\\) (i-th row, j-th column). A number of useful operations that are commonly applied on vectors and matrices are now described: Transpose: \\(\\mathbf{Y} = \\mathbf{X}^T\\) , where \\(Y_{i, j} = X_{j, i}\\) Matrix plus vector: \\(\\mathbf{Y}_{[N \\times M]} = \\mathbf{X}_{[N \\times M]} + \\mathbf{z}_{[1 \\times M]}\\) , where \\(Y_{i, j} = X_{i, j} + z_{j}\\) ( \\(\\mathbf{z}\\) is added to each row of the matrix \\(\\mathbf{X}\\) ) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-matrix product: \\(\\mathbf{C}_{[N \\times K]} = \\mathbf{A}_{[N \\times M]} \\mathbf{B}_{[M \\times K]}\\) , where \\(C_{i,k} = \\sum_{j=1}^M A_{i, j} B_{j, k}\\) Hadamart product (i.e., element-wise product): \\(\\mathbf{C}_{[N \\times M]} = \\mathbf{A}_{[N \\times M]} \\odot \\mathbf{B}_{[N \\times M]}\\) , where \\(C_{i,j} = A_{i, j} B_{i, j}\\) Dot product: \\(a = \\mathbf{x}_{[N \\times 1]}^T \\mathbf{y}_{[N \\times 1]} = \\sum_{i=1}^N x_i y_i\\) Identity matrix: \\(\\mathbf{I}_N = diag\\{\\mathbf{1}_N\\}\\) . Based on its definition, we have that \\(\\mathbf{I}_N \\mathbf{x} = \\mathbf{x}\\) and \\(\\mathbf{I}_N \\mathbf{X} = \\mathbf{X}\\) Inverse matrix: given \\(\\mathbf{y} = \\mathbf{A} \\mathbf{x}\\) , the inverse matrix of \\(\\mathbf{A}\\) is a matrix that satisfies the following equality \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_N\\) . We can finally write \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{y}\\) Orthogonal vectors and matrices: given two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) , they are said to be orthogonal if \\(\\mathbf{y}^T \\mathbf{x} = 0\\) . Given two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) , they are said to be orthogonal if \\(\\mathbf{Y}^T \\mathbf{X} = \\mathbf{I}_N\\) . Orthogonal matrices are especially interesting because their inverse is very cheap \\(\\mathbf{X}^{-1} = \\mathbf{X}^T\\) Matrix decomposition: like any scalar number can be decomposed into a product of prime numbers, a matrix \\(\\mathbf{A}\\) can also be decomposed into a combination of vectors (i.e., eigenvectors) and scalars (i.e., eigenvalues). Eigendecomposition: real-valued, square, symmetric matrices can be written as \\(\\mathbf{A} = \\mathbf{V} \\Lambda \\mathbf{V}^T = \\sum_i \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\) where \\(\\lambda_i\\) and \\(\\mathbf{v}_i\\) are the eigenvalues and eigenvectors of the matrix \\(\\mathbf{A}\\) , respectively. Eigenvectors are placed along the columns of the matrix \\(\\mathbf{V}\\) , which is an orthogonal matrix (i.e., \\(\\mathbf{V}^T=\\mathbf{V}^{-1}\\) ). Eigenvalues are placed along the diagonal of the matrix \\(\\Lambda=diag\\{\\lambda\\}\\) and tell us about the rank of the matrix, \\(rank(\\mathbf{A}) = \\# \\lambda \\neq 0\\) . A full rank matrix is matrix whose eigenvalues are all non-zero and can be inverted. In this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\Lambda^{-1}\\mathbf{V}^T\\) Singular value decomposition (SVD): this is a more general decomposition which can be applied to real-valued, non-square, non-symmetric matrices. Singular vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) and singular values \\(\\lambda\\) generalized the concept of eigenvectors and and eigenvalues. The matrix \\(\\mathbf{A}\\) can be decomposed as \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T\\) where \\(\\mathbf{D} = \\Lambda\\) for square matrices, \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]^T\\) for \\(N>M\\) and \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]\\) for \\(M>N\\) . Similar to the eigendecomposition, in this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\mathbf{D}^{-1}\\mathbf{U}^T\\) Conditioning: in general, it refers to how fast a function \\(f(x)\\) changes given a small change in its input \\(x\\) . Similarly for a matrix, conditioning is linked to the curvature of its associated quadratic form \\(f(\\mathbf{A}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) and it generally indicates how rapidly this function changes as function of \\(\\mathbf{x}\\) . It is defined as \\(cond(\\mathbf{A})=\\frac{|\\lambda_{max}|}{|\\lambda_{min}|}\\) . Norms : another important object that we will be using when defining cost functions for ML models are norms. A norm is a function that maps a vector \\(\\mathbf{x} \\in \\mathbb{R}^N\\) to a scalar \\(d \\in \\mathbb{R}\\) and it can be loosely seen as measure of the lenght of the vector (i.e., distance from the origin). In general, the \\(L^p\\) norm is defined as: \\[ ||\\mathbf{x}||_p = \\left( \\sum_i |x_i|^p \\right) ^{1/p} \\; p \\ge 0 \\] Popular norms are: Euclidean norm ( \\(L_2\\) ): \\(||\\mathbf{x}||_2 = \\sqrt{\\sum_i x_i^2}\\) , is a real distance of a vector from the origin of the N-d Euclidean space. Note that \\(||\\mathbf{x}||_2^2 = \\mathbf{x}^T \\mathbf{x}\\) and that \\(||\\mathbf{x}||_2=1\\) for a unit vector; \\(L_1\\) norm: \\(||\\mathbf{x}||_1 = \\sum_i |x_i|\\) \\(L_0\\) norm: number of non-zero elements in the vector \\(\\mathbf{x}\\) \\(L_\\inf\\) norm: \\(||\\mathbf{x}||_2 = max |x_i|\\) Frobenious norm (for matrices): \\(||\\mathbf{A}||_F = \\sqrt{\\sum_{i,j} A_{i,j}^2}\\) ,","title":"Linear Algebra refresher"},{"location":"lectures/2_prob/","text":"Probability refresher Another set of fundamental mathematical tools required to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) In order to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) we need to familarize with some basic concepts of: mathematical tools from: Probability : mathematical framework to handle uncertain statements; Information Theory : scientific field focused on the quantification of amount of uncertainty in a probability distribution. Probability Random Variable : a variable whose value is unknown, all we know is that it can take on different values with a given probability. It is generally defined by an uppercase letter \\(X\\) , whilst the values it can take are in lowercase letter \\(x\\) . Probability distribution : description of how likely a variable \\(x\\) is, \\(P(x)\\) (or \\(p(x)\\) ). Depending on the type of variable we have: Discrete distributions : \\(P(X)\\) called Probability Mass Function (PMF) and \\(X\\) can take on a discrete number of states N. A classical example is represented by a coin where N=2 and \\(X={0,1}\\) . For a fair coin, \\(P(X=0)=0.5\\) and \\(P(X=1)=0.5\\) . Continuous distributions : \\(p(X)\\) called Probability Density Function (PDF) and \\(X\\) can take on any value from a continuous space (e.g., \\(\\mathbb{R}\\) ). A classical example is represented by the gaussian distribution where \\(x \\in (-\\infty, \\infty)\\) . A probability distribution must satisfy the following conditions: each of the possible states must have probability bounded between 0 (no occurrance) and 1 (certainty of occurcence): \\(\\forall x \\in X, \\; 0 \\leq P(x) \\leq 1\\) (or \\(p(x) \\geq 0\\) , where the upper bound is removed because of the fact that the integration step \\(\\delta x\\) in the second condition can be smaller than 1: \\(p(X=x) \\delta x <=1\\) ); the sum of the probabilities of all possible states must equal to 1: \\(\\sum_x P(X=x)=1\\) (or \\(\\int p(X=x)dx=1\\) ). Joint and Marginal Probabilities : assuming we have a probability distribution acting over a set of variables (e.g., \\(X\\) and \\(Y\\) ) we can define Joint distribution : \\(P(X=x, Y=y)\\) (or \\(p(X=x, Y=y)\\) ); Marginal distribution : \\(P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y)\\) (or \\(p(X=x) = \\int P(X=x, Y=y) dy\\) ), which is the probability spanning one or a subset of the original variables; Conditional Probability : provides us with the probability of an event given the knowledge that another event has already occurred \\[ P(Y=y | X=x) = \\frac{P(X=x, Y=y)}{P(X=x)} \\] This formula can be used recursively to define the joint probability of a number N of variables as product of conditional probabilities (so-called Chain Rule of Probability ) \\[ P(x_1, x_2, ..., x_N) = P(x_1) \\prod_{i=2}^N P(x_i | x_1, x_2, x_{i-1}) \\] Independence and Conditional Independence : Two variables X and Y are said to be indipendent if \\[ P(X=x, Y=y) = P(X=x) P(Y=y) \\] If both variables are conditioned on a third variable Z (i.e., P(X=x, Y=y | Z=z)), they are said to be conditionally independent if \\[ P(X=x, Y=y | Z=z) = P(X=x | Z=z) P(Y=y| Z=z) \\] Mean (or Expectation) : Given a function \\(f(x)\\) where \\(x\\) is a stochastic variable with probability \\(P(x)\\) , its average or mean value is defined as follows for the discrete case: \\[ \\mu = E_{x \\sim P} [f(x)] = \\sum_x P(x) f(x) \\] and for the continuos case \\[ \\mu = E_{x \\sim p} [f(x)] = \\int p(x) f(x) dx \\] In most Machine Learning applications we do not have knowledge of the full distribution to evaluate the mean, rather we have access to N equi-probable samples that we assume are drawn from the underlying distribution. We can approximate the mean via the Sample Mean : \\[ \\mu \\approx \\sum_i \\frac{1}{N} f(x_i) \\] Variance (and Covariance) : Information theory","title":"Probability refresher"},{"location":"lectures/2_prob/#probability-refresher","text":"Another set of fundamental mathematical tools required to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) In order to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) we need to familarize with some basic concepts of: mathematical tools from: Probability : mathematical framework to handle uncertain statements; Information Theory : scientific field focused on the quantification of amount of uncertainty in a probability distribution.","title":"Probability refresher"},{"location":"lectures/2_prob/#probability","text":"Random Variable : a variable whose value is unknown, all we know is that it can take on different values with a given probability. It is generally defined by an uppercase letter \\(X\\) , whilst the values it can take are in lowercase letter \\(x\\) . Probability distribution : description of how likely a variable \\(x\\) is, \\(P(x)\\) (or \\(p(x)\\) ). Depending on the type of variable we have: Discrete distributions : \\(P(X)\\) called Probability Mass Function (PMF) and \\(X\\) can take on a discrete number of states N. A classical example is represented by a coin where N=2 and \\(X={0,1}\\) . For a fair coin, \\(P(X=0)=0.5\\) and \\(P(X=1)=0.5\\) . Continuous distributions : \\(p(X)\\) called Probability Density Function (PDF) and \\(X\\) can take on any value from a continuous space (e.g., \\(\\mathbb{R}\\) ). A classical example is represented by the gaussian distribution where \\(x \\in (-\\infty, \\infty)\\) . A probability distribution must satisfy the following conditions: each of the possible states must have probability bounded between 0 (no occurrance) and 1 (certainty of occurcence): \\(\\forall x \\in X, \\; 0 \\leq P(x) \\leq 1\\) (or \\(p(x) \\geq 0\\) , where the upper bound is removed because of the fact that the integration step \\(\\delta x\\) in the second condition can be smaller than 1: \\(p(X=x) \\delta x <=1\\) ); the sum of the probabilities of all possible states must equal to 1: \\(\\sum_x P(X=x)=1\\) (or \\(\\int p(X=x)dx=1\\) ). Joint and Marginal Probabilities : assuming we have a probability distribution acting over a set of variables (e.g., \\(X\\) and \\(Y\\) ) we can define Joint distribution : \\(P(X=x, Y=y)\\) (or \\(p(X=x, Y=y)\\) ); Marginal distribution : \\(P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y)\\) (or \\(p(X=x) = \\int P(X=x, Y=y) dy\\) ), which is the probability spanning one or a subset of the original variables; Conditional Probability : provides us with the probability of an event given the knowledge that another event has already occurred \\[ P(Y=y | X=x) = \\frac{P(X=x, Y=y)}{P(X=x)} \\] This formula can be used recursively to define the joint probability of a number N of variables as product of conditional probabilities (so-called Chain Rule of Probability ) \\[ P(x_1, x_2, ..., x_N) = P(x_1) \\prod_{i=2}^N P(x_i | x_1, x_2, x_{i-1}) \\] Independence and Conditional Independence : Two variables X and Y are said to be indipendent if \\[ P(X=x, Y=y) = P(X=x) P(Y=y) \\] If both variables are conditioned on a third variable Z (i.e., P(X=x, Y=y | Z=z)), they are said to be conditionally independent if \\[ P(X=x, Y=y | Z=z) = P(X=x | Z=z) P(Y=y| Z=z) \\] Mean (or Expectation) : Given a function \\(f(x)\\) where \\(x\\) is a stochastic variable with probability \\(P(x)\\) , its average or mean value is defined as follows for the discrete case: \\[ \\mu = E_{x \\sim P} [f(x)] = \\sum_x P(x) f(x) \\] and for the continuos case \\[ \\mu = E_{x \\sim p} [f(x)] = \\int p(x) f(x) dx \\] In most Machine Learning applications we do not have knowledge of the full distribution to evaluate the mean, rather we have access to N equi-probable samples that we assume are drawn from the underlying distribution. We can approximate the mean via the Sample Mean : \\[ \\mu \\approx \\sum_i \\frac{1}{N} f(x_i) \\] Variance (and Covariance) :","title":"Probability"},{"location":"lectures/2_prob/#information-theory","text":"","title":"Information theory"},{"location":"lectures/3_gradopt/","text":"Gradient-based optimization After reviewing some of the basic concepts of linear algebra that we will be using during this course, we are now in a position to start our journey in the field of learning algorithms . Any learning algorithm, no matter its level of complexity, is composed of 4 key elements: Dataset : a collection of many examples (sometimes referred to as samples of data points) that represents the experience we wish our machine learning algorithm to learn from. More speficically, the dataset is defined as: $$ \\mathbf{x} = [x_1, x_2, ..., x_{N_f}]^T \\quad \\mathbf{X} = [\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(N_s)}] $$ and $$ \\mathbf{y} = [y_1, y_2, ..., y_{N_t}]^T \\quad \\mathbf{Y} = [\\mathbf{y}^{(1)}, \\mathbf{y}^{(2)}, ..., \\mathbf{y}^{(N_s)}] $$ where \\(N_f\\) and \\(N_t\\) are the number of features and targets for each sample in the dataset, respectively, and \\(N_s\\) is the number of samples. Model : a mathematical relation between the input (or features) and output (or target) or our dataset. It is generally parametrized as function \\(f\\) of a number of free parameters \\(\\theta\\) which we want the learning algorithm to estimate given a task and a measure of performance, and we write it as $$ \\mathbf{y} = f_\\theta(\\mathbf{x}) $$ Loss (and cost) function : quantitative measure of the performance of the learning algorithm, which we wish to minimize (or maximize) in order to make accurate predictions on unseen data. It is written as $$ J = \\sum_{i=1}^{N_s} \\mathscr{L} (\\mathbf{y}^{(i)}, f(\\mathbf{x}^{(i)})) $$ where \\(\\mathscr{L}\\) is the loss function for each input-output pair and \\(J\\) is the overall cost function. Optimization algorithm : mathematical method that modifies the aims to drive down (up) the cost function by modifying its free-parameters \\(\\theta\\) : $$ \\hat{\\theta} = \\underset{\\theta} {\\mathrm{argmin}} J $$ Optimization algorithms are generally divided into two main families: gradient-based (or local) and gradient-free (or global). Gradient-based optimization is by far the most popular way to train NNs and will be discussed in more details below. Gradient- and steepest-descent algorithms The simplest of gradient-based methods is the so-called Gradient-descent algorithm. As the name implies, this algorithm uses local gradient information of the functional to minimize/maximize to move towards its global mimimum/maximum as depicted in the figure below. More formally, given a functional \\(f_\\theta\\) and its gradient \\(\\nabla f = \\frac{\\delta f}{\\delta \\theta}\\) , the (minimization) algorithm can be written as: Initialization: choose \\(\\theta \\in \\mathbb{R}\\) For \\(i=0,...N-1\\) ; Compute update direction \\(d_i = -\\nabla f |_{\\theta_i}\\) Estimate step-lenght \\(\\alpha_i\\) Update \\(\\theta_{i+1} = \\theta_{i} + \\alpha_i d_i\\) Note that the maximization version of this algorithm simply switches the sign in the update direction (first equation of the algorithm). Moreover, the proposed algorithm can be easily extended to N-dimensional model vectors \\(\\theta=[\\theta_1, \\theta_2, ..., \\theta_N]\\) by defining the following gradient vector \\(\\nabla f=[\\delta f / \\delta\\theta_1, \\delta f / \\delta\\theta_2, ..., \\delta f/ \\delta\\theta_N]\\) . Step lenght selection The choice of the step-lenght has tremendous impact on the performance of the algorithm and its ability to converge fast (i.e., in a small number of iterations) to the optimal solution. The most used selection rules are: Constant: the step size is fixed to a constant value \\(\\alpha_i=\\hat{\\alpha}\\) . This is the most common situation that we will encounter when training neural networks. In practice, some adaptive schemes based on the evolution of the train (or validation) norm are generally adopted, but we will still refer to this case as costant step size; Exact linesearch: at each iteration, \\(\\alpha_i\\) is chosen such that it minimizes \\(f(\\theta_{i} + \\alpha_i d_i)\\) . This is the most commonly used approach when dealing with linear systems of equations. Backtracking \"Armijo\" linesearch: at each iteration, given a parameter \\(\\mu \\in (0,1)\\) , start with \\(\\alpha_i=1\\) and reduce it by a factor of 2 until the following condition is satisfied: \\(f(\\theta_i) - f(\\theta_{i} + \\alpha_i d_i) \\ge \\mu \\alpha_i \\nabla f^T d_i\\) Second-order optimization Up until now we have discussed first-order optimization techniques that rely on the ability to evaluate the function \\(f\\) and its gradient \\(\\nabla f\\) . Second-order optimization method go one step beyond in that they use information from both the local slope and curvature of the function f. When a function has small curvature, the function and its tangent line are very similar: the gradient alone is therefore able to provide a good local approximation of the function (i.e., \\(f(\\theta+\\delta \\theta)\\approx f(\\theta) + \\nabla f \\delta \\theta\\) ). On the other hand, if the curvature of the function of large, the function and its tangent line start to differ very quickly away from the linearization point. The gradient alone is not able anymore to provide a good local approximation of the function (i.e., \\(f(\\theta+\\delta \\theta)\\approx f(\\theta) + \\nabla f \\delta \\theta + \\nabla^2 f \\delta \\theta^2\\) ). Let's start again from the one-dimensional case and the well-known Newton's method . This method is generally employed to find the zeros of a function: \\(x: f(x)=0\\) and can be written as: \\[ x_{i+1} = x_i - \\frac{f(x)|_{x_i}}{f'(x)|_{x_i}} \\] which can be easily derived from the Taylor expansion of \\(f(x)\\) around \\(x_{i+1}\\) . If we remember that finding the minimum (or maximum) of a function is equivalent to find the zeros of its first derivative ( \\(x: min_x f(x) \\leftrightarrow x: f'(x)=0\\) ), the Netwon's method can be written as: \\[ x_{i+1} = x_i - \\frac{f'(x)|_{x_i}}{f''(x)|_{x_i}} \\] This is the so-called Gauss-Netwon method . In order to be able to discuss second-order optimization algorithms for the multi-dimensional case, let's first introduce the notion of Jacobian : \\[\\mathbf{y} = f(\\mathbf{x}) \\rightarrow \\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial f_N}{\\partial x_1} & \\frac{\\partial f_N}{\\partial x_2} & ... & \\frac{\\partial f_N}{\\partial x_M} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[N \\times M]} \\] Through the notion of Jacobian, we can define the Hessian as the Jacobian of the gradient vector \\[\\mathbf{H} = \\nabla (\\nabla f) = \\begin{bmatrix} \\frac{\\partial f^2}{\\partial x_1^2} & \\frac{\\partial f^2}{\\partial x_1 \\partial x_2 } & ... & \\frac{\\partial f^2}{\\partial x_1\\partial x_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial f^2}{\\partial x_M \\partial x_1} & \\frac{\\partial f^2}{\\partial x_M \\partial x_2} & ... & \\frac{\\partial f^2}{\\partial x_M^2} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[M \\times M]} \\] where we note that when \\(f\\) is continuous, \\(\\partial / \\partial x_i \\partial x_j = \\partial / \\partial x_j \\partial x_i\\) , and \\(\\mathbf{H}\\) is symmetric. The Gauss-Newton method can be written as: \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_i - \\mathbf{H}^{-1}\\nabla f \\] Approximated version of the Gauss-Netwon method have been developed over the years, mostly based on the idea that inverting \\(\\mathbf{H}\\) is sometimes a prohibitive task. Such methods, generally referred to as Quasi-Netwon methods attempt to approximate the Hessian (or its inverse) using the gradient at the current iteration and that of a number of previous iterations. BFGS or its limited memory version L-BFGS are examples of such a kind. Due to their computational cost (as well as the lack of solid theories for their use in conjunction with approximate gradients), these methods are not yet commonly used by the machine learning community to optimize the parameters of NNs in deep learning. Stochastic-gradient descent !!HERE!! Finally, I encourage everyone to read the following blog post for a more detailed overview of the optimization algorithms discussed here. Note that we will also look more into some of the recent optimization algorithms that overcome some of the limitations of standard SGD in this lecture .","title":"Gradient-based optimization"},{"location":"lectures/3_gradopt/#gradient-based-optimization","text":"After reviewing some of the basic concepts of linear algebra that we will be using during this course, we are now in a position to start our journey in the field of learning algorithms . Any learning algorithm, no matter its level of complexity, is composed of 4 key elements: Dataset : a collection of many examples (sometimes referred to as samples of data points) that represents the experience we wish our machine learning algorithm to learn from. More speficically, the dataset is defined as: $$ \\mathbf{x} = [x_1, x_2, ..., x_{N_f}]^T \\quad \\mathbf{X} = [\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(N_s)}] $$ and $$ \\mathbf{y} = [y_1, y_2, ..., y_{N_t}]^T \\quad \\mathbf{Y} = [\\mathbf{y}^{(1)}, \\mathbf{y}^{(2)}, ..., \\mathbf{y}^{(N_s)}] $$ where \\(N_f\\) and \\(N_t\\) are the number of features and targets for each sample in the dataset, respectively, and \\(N_s\\) is the number of samples. Model : a mathematical relation between the input (or features) and output (or target) or our dataset. It is generally parametrized as function \\(f\\) of a number of free parameters \\(\\theta\\) which we want the learning algorithm to estimate given a task and a measure of performance, and we write it as $$ \\mathbf{y} = f_\\theta(\\mathbf{x}) $$ Loss (and cost) function : quantitative measure of the performance of the learning algorithm, which we wish to minimize (or maximize) in order to make accurate predictions on unseen data. It is written as $$ J = \\sum_{i=1}^{N_s} \\mathscr{L} (\\mathbf{y}^{(i)}, f(\\mathbf{x}^{(i)})) $$ where \\(\\mathscr{L}\\) is the loss function for each input-output pair and \\(J\\) is the overall cost function. Optimization algorithm : mathematical method that modifies the aims to drive down (up) the cost function by modifying its free-parameters \\(\\theta\\) : $$ \\hat{\\theta} = \\underset{\\theta} {\\mathrm{argmin}} J $$ Optimization algorithms are generally divided into two main families: gradient-based (or local) and gradient-free (or global). Gradient-based optimization is by far the most popular way to train NNs and will be discussed in more details below.","title":"Gradient-based optimization"},{"location":"lectures/3_gradopt/#gradient-and-steepest-descent-algorithms","text":"The simplest of gradient-based methods is the so-called Gradient-descent algorithm. As the name implies, this algorithm uses local gradient information of the functional to minimize/maximize to move towards its global mimimum/maximum as depicted in the figure below. More formally, given a functional \\(f_\\theta\\) and its gradient \\(\\nabla f = \\frac{\\delta f}{\\delta \\theta}\\) , the (minimization) algorithm can be written as: Initialization: choose \\(\\theta \\in \\mathbb{R}\\) For \\(i=0,...N-1\\) ; Compute update direction \\(d_i = -\\nabla f |_{\\theta_i}\\) Estimate step-lenght \\(\\alpha_i\\) Update \\(\\theta_{i+1} = \\theta_{i} + \\alpha_i d_i\\) Note that the maximization version of this algorithm simply switches the sign in the update direction (first equation of the algorithm). Moreover, the proposed algorithm can be easily extended to N-dimensional model vectors \\(\\theta=[\\theta_1, \\theta_2, ..., \\theta_N]\\) by defining the following gradient vector \\(\\nabla f=[\\delta f / \\delta\\theta_1, \\delta f / \\delta\\theta_2, ..., \\delta f/ \\delta\\theta_N]\\) .","title":"Gradient- and steepest-descent algorithms"},{"location":"lectures/3_gradopt/#step-lenght-selection","text":"The choice of the step-lenght has tremendous impact on the performance of the algorithm and its ability to converge fast (i.e., in a small number of iterations) to the optimal solution. The most used selection rules are: Constant: the step size is fixed to a constant value \\(\\alpha_i=\\hat{\\alpha}\\) . This is the most common situation that we will encounter when training neural networks. In practice, some adaptive schemes based on the evolution of the train (or validation) norm are generally adopted, but we will still refer to this case as costant step size; Exact linesearch: at each iteration, \\(\\alpha_i\\) is chosen such that it minimizes \\(f(\\theta_{i} + \\alpha_i d_i)\\) . This is the most commonly used approach when dealing with linear systems of equations. Backtracking \"Armijo\" linesearch: at each iteration, given a parameter \\(\\mu \\in (0,1)\\) , start with \\(\\alpha_i=1\\) and reduce it by a factor of 2 until the following condition is satisfied: \\(f(\\theta_i) - f(\\theta_{i} + \\alpha_i d_i) \\ge \\mu \\alpha_i \\nabla f^T d_i\\)","title":"Step lenght selection"},{"location":"lectures/3_gradopt/#second-order-optimization","text":"Up until now we have discussed first-order optimization techniques that rely on the ability to evaluate the function \\(f\\) and its gradient \\(\\nabla f\\) . Second-order optimization method go one step beyond in that they use information from both the local slope and curvature of the function f. When a function has small curvature, the function and its tangent line are very similar: the gradient alone is therefore able to provide a good local approximation of the function (i.e., \\(f(\\theta+\\delta \\theta)\\approx f(\\theta) + \\nabla f \\delta \\theta\\) ). On the other hand, if the curvature of the function of large, the function and its tangent line start to differ very quickly away from the linearization point. The gradient alone is not able anymore to provide a good local approximation of the function (i.e., \\(f(\\theta+\\delta \\theta)\\approx f(\\theta) + \\nabla f \\delta \\theta + \\nabla^2 f \\delta \\theta^2\\) ). Let's start again from the one-dimensional case and the well-known Newton's method . This method is generally employed to find the zeros of a function: \\(x: f(x)=0\\) and can be written as: \\[ x_{i+1} = x_i - \\frac{f(x)|_{x_i}}{f'(x)|_{x_i}} \\] which can be easily derived from the Taylor expansion of \\(f(x)\\) around \\(x_{i+1}\\) . If we remember that finding the minimum (or maximum) of a function is equivalent to find the zeros of its first derivative ( \\(x: min_x f(x) \\leftrightarrow x: f'(x)=0\\) ), the Netwon's method can be written as: \\[ x_{i+1} = x_i - \\frac{f'(x)|_{x_i}}{f''(x)|_{x_i}} \\] This is the so-called Gauss-Netwon method . In order to be able to discuss second-order optimization algorithms for the multi-dimensional case, let's first introduce the notion of Jacobian : \\[\\mathbf{y} = f(\\mathbf{x}) \\rightarrow \\mathbf{J} = \\begin{bmatrix} \\frac{\\partial f_1}{\\partial x_1} & \\frac{\\partial f_1}{\\partial x_2} & ... & \\frac{\\partial f_1}{\\partial x_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial f_N}{\\partial x_1} & \\frac{\\partial f_N}{\\partial x_2} & ... & \\frac{\\partial f_N}{\\partial x_M} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[N \\times M]} \\] Through the notion of Jacobian, we can define the Hessian as the Jacobian of the gradient vector \\[\\mathbf{H} = \\nabla (\\nabla f) = \\begin{bmatrix} \\frac{\\partial f^2}{\\partial x_1^2} & \\frac{\\partial f^2}{\\partial x_1 \\partial x_2 } & ... & \\frac{\\partial f^2}{\\partial x_1\\partial x_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial f^2}{\\partial x_M \\partial x_1} & \\frac{\\partial f^2}{\\partial x_M \\partial x_2} & ... & \\frac{\\partial f^2}{\\partial x_M^2} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[M \\times M]} \\] where we note that when \\(f\\) is continuous, \\(\\partial / \\partial x_i \\partial x_j = \\partial / \\partial x_j \\partial x_i\\) , and \\(\\mathbf{H}\\) is symmetric. The Gauss-Newton method can be written as: \\[ \\mathbf{x}_{i+1} = \\mathbf{x}_i - \\mathbf{H}^{-1}\\nabla f \\] Approximated version of the Gauss-Netwon method have been developed over the years, mostly based on the idea that inverting \\(\\mathbf{H}\\) is sometimes a prohibitive task. Such methods, generally referred to as Quasi-Netwon methods attempt to approximate the Hessian (or its inverse) using the gradient at the current iteration and that of a number of previous iterations. BFGS or its limited memory version L-BFGS are examples of such a kind. Due to their computational cost (as well as the lack of solid theories for their use in conjunction with approximate gradients), these methods are not yet commonly used by the machine learning community to optimize the parameters of NNs in deep learning.","title":"Second-order optimization"},{"location":"lectures/3_gradopt/#stochastic-gradient-descent","text":"!!HERE!! Finally, I encourage everyone to read the following blog post for a more detailed overview of the optimization algorithms discussed here. Note that we will also look more into some of the recent optimization algorithms that overcome some of the limitations of standard SGD in this lecture .","title":"Stochastic-gradient descent"},{"location":"lectures/4_autoencoder/","text":"Autoencoders At different stages of this course we have seen the importance of choosing a good set of input features when solving a machine learning problem. We have also discussed how, whilst this may require a great deal of human time and e\ufb00ort to find as it varies from problem to problem, Neural Networks have gained popularity because of their ability to find good representations from raw data (e.g., images). The problem of discovering a good representation directly from an input dataset is known as representation learning . The quintessential example of a representation learning algorithm is the Autoencoder . An autoencoder is the combination of an encoder function \\(e_\\theta\\) , which converts the input data into a di\ufb00erent representation, and a decoder function \\(d_\\theta\\) , which converts the new representation back into the original format. ...","title":"Autoencoders"},{"location":"lectures/4_autoencoder/#autoencoders","text":"At different stages of this course we have seen the importance of choosing a good set of input features when solving a machine learning problem. We have also discussed how, whilst this may require a great deal of human time and e\ufb00ort to find as it varies from problem to problem, Neural Networks have gained popularity because of their ability to find good representations from raw data (e.g., images). The problem of discovering a good representation directly from an input dataset is known as representation learning . The quintessential example of a representation learning algorithm is the Autoencoder . An autoencoder is the combination of an encoder function \\(e_\\theta\\) , which converts the input data into a di\ufb00erent representation, and a decoder function \\(d_\\theta\\) , which converts the new representation back into the original format. ...","title":"Autoencoders"},{"location":"lectures/4_linreg/","text":"Linear and Logistic Regression Lin regr: Log regr: derivative derivation and implementation, look here https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17 Explain that analytic expression for gradient isnt always possible, nor the most efficient... we will show later a more general approach to it. Mention that in next class we will understand why we choose such cost functions","title":"Linear and Logistic Regression"},{"location":"lectures/4_linreg/#linear-and-logistic-regression","text":"Lin regr: Log regr: derivative derivation and implementation, look here https://towardsdatascience.com/logistic-regression-from-scratch-69db4f587e17 Explain that analytic expression for gradient isnt always possible, nor the most efficient... we will show later a more general approach to it. Mention that in next class we will understand why we choose such cost functions","title":"Linear and Logistic Regression"},{"location":"lectures/4_pca/","text":"PCA Use material in Linear Algebra page45","title":"PCA"},{"location":"lectures/4_pca/#pca","text":"Use material in Linear Algebra page45","title":"PCA"},{"location":"lectures/5_bestpractice/","text":"Best practice in Machine Learning Generalize the loss functions from LinReg/LogReg, Linreg/Reg:MSE, Logreg/Class:softmax, Class multi see 5.5.1 and 6.2.2.1 and 6.2.2.2, 6.2.2.3 - DONE!! Pretty much all from Generalization. Follow chapter 5.2 for capacity, over-underfitting, Regularization, hyperparams etc. and also the course Also need to talk about Performance meausure, TP, FP, Accuracy, Precision etc... look at chapter 5 performance section and nice blog post https://yanndubs.github.io/machine-learning-glossary/","title":"Best practice in Machine Learning"},{"location":"lectures/5_bestpractice/#best-practice-in-machine-learning","text":"Generalize the loss functions from LinReg/LogReg, Linreg/Reg:MSE, Logreg/Class:softmax, Class multi see 5.5.1 and 6.2.2.1 and 6.2.2.2, 6.2.2.3 - DONE!! Pretty much all from Generalization. Follow chapter 5.2 for capacity, over-underfitting, Regularization, hyperparams etc. and also the course Also need to talk about Performance meausure, TP, FP, Accuracy, Precision etc... look at chapter 5 performance section and nice blog post https://yanndubs.github.io/machine-learning-glossary/","title":"Best practice in Machine Learning"},{"location":"lectures/6_nn/","text":"Basics of Neural Networks Perceptron N Perceptrons = 1layer Activation functions for hidden units (recap on those seen already + tanh & relu, leaky relu, cos, softplus, RBF, softmax section 6.3)... Teach p168 this to show the importance of activation and give assigment later on! Also teach the low-rank example p.192 and ask in the example where it makes sense to have two layers without activation in the middle","title":"Basics of Neural Networks"},{"location":"lectures/6_nn/#basics-of-neural-networks","text":"Perceptron N Perceptrons = 1layer Activation functions for hidden units (recap on those seen already + tanh & relu, leaky relu, cos, softplus, RBF, softmax section 6.3)... Teach p168 this to show the importance of activation and give assigment later on! Also teach the low-rank example p.192 and ask in the example where it makes sense to have two layers without activation in the middle","title":"Basics of Neural Networks"},{"location":"lectures/7_nn1/","text":"More on Neural networks ARCHITECTURE MLP : Architecture design section 6.4 Universal approx theorem and trade-off between shallow with many units in the layers and deep with fewer units. section 6.5 \"\"The universal approximation theorem means that regardless of what functionwe are trying to learn, we know that a large MLP will be able to represent thisfunction. We are not guaranteed, however, that the training algorithm will beable to learn that function. Even if the MLP is able to represent the function, learning can fail for two di\ufb00erent reasons. First, the optimization algorithm usedfor training may not be able to \ufb01nd the value of the parameters that correspondsto the desired function. Second, the training algorithm might choose the wrong function as a result of over\ufb01tting. ... In summary, a feedforward network with a single layer is su\ufb03cient to representany function, but the layer may be infeasibly large and may fail to learn andgeneralize correctly. In many circumstances, using deeper models can reduce thenumber of units required to represent the desired function and can reduce theamount of generalization error\" BACKPROP Back-prop section 6.5... mention that this is not just ML, can be used to derive derivative of any function thruough its computation graph and it is part of more general field called AD Automatic differentiation (6.5.9) General idea plus example for fully-connected MLP (see coursera videos... simpler) More details on back-prop (optional): read 6.5.6 INITIALIZATION Good link https://medium.com/@safrin1128/weight-initialization-in-neural-network-inspired-by-andrew-ng-e0066dc4a566 https://www.deeplearning.ai/ai-notes/initialization/ WHY NN/DEEP LEARNING TOOK OFF Finally lets remark why theories are all from '80 but until early 2000 NN were not popular (niche field) \"The core ideas behind modern feedforward networks have not changed sub-stantially since the 1980s. The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of the improvement in neuralnetwork performance from 1986 to 2015 can be attributed to two factors. First,larger datasets have reduced the degree to which statistical generalization is achallenge for neural networks. Second, neural networks have become much larger,because of more powerful computers and better software infrastructure\" Plus a few algorithmic changes: - \"One of these algorithmic changes was the replacement of mean squared errorwith the cross-entropy family of loss functions. Mean squared error was popular inthe 1980s and 1990s but was gradually replaced by cross-entropy losses and the principle of maximum likelihood as ideas spread between the statistics community and the machine learning community. The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, whichhad previously Su\ufb00ered from saturation and slow learning when using the meansquared error loss\" --> NOTE for regression ML with gaussianity assumption on p(y|x) is still MSE, but only for this edge case! \"change that has greatly improved the performanceof feedforward networks was the replacement of sigmoid hidden units with piecewiselinear hidden units, such as recti\ufb01ed linear units. Recti\ufb01cation using themax{0, z}function was introduced in early neural network models and dates back at least as faras the cognitron and neocognitron (Fukushima, 1975, 1980)... This began to change in about 2009. Jarrett et al. (2009)observed that \u201cusing a rectifying nonlinearity is the single most important factorin improving the performance of a recognition system,\u201d --> LEARN TO CHALLANGE STATUS QUO, SOMETIMES UNDERSTANDING OF PROBLEMS CAN CHANGE OR NEW EXTERNAL FACTORS (EG MORE DATA) MAKE SOMETHING THAT WAS WORSE BECOME BETTER... SIMILAR STORY IN FWI FOR GEOPHYSCISTS Also add mixture density 'network' (example of petroelastic with facies - Andrews paper) - https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca","title":"More on Neural networks"},{"location":"lectures/7_nn1/#more-on-neural-networks","text":"","title":"More on Neural networks"},{"location":"lectures/7_nn1/#architecture","text":"MLP : Architecture design section 6.4 Universal approx theorem and trade-off between shallow with many units in the layers and deep with fewer units. section 6.5 \"\"The universal approximation theorem means that regardless of what functionwe are trying to learn, we know that a large MLP will be able to represent thisfunction. We are not guaranteed, however, that the training algorithm will beable to learn that function. Even if the MLP is able to represent the function, learning can fail for two di\ufb00erent reasons. First, the optimization algorithm usedfor training may not be able to \ufb01nd the value of the parameters that correspondsto the desired function. Second, the training algorithm might choose the wrong function as a result of over\ufb01tting. ... In summary, a feedforward network with a single layer is su\ufb03cient to representany function, but the layer may be infeasibly large and may fail to learn andgeneralize correctly. In many circumstances, using deeper models can reduce thenumber of units required to represent the desired function and can reduce theamount of generalization error\"","title":"ARCHITECTURE"},{"location":"lectures/7_nn1/#backprop","text":"Back-prop section 6.5... mention that this is not just ML, can be used to derive derivative of any function thruough its computation graph and it is part of more general field called AD Automatic differentiation (6.5.9) General idea plus example for fully-connected MLP (see coursera videos... simpler) More details on back-prop (optional): read 6.5.6","title":"BACKPROP"},{"location":"lectures/7_nn1/#initialization","text":"Good link https://medium.com/@safrin1128/weight-initialization-in-neural-network-inspired-by-andrew-ng-e0066dc4a566 https://www.deeplearning.ai/ai-notes/initialization/","title":"INITIALIZATION"},{"location":"lectures/7_nn1/#why-nndeep-learning-took-off","text":"Finally lets remark why theories are all from '80 but until early 2000 NN were not popular (niche field) \"The core ideas behind modern feedforward networks have not changed sub-stantially since the 1980s. The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of the improvement in neuralnetwork performance from 1986 to 2015 can be attributed to two factors. First,larger datasets have reduced the degree to which statistical generalization is achallenge for neural networks. Second, neural networks have become much larger,because of more powerful computers and better software infrastructure\" Plus a few algorithmic changes: - \"One of these algorithmic changes was the replacement of mean squared errorwith the cross-entropy family of loss functions. Mean squared error was popular inthe 1980s and 1990s but was gradually replaced by cross-entropy losses and the principle of maximum likelihood as ideas spread between the statistics community and the machine learning community. The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, whichhad previously Su\ufb00ered from saturation and slow learning when using the meansquared error loss\" --> NOTE for regression ML with gaussianity assumption on p(y|x) is still MSE, but only for this edge case! \"change that has greatly improved the performanceof feedforward networks was the replacement of sigmoid hidden units with piecewiselinear hidden units, such as recti\ufb01ed linear units. Recti\ufb01cation using themax{0, z}function was introduced in early neural network models and dates back at least as faras the cognitron and neocognitron (Fukushima, 1975, 1980)... This began to change in about 2009. Jarrett et al. (2009)observed that \u201cusing a rectifying nonlinearity is the single most important factorin improving the performance of a recognition system,\u201d --> LEARN TO CHALLANGE STATUS QUO, SOMETIMES UNDERSTANDING OF PROBLEMS CAN CHANGE OR NEW EXTERNAL FACTORS (EG MORE DATA) MAKE SOMETHING THAT WAS WORSE BECOME BETTER... SIMILAR STORY IN FWI FOR GEOPHYSCISTS Also add mixture density 'network' (example of petroelastic with facies - Andrews paper) - https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca","title":"WHY NN/DEEP LEARNING TOOK OFF"}]}