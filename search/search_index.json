{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Homepage This course covers the fundamentals of machine learning, its applications to geoscientific problems, and it provides basic best practices for the rigorous development and evaluation of machine learning models. The main focus of the course is on describing the fundamental theory of linear regression , logistic regression , neural networks , convolutional neural networks , sequence modelling , dimensionality reduction , generative modelling , and physics-inspired neural networks . Students will also be introduced to practical applications in geoscience for each of the presented methods; lab sessions will be held using the PyTorch computational framework in the Python programming language. Lectures Tuesday, and Thursday, 10:15am - 11:45am Teaching Staff Instructor: Matteo Ravasi - Office Hours: Tuesday 4pm to 5pm (by Appointment: Zoom or Office - BI-1432) Textbook Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville \u2013 MIT Press. Pre-requisites Knowledge of calculus, linear algebra ad statistics is required. Basic Python knowledge is preferred. Course Requirements ErSE 213 - Inverse problems","title":"Homepage"},{"location":"#homepage","text":"This course covers the fundamentals of machine learning, its applications to geoscientific problems, and it provides basic best practices for the rigorous development and evaluation of machine learning models. The main focus of the course is on describing the fundamental theory of linear regression , logistic regression , neural networks , convolutional neural networks , sequence modelling , dimensionality reduction , generative modelling , and physics-inspired neural networks . Students will also be introduced to practical applications in geoscience for each of the presented methods; lab sessions will be held using the PyTorch computational framework in the Python programming language.","title":"Homepage"},{"location":"#lectures","text":"Tuesday, and Thursday, 10:15am - 11:45am","title":"Lectures"},{"location":"#teaching-staff","text":"Instructor: Matteo Ravasi - Office Hours: Tuesday 4pm to 5pm (by Appointment: Zoom or Office - BI-1432)","title":"Teaching Staff"},{"location":"#textbook","text":"Deep Learning by Ian Goodfellow and Yoshua Bengio and Aaron Courville \u2013 MIT Press.","title":"Textbook"},{"location":"#pre-requisites","text":"Knowledge of calculus, linear algebra ad statistics is required. Basic Python knowledge is preferred.","title":"Pre-requisites"},{"location":"#course-requirements","text":"ErSE 213 - Inverse problems","title":"Course Requirements"},{"location":"gradind/","text":"Grading system The final grade will be obtained as the combination of the following: 50.00% - Course Project 30.00% - Midterm exam 20.00% - Homeworks Homeworks Homeworks will be assigned at the end of each topic. They consist of both pen and paper questions and programming exercises. The submitted codes must be properly commented and implementation choices must be justified (this is as important as the code itself and counts towards the final mark). Project The project should cover one of the topics learned in this course. It could be focused on implementing a novel machine learning algorithm to a geoscientific problem or on performing a systematic comparison of different machine learning algorithms to a geoscientific dataset. Students are encouraged to start the project early. The best way is to define a problem statement at the beginning of the term and learn how to use machine learning to solve such a problem during the course. Collaboration Most homeworks involve programming assignments. Students are encouraged to collaborate and consult with each other, but an individual assignments (and code) must be handed in. Acknowledge explicitly in your submitted assignment if you have collaborated with someone else while working on the assignment. Late submissions Each student has access to one late submission wildcard of no more than 2 days from the submission deadline. Apart from using this wildcard, late submissions will be penalized with a loss of 40% of the achieved score.","title":"Grading system"},{"location":"gradind/#grading-system","text":"The final grade will be obtained as the combination of the following: 50.00% - Course Project 30.00% - Midterm exam 20.00% - Homeworks","title":"Grading system"},{"location":"gradind/#homeworks","text":"Homeworks will be assigned at the end of each topic. They consist of both pen and paper questions and programming exercises. The submitted codes must be properly commented and implementation choices must be justified (this is as important as the code itself and counts towards the final mark).","title":"Homeworks"},{"location":"gradind/#project","text":"The project should cover one of the topics learned in this course. It could be focused on implementing a novel machine learning algorithm to a geoscientific problem or on performing a systematic comparison of different machine learning algorithms to a geoscientific dataset. Students are encouraged to start the project early. The best way is to define a problem statement at the beginning of the term and learn how to use machine learning to solve such a problem during the course.","title":"Project"},{"location":"gradind/#collaboration","text":"Most homeworks involve programming assignments. Students are encouraged to collaborate and consult with each other, but an individual assignments (and code) must be handed in. Acknowledge explicitly in your submitted assignment if you have collaborated with someone else while working on the assignment.","title":"Collaboration"},{"location":"gradind/#late-submissions","text":"Each student has access to one late submission wildcard of no more than 2 days from the submission deadline. Apart from using this wildcard, late submissions will be penalized with a loss of 40% of the achieved score.","title":"Late submissions"},{"location":"schedule/","text":"Schedule Lecture Date Topic Exercise 1 XX Course overview and introduction to Machine Learning - 2 XX Linear algebra refresher - 2 XX Probability refresher - 3 XX Gradient-based optimization link 4 XX Linear and Logistic regression link 5 XX Neural Networks: perceptron, activation functions link1 link2 6 XX Neural Networks: backpropagation, initialization, and statistical interpretation - 7 XX Best practices in training Machine Learning models - 8 XX Advanced solvers: momentum, RMSProp, Adam, greedy training - 9 XX Mixture Density Networks - 10 XX Autoencoders - 11 XX More on gradient-based optimization - 12 XX Autoencoders - 13 XX More on gradient-based optimization - 14 XX Autoencoders -","title":"Schedule"},{"location":"schedule/#schedule","text":"Lecture Date Topic Exercise 1 XX Course overview and introduction to Machine Learning - 2 XX Linear algebra refresher - 2 XX Probability refresher - 3 XX Gradient-based optimization link 4 XX Linear and Logistic regression link 5 XX Neural Networks: perceptron, activation functions link1 link2 6 XX Neural Networks: backpropagation, initialization, and statistical interpretation - 7 XX Best practices in training Machine Learning models - 8 XX Advanced solvers: momentum, RMSProp, Adam, greedy training - 9 XX Mixture Density Networks - 10 XX Autoencoders - 11 XX More on gradient-based optimization - 12 XX Autoencoders - 13 XX More on gradient-based optimization - 14 XX Autoencoders -","title":"Schedule"},{"location":"lectures/10_gradopt1/","text":"More on gradient-based optimization XX","title":"More on gradient-based optimization"},{"location":"lectures/10_gradopt1/#more-on-gradient-based-optimization","text":"XX","title":"More on gradient-based optimization"},{"location":"lectures/11_vae/","text":"VAE References: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73 https://www.jeremyjordan.me/variational-autoencoders/ https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html","title":"VAE"},{"location":"lectures/11_vae/#vae","text":"References: https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73 https://www.jeremyjordan.me/variational-autoencoders/ https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html","title":"VAE"},{"location":"lectures/12_mdn/","text":"Mixture-density networks https://towardsdatascience.com/mixture-density-networks-probabilistic-regression-for-uncertainty-estimation-5f7250207431 iMR","title":"Mixture-density networks"},{"location":"lectures/12_mdn/#mixture-density-networks","text":"https://towardsdatascience.com/mixture-density-networks-probabilistic-regression-for-uncertainty-estimation-5f7250207431 iMR","title":"Mixture-density networks"},{"location":"lectures/1_intro/","text":"Introduction to Machine Learning Humans have long dreamed of creating machines that can think and act independently . For many years this has been the aim of Artificial Intelligence (AI) . In the early days of AI, many problems that are difficult to solve by humans (e.g., large summations or multiplications, solution of systems of equations) turn out to be easier for computers as long as humans could define a list of tasks that machines could perform at faster speed and higher precisions than humans can do themselves. On the other hand, tasks that are very easily solved by adult humans and even kids (e.g., recognizing animals in pictures or singing a song) turned out to be very difficult for computers. The main reason of such difficulties lies in the fact that humans cannot explain in words (and with a simple set of instructions) how they have learned to accomplish these tasks. This is where instead the second era of AI solutions, belonging to the field of Machine Learning (ML) , have shown astonishing results in the last decade. Instead of relying on hard-coded rules, these algorithms operate in a similar fashion to human beings as they learn from experience . In other words, given enough training data in the form of inputs (e.g., photos) and outputs (e.g., label of the animal present in the photo), ML algorithms can learn a complex nonlinear mapping between them such that they can infer the output from the input when provided with unseen inputs. A large variety of ML algorithms have been developed by the scientific community, ranging from the basic linear and logistic regression that we will see in our third lecture , decision tree-based statistical methods such as random forrest or gradient boosting , all the way to deep neural networks , which have recently shown to outperform previously developed algorithms in many fields (e.g., computer science, text analysis and speech recognition, seismic interpretation). This subfield has grown exponentially in the last few years and it is now referred to as Deep Learning and will be subject of most of our course. In short, Deep learning is a particular kind of machine learning that represent the world as a nested hierarchy of increasingly complicated concepts the more we move away from the input and towards the output of the associated computational graph. Whilst sharing the same underlying principle of learning from experience in the form of a training data , different algorithms presents their own strengths and limitations and a machine learning practitioner must make a careful judgment at any time depending on the problem to be solved. Terminology Machine Learning is divided into 3 main categories: Supervised Learning : learn a function that maps an input to an output ( \\(X \\rightarrow Y\\) ). Inputs are also referred to as features and outputs are called targets. In practice we have access to a number of training pairs \\(\\{ \\textbf{x}_i, \\textbf{y}_i \\} \\; i=1,..,N\\) and we learn \\(\\textbf{y}_i=f_\\theta(\\textbf{x}_i)\\) where \\(f_\\theta\\) is for example parametrized via a neural network. Two main applications of supervised learning are Classification : the target is discrete Regression : the target is continuous Unsupervised Learning : learn patterns from unlabelled data. These methods have been shown to be able to find compact internal representation of the manifold the input data belongs to. Such compact representations can become valuable input features for subsequent tasks of supervised learning. In the context of deep learning, unsupervised models may even attempt to estimate the entire probability distribution of the dataset or how to generate new, indipendent samples from such distribution. We will get into the mathematical details of these families of models in the second part of our course. Semi-supervised Learning : it lies in between the other other learning paradigms as it learns from some examples that include a target and some that do not. Input data can also come in 2 different types: Structured data : tables (e.g., databases) Unstructured data : images, audio, text, ... Examples of applications in geoscience are displayed in the figure below. A number of available data types in various geoscientific contexts is also displayed. History Finally, we take a brief look at the history of Deep Learning. This field has so far experienced three main waves of major development (and periods of success) interspersed by winters (or periods of disbelief): '40 - '50 : first learning algorithms heavily influenced by our understanding of the inner working of the human brain. Mostly linear models such as the McCulloch-Pitts neuron, the perceptron by Rosenblatt, and the adaptive linear element (ADALINE). The latter was trained on an algorithm very similar to Stochastic Gradient Descent (SGD). These models showed poor performance in learning complex functions (e.g., XOR) and led to a drop in popularity of the field. '80 - '90 : these years so the creation of the Multi Layer Perceptron (MLP), the neocognitron (the ancestor of the convolutional layer), the first deep neural networks (e.g., LeNet for MNIST classification), the first sequence-to-sequence networks and the LSTM layer. from 2010 till now : a major moment for the history of this field can be traced back to 2012, when a deep convolution neural network developed by Krizhevsky and co-authors won the ImageNet competition lowering the top-5 error rate from 26.1 percent (previous winning solution not based on a neural network) to 15.3 percent. Since then the field has exploded with advances both in terms of model architectures (AlexNet, VGG, ResNet, GoogleLeNet, ...) optimization algorithms (AdaGrad, RMSProp, Adam, ...), applications (computer vision, text analysis, speech recognition, ...). Moreover, recente developments in the area of unsupervised learning have led to the creation of dimensionality reduction and generative algorithms that can now outperform any state-of-the-art method that is not based on neural networks. If want to dig deeper into the history of this field, an interesting read can be found here .","title":"Introduction to Machine Learning"},{"location":"lectures/1_intro/#introduction-to-machine-learning","text":"Humans have long dreamed of creating machines that can think and act independently . For many years this has been the aim of Artificial Intelligence (AI) . In the early days of AI, many problems that are difficult to solve by humans (e.g., large summations or multiplications, solution of systems of equations) turn out to be easier for computers as long as humans could define a list of tasks that machines could perform at faster speed and higher precisions than humans can do themselves. On the other hand, tasks that are very easily solved by adult humans and even kids (e.g., recognizing animals in pictures or singing a song) turned out to be very difficult for computers. The main reason of such difficulties lies in the fact that humans cannot explain in words (and with a simple set of instructions) how they have learned to accomplish these tasks. This is where instead the second era of AI solutions, belonging to the field of Machine Learning (ML) , have shown astonishing results in the last decade. Instead of relying on hard-coded rules, these algorithms operate in a similar fashion to human beings as they learn from experience . In other words, given enough training data in the form of inputs (e.g., photos) and outputs (e.g., label of the animal present in the photo), ML algorithms can learn a complex nonlinear mapping between them such that they can infer the output from the input when provided with unseen inputs. A large variety of ML algorithms have been developed by the scientific community, ranging from the basic linear and logistic regression that we will see in our third lecture , decision tree-based statistical methods such as random forrest or gradient boosting , all the way to deep neural networks , which have recently shown to outperform previously developed algorithms in many fields (e.g., computer science, text analysis and speech recognition, seismic interpretation). This subfield has grown exponentially in the last few years and it is now referred to as Deep Learning and will be subject of most of our course. In short, Deep learning is a particular kind of machine learning that represent the world as a nested hierarchy of increasingly complicated concepts the more we move away from the input and towards the output of the associated computational graph. Whilst sharing the same underlying principle of learning from experience in the form of a training data , different algorithms presents their own strengths and limitations and a machine learning practitioner must make a careful judgment at any time depending on the problem to be solved.","title":"Introduction to Machine Learning"},{"location":"lectures/1_intro/#terminology","text":"Machine Learning is divided into 3 main categories: Supervised Learning : learn a function that maps an input to an output ( \\(X \\rightarrow Y\\) ). Inputs are also referred to as features and outputs are called targets. In practice we have access to a number of training pairs \\(\\{ \\textbf{x}_i, \\textbf{y}_i \\} \\; i=1,..,N\\) and we learn \\(\\textbf{y}_i=f_\\theta(\\textbf{x}_i)\\) where \\(f_\\theta\\) is for example parametrized via a neural network. Two main applications of supervised learning are Classification : the target is discrete Regression : the target is continuous Unsupervised Learning : learn patterns from unlabelled data. These methods have been shown to be able to find compact internal representation of the manifold the input data belongs to. Such compact representations can become valuable input features for subsequent tasks of supervised learning. In the context of deep learning, unsupervised models may even attempt to estimate the entire probability distribution of the dataset or how to generate new, indipendent samples from such distribution. We will get into the mathematical details of these families of models in the second part of our course. Semi-supervised Learning : it lies in between the other other learning paradigms as it learns from some examples that include a target and some that do not. Input data can also come in 2 different types: Structured data : tables (e.g., databases) Unstructured data : images, audio, text, ... Examples of applications in geoscience are displayed in the figure below. A number of available data types in various geoscientific contexts is also displayed.","title":"Terminology"},{"location":"lectures/1_intro/#history","text":"Finally, we take a brief look at the history of Deep Learning. This field has so far experienced three main waves of major development (and periods of success) interspersed by winters (or periods of disbelief): '40 - '50 : first learning algorithms heavily influenced by our understanding of the inner working of the human brain. Mostly linear models such as the McCulloch-Pitts neuron, the perceptron by Rosenblatt, and the adaptive linear element (ADALINE). The latter was trained on an algorithm very similar to Stochastic Gradient Descent (SGD). These models showed poor performance in learning complex functions (e.g., XOR) and led to a drop in popularity of the field. '80 - '90 : these years so the creation of the Multi Layer Perceptron (MLP), the neocognitron (the ancestor of the convolutional layer), the first deep neural networks (e.g., LeNet for MNIST classification), the first sequence-to-sequence networks and the LSTM layer. from 2010 till now : a major moment for the history of this field can be traced back to 2012, when a deep convolution neural network developed by Krizhevsky and co-authors won the ImageNet competition lowering the top-5 error rate from 26.1 percent (previous winning solution not based on a neural network) to 15.3 percent. Since then the field has exploded with advances both in terms of model architectures (AlexNet, VGG, ResNet, GoogleLeNet, ...) optimization algorithms (AdaGrad, RMSProp, Adam, ...), applications (computer vision, text analysis, speech recognition, ...). Moreover, recente developments in the area of unsupervised learning have led to the creation of dimensionality reduction and generative algorithms that can now outperform any state-of-the-art method that is not based on neural networks. If want to dig deeper into the history of this field, an interesting read can be found here .","title":"History"},{"location":"lectures/2_linalg/","text":"Linear Algebra refresher In this lecture we will go through some of the key concepts of linear algebra and inverse problem theory that are required to develop the theories of the different machine learning algorithm presented in this course. This is not meant to be an exhaustive treatise and students are strongly advised to take the ErSE 213 - Inverse Problems prior to this course. Three key mathematical objects arise in the study of linear algebra: Scalars : \\(a \\in \\mathbb{R}\\) , a single number represented by a lower case italic letter; Vectors : \\(\\mathbf{x} = [x_1, x_2, ..., x_N]^T \\in \\mathbb{R}^N\\) , ordered collection of \\(N\\) numbers represented by a lower case bold letter; it is sometimes useful to extract a subset of elements by defining a set \\(\\mathbb{S}\\) and add it to as a superscript, \\(\\mathbf{x}_\\mathbb{S}\\) . As an example, given \\(\\mathbb{S} = {1, 3, N}\\) we can define the vector \\(\\mathbf{x}_\\mathbb{S} = [x_1, x_3, ..., x_N]\\) and its complementary vector \\(\\mathbf{x}_{-\\mathbb{S}} = [x_2, x_4, ..., x_{N-1}]\\) Matrices : \\(\\mathbf{X} \\in \\mathbb{R}^{[N \\times M]}\\) , two dimensional collection of numbers represented by an upper case bold letter where \\(N\\) and \\(M\\) are referred to as the height and width of the matrix. More specifically a matrix can be written as \\[\\mathbf{X} = \\begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,M} \\\\ ... & ... & ... \\\\ x_{N,1} & x_{N,2} & x_{N,M} \\end{bmatrix} \\] A matrix can be indexed by rows \\(\\mathbf{X}_{i, :}\\) (i-th row), by columns \\(\\mathbf{X}_{:, j}\\) (j-th column), and by element \\(\\mathbf{X}_{i, j}\\) (i-th row, j-th column). A number of useful operations that are commonly applied on vectors and matrices are now described: Transpose: \\(\\mathbf{Y} = \\mathbf{X}^T\\) , where \\(Y_{i, j} = X_{j, i}\\) Matrix plus vector: \\(\\mathbf{Y}_{[N \\times M]} = \\mathbf{X}_{[N \\times M]} + \\mathbf{z}_{[1 \\times M]}\\) , where \\(Y_{i, j} = X_{i, j} + z_{j}\\) ( \\(\\mathbf{z}\\) is added to each row of the matrix \\(\\mathbf{X}\\) ) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-matrix product: \\(\\mathbf{C}_{[N \\times K]} = \\mathbf{A}_{[N \\times M]} \\mathbf{B}_{[M \\times K]}\\) , where \\(C_{i,k} = \\sum_{j=1}^M A_{i, j} B_{j, k}\\) Hadamart product (i.e., element-wise product): \\(\\mathbf{C}_{[N \\times M]} = \\mathbf{A}_{[N \\times M]} \\odot \\mathbf{B}_{[N \\times M]}\\) , where \\(C_{i,j} = A_{i, j} B_{i, j}\\) Dot product: \\(a = \\mathbf{x}_{[N \\times 1]}^T \\mathbf{y}_{[N \\times 1]} = \\sum_{i=1}^N x_i y_i\\) Identity matrix: \\(\\mathbf{I}_N = diag\\{\\mathbf{1}_N\\}\\) . Based on its definition, we have that \\(\\mathbf{I}_N \\mathbf{x} = \\mathbf{x}\\) and \\(\\mathbf{I}_N \\mathbf{X} = \\mathbf{X}\\) Inverse matrix: given \\(\\mathbf{y} = \\mathbf{A} \\mathbf{x}\\) , the inverse matrix of \\(\\mathbf{A}\\) is a matrix that satisfies the following equality \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_N\\) . We can finally write \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{y}\\) Orthogonal vectors and matrices: given two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) , they are said to be orthogonal if \\(\\mathbf{y}^T \\mathbf{x} = 0\\) . Given two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) , they are said to be orthogonal if \\(\\mathbf{Y}^T \\mathbf{X} = \\mathbf{I}_N\\) . Orthogonal matrices are especially interesting because their inverse is very cheap \\(\\mathbf{X}^{-1} = \\mathbf{X}^T\\) Matrix decomposition: like any scalar number can be decomposed into a product of prime numbers, a matrix \\(\\mathbf{A}\\) can also be decomposed into a combination of vectors (i.e., eigenvectors) and scalars (i.e., eigenvalues). Eigendecomposition: real-valued, square, symmetric matrices can be written as \\(\\mathbf{A} = \\mathbf{V} \\Lambda \\mathbf{V}^T = \\sum_i \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\) where \\(\\lambda_i\\) and \\(\\mathbf{v}_i\\) are the eigenvalues and eigenvectors of the matrix \\(\\mathbf{A}\\) , respectively. Eigenvectors are placed along the columns of the matrix \\(\\mathbf{V}\\) , which is an orthogonal matrix (i.e., \\(\\mathbf{V}^T=\\mathbf{V}^{-1}\\) ). Eigenvalues are placed along the diagonal of the matrix \\(\\Lambda=diag\\{\\lambda\\}\\) and tell us about the rank of the matrix, \\(rank(\\mathbf{A}) = \\# \\lambda \\neq 0\\) . A full rank matrix is matrix whose eigenvalues are all non-zero and can be inverted. In this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\Lambda^{-1}\\mathbf{V}^T\\) Singular value decomposition (SVD): this is a more general decomposition which can be applied to real-valued, non-square, non-symmetric matrices. Singular vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) and singular values \\(\\lambda\\) generalized the concept of eigenvectors and and eigenvalues. The matrix \\(\\mathbf{A}\\) can be decomposed as \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T\\) where \\(\\mathbf{D} = \\Lambda\\) for square matrices, \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]^T\\) for \\(N>M\\) and \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]\\) for \\(M>N\\) . Similar to the eigendecomposition, in this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\mathbf{D}^{-1}\\mathbf{U}^T\\) Conditioning: in general, it refers to how fast a function \\(f(x)\\) changes given a small change in its input \\(x\\) . Similarly for a matrix, conditioning is linked to the curvature of its associated quadratic form \\(f(\\mathbf{A}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) and it generally indicates how rapidly this function changes as function of \\(\\mathbf{x}\\) . It is defined as \\(cond(\\mathbf{A})=\\frac{|\\lambda_{max}|}{|\\lambda_{min}|}\\) . Norms : another important object that we will be using when defining cost functions for ML models are norms. A norm is a function that maps a vector \\(\\mathbf{x} \\in \\mathbb{R}^N\\) to a scalar \\(d \\in \\mathbb{R}\\) and it can be loosely seen as measure of the lenght of the vector (i.e., distance from the origin). In general, the \\(L^p\\) norm is defined as: \\[ ||\\mathbf{x}||_p = \\left( \\sum_i |x_i|^p \\right) ^{1/p} \\; p \\ge 0 \\] Popular norms are: Euclidean norm ( \\(L_2\\) ): \\(||\\mathbf{x}||_2 = \\sqrt{\\sum_i x_i^2}\\) , is a real distance of a vector from the origin of the N-d Euclidean space. Note that \\(||\\mathbf{x}||_2^2 = \\mathbf{x}^T \\mathbf{x}\\) and that \\(||\\mathbf{x}||_2=1\\) for a unit vector; \\(L_1\\) norm: \\(||\\mathbf{x}||_1 = \\sum_i |x_i|\\) \\(L_0\\) norm: number of non-zero elements in the vector \\(\\mathbf{x}\\) \\(L_\\infty\\) norm: \\(||\\mathbf{x}||_2 = max |x_i|\\) Frobenious norm (for matrices): \\(||\\mathbf{A}||_F = \\sqrt{\\sum_{i,j} A_{i,j}^2}\\) ,","title":"Linear Algebra refresher"},{"location":"lectures/2_linalg/#linear-algebra-refresher","text":"In this lecture we will go through some of the key concepts of linear algebra and inverse problem theory that are required to develop the theories of the different machine learning algorithm presented in this course. This is not meant to be an exhaustive treatise and students are strongly advised to take the ErSE 213 - Inverse Problems prior to this course. Three key mathematical objects arise in the study of linear algebra: Scalars : \\(a \\in \\mathbb{R}\\) , a single number represented by a lower case italic letter; Vectors : \\(\\mathbf{x} = [x_1, x_2, ..., x_N]^T \\in \\mathbb{R}^N\\) , ordered collection of \\(N\\) numbers represented by a lower case bold letter; it is sometimes useful to extract a subset of elements by defining a set \\(\\mathbb{S}\\) and add it to as a superscript, \\(\\mathbf{x}_\\mathbb{S}\\) . As an example, given \\(\\mathbb{S} = {1, 3, N}\\) we can define the vector \\(\\mathbf{x}_\\mathbb{S} = [x_1, x_3, ..., x_N]\\) and its complementary vector \\(\\mathbf{x}_{-\\mathbb{S}} = [x_2, x_4, ..., x_{N-1}]\\) Matrices : \\(\\mathbf{X} \\in \\mathbb{R}^{[N \\times M]}\\) , two dimensional collection of numbers represented by an upper case bold letter where \\(N\\) and \\(M\\) are referred to as the height and width of the matrix. More specifically a matrix can be written as \\[\\mathbf{X} = \\begin{bmatrix} x_{1,1} & x_{1,2} & x_{1,M} \\\\ ... & ... & ... \\\\ x_{N,1} & x_{N,2} & x_{N,M} \\end{bmatrix} \\] A matrix can be indexed by rows \\(\\mathbf{X}_{i, :}\\) (i-th row), by columns \\(\\mathbf{X}_{:, j}\\) (j-th column), and by element \\(\\mathbf{X}_{i, j}\\) (i-th row, j-th column). A number of useful operations that are commonly applied on vectors and matrices are now described: Transpose: \\(\\mathbf{Y} = \\mathbf{X}^T\\) , where \\(Y_{i, j} = X_{j, i}\\) Matrix plus vector: \\(\\mathbf{Y}_{[N \\times M]} = \\mathbf{X}_{[N \\times M]} + \\mathbf{z}_{[1 \\times M]}\\) , where \\(Y_{i, j} = X_{i, j} + z_{j}\\) ( \\(\\mathbf{z}\\) is added to each row of the matrix \\(\\mathbf{X}\\) ) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-vector product: \\(\\mathbf{y}_{[N \\times 1]} = \\mathbf{A}_{[N \\times M]} \\mathbf{x}_{[M \\times 1]}\\) , where \\(y_i = \\sum_{j=1}^M A_{i, j} x_j\\) Matrix-matrix product: \\(\\mathbf{C}_{[N \\times K]} = \\mathbf{A}_{[N \\times M]} \\mathbf{B}_{[M \\times K]}\\) , where \\(C_{i,k} = \\sum_{j=1}^M A_{i, j} B_{j, k}\\) Hadamart product (i.e., element-wise product): \\(\\mathbf{C}_{[N \\times M]} = \\mathbf{A}_{[N \\times M]} \\odot \\mathbf{B}_{[N \\times M]}\\) , where \\(C_{i,j} = A_{i, j} B_{i, j}\\) Dot product: \\(a = \\mathbf{x}_{[N \\times 1]}^T \\mathbf{y}_{[N \\times 1]} = \\sum_{i=1}^N x_i y_i\\) Identity matrix: \\(\\mathbf{I}_N = diag\\{\\mathbf{1}_N\\}\\) . Based on its definition, we have that \\(\\mathbf{I}_N \\mathbf{x} = \\mathbf{x}\\) and \\(\\mathbf{I}_N \\mathbf{X} = \\mathbf{X}\\) Inverse matrix: given \\(\\mathbf{y} = \\mathbf{A} \\mathbf{x}\\) , the inverse matrix of \\(\\mathbf{A}\\) is a matrix that satisfies the following equality \\(\\mathbf{A}^{-1} \\mathbf{A} = \\mathbf{I}_N\\) . We can finally write \\(\\mathbf{x} = \\mathbf{A}^{-1} \\mathbf{y}\\) Orthogonal vectors and matrices: given two vectors \\(\\mathbf{x}\\) and \\(\\mathbf{y}\\) , they are said to be orthogonal if \\(\\mathbf{y}^T \\mathbf{x} = 0\\) . Given two matrices \\(\\mathbf{X}\\) and \\(\\mathbf{Y}\\) , they are said to be orthogonal if \\(\\mathbf{Y}^T \\mathbf{X} = \\mathbf{I}_N\\) . Orthogonal matrices are especially interesting because their inverse is very cheap \\(\\mathbf{X}^{-1} = \\mathbf{X}^T\\) Matrix decomposition: like any scalar number can be decomposed into a product of prime numbers, a matrix \\(\\mathbf{A}\\) can also be decomposed into a combination of vectors (i.e., eigenvectors) and scalars (i.e., eigenvalues). Eigendecomposition: real-valued, square, symmetric matrices can be written as \\(\\mathbf{A} = \\mathbf{V} \\Lambda \\mathbf{V}^T = \\sum_i \\lambda_i \\mathbf{v}_i \\mathbf{v}_i^T\\) where \\(\\lambda_i\\) and \\(\\mathbf{v}_i\\) are the eigenvalues and eigenvectors of the matrix \\(\\mathbf{A}\\) , respectively. Eigenvectors are placed along the columns of the matrix \\(\\mathbf{V}\\) , which is an orthogonal matrix (i.e., \\(\\mathbf{V}^T=\\mathbf{V}^{-1}\\) ). Eigenvalues are placed along the diagonal of the matrix \\(\\Lambda=diag\\{\\lambda\\}\\) and tell us about the rank of the matrix, \\(rank(\\mathbf{A}) = \\# \\lambda \\neq 0\\) . A full rank matrix is matrix whose eigenvalues are all non-zero and can be inverted. In this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\Lambda^{-1}\\mathbf{V}^T\\) Singular value decomposition (SVD): this is a more general decomposition which can be applied to real-valued, non-square, non-symmetric matrices. Singular vectors \\(\\mathbf{u}\\) and \\(\\mathbf{v}\\) and singular values \\(\\lambda\\) generalized the concept of eigenvectors and and eigenvalues. The matrix \\(\\mathbf{A}\\) can be decomposed as \\(\\mathbf{A} = \\mathbf{U} \\mathbf{D} \\mathbf{V}^T\\) where \\(\\mathbf{D} = \\Lambda\\) for square matrices, \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]^T\\) for \\(N>M\\) and \\(\\mathbf{D} = [\\Lambda \\; \\mathbf{0}]\\) for \\(M>N\\) . Similar to the eigendecomposition, in this case the inverse of \\(\\mathbf{A}=\\mathbf{V}\\mathbf{D}^{-1}\\mathbf{U}^T\\) Conditioning: in general, it refers to how fast a function \\(f(x)\\) changes given a small change in its input \\(x\\) . Similarly for a matrix, conditioning is linked to the curvature of its associated quadratic form \\(f(\\mathbf{A}) = \\mathbf{x}^T \\mathbf{A} \\mathbf{x}\\) and it generally indicates how rapidly this function changes as function of \\(\\mathbf{x}\\) . It is defined as \\(cond(\\mathbf{A})=\\frac{|\\lambda_{max}|}{|\\lambda_{min}|}\\) . Norms : another important object that we will be using when defining cost functions for ML models are norms. A norm is a function that maps a vector \\(\\mathbf{x} \\in \\mathbb{R}^N\\) to a scalar \\(d \\in \\mathbb{R}\\) and it can be loosely seen as measure of the lenght of the vector (i.e., distance from the origin). In general, the \\(L^p\\) norm is defined as: \\[ ||\\mathbf{x}||_p = \\left( \\sum_i |x_i|^p \\right) ^{1/p} \\; p \\ge 0 \\] Popular norms are: Euclidean norm ( \\(L_2\\) ): \\(||\\mathbf{x}||_2 = \\sqrt{\\sum_i x_i^2}\\) , is a real distance of a vector from the origin of the N-d Euclidean space. Note that \\(||\\mathbf{x}||_2^2 = \\mathbf{x}^T \\mathbf{x}\\) and that \\(||\\mathbf{x}||_2=1\\) for a unit vector; \\(L_1\\) norm: \\(||\\mathbf{x}||_1 = \\sum_i |x_i|\\) \\(L_0\\) norm: number of non-zero elements in the vector \\(\\mathbf{x}\\) \\(L_\\infty\\) norm: \\(||\\mathbf{x}||_2 = max |x_i|\\) Frobenious norm (for matrices): \\(||\\mathbf{A}||_F = \\sqrt{\\sum_{i,j} A_{i,j}^2}\\) ,","title":"Linear Algebra refresher"},{"location":"lectures/2_prob/","text":"Probability refresher Another set of fundamental mathematical tools required to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) In order to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) we need to familarize with some basic concepts of: mathematical tools from: Probability : mathematical framework to handle uncertain statements; Information Theory : scientific field focused on the quantification of amount of uncertainty in a probability distribution. Probability Random Variable : a variable whose value is unknown, all we know is that it can take on different values with a given probability. It is generally defined by an uppercase letter \\(X\\) , whilst the values it can take are in lowercase letter \\(x\\) . Probability distribution : description of how likely a variable \\(x\\) is, \\(P(x)\\) (or \\(p(x)\\) ). Depending on the type of variable we have: Discrete distributions : \\(P(X)\\) called Probability Mass Function (PMF) and \\(X\\) can take on a discrete number of states N. A classical example is represented by a coin where N=2 and \\(X={0,1}\\) . For a fair coin, \\(P(X=0)=0.5\\) and \\(P(X=1)=0.5\\) . Continuous distributions : \\(p(X)\\) called Probability Density Function (PDF) and \\(X\\) can take on any value from a continuous space (e.g., \\(\\mathbb{R}\\) ). A classical example is represented by the gaussian distribution where \\(x \\in (-\\infty, \\infty)\\) . A probability distribution must satisfy the following conditions: each of the possible states must have probability bounded between 0 (no occurrance) and 1 (certainty of occurcence): \\(\\forall x \\in X, \\; 0 \\leq P(x) \\leq 1\\) (or \\(p(x) \\geq 0\\) , where the upper bound is removed because of the fact that the integration step \\(\\delta x\\) in the second condition can be smaller than 1: \\(p(X=x) \\delta x <=1\\) ); the sum of the probabilities of all possible states must equal to 1: \\(\\sum_x P(X=x)=1\\) (or \\(\\int p(X=x)dx=1\\) ). Joint and Marginal Probabilities : assuming we have a probability distribution acting over a set of variables (e.g., \\(X\\) and \\(Y\\) ) we can define Joint distribution : \\(P(X=x, Y=y)\\) (or \\(p(X=x, Y=y)\\) ); Marginal distribution : \\(P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y)\\) (or \\(p(X=x) = \\int P(X=x, Y=y) dy\\) ), which is the probability spanning one or a subset of the original variables; Conditional Probability : provides us with the probability of an event given the knowledge that another event has already occurred \\[ P(Y=y | X=x) = \\frac{P(X=x, Y=y)}{P(X=x)} \\] This formula can be used recursively to define the joint probability of a number N of variables as product of conditional probabilities (so-called Chain Rule of Probability ) \\[ P(x_1, x_2, ..., x_N) = P(x_1) \\prod_{i=2}^N P(x_i | x_1, x_2, x_{i-1}) \\] Independence and Conditional Independence : Two variables X and Y are said to be indipendent if \\[ P(X=x, Y=y) = P(X=x) P(Y=y) \\] If both variables are conditioned on a third variable Z (i.e., P(X=x, Y=y | Z=z)), they are said to be conditionally independent if \\[ P(X=x, Y=y | Z=z) = P(X=x | Z=z) P(Y=y| Z=z) \\] Bayes Rule : probabilistic way to update our knowledge of a certain phenomenon (called prior) based on a new piece of evidence (called likelihood): \\[ P(x | y) = \\frac{P(y|x) P(x)}{P(y)} \\] where \\(P(y) = \\sum_x P(x, y) = \\sum_x P(y |x) P(x)\\) is called the evidence. In practice it is unfeasible to compute this quantity as it would require evaluating \\(y\\) for any possible combination of \\(x\\) (we will see later how it is possible to devise methods for which \\(P(y)\\) can be ignored). Mean (or Expectation) : Given a function \\(f(x)\\) where \\(x\\) is a stochastic variable with probability \\(P(x)\\) , its average or mean value is defined as follows for the discrete case: \\[ \\mu = E_{x \\sim P} [f(x)] = \\sum_x P(x) f(x) \\] and for the continuos case \\[ \\mu = E_{x \\sim p} [f(x)] = \\int p(x) f(x) dx \\] In most Machine Learning applications we do not have knowledge of the full distribution to evaluate the mean, rather we have access to N equi-probable samples that we assume are drawn from the underlying distribution. We can approximate the mean via the Sample Mean : \\[ \\mu \\approx \\sum_i \\frac{1}{N} f(x_i) \\] Variance (and Covariance) : Given a function \\(f(x)\\) where \\(x\\) is a stochastic variable with probability \\(P(x)\\) , it represents a masure of how much the values of the function vary from the mean: \\[ \\sigma^2 = E_{x \\sim p} [(f(x)-\\mu)^2] \\] Covariance is the extension of the variance to two or more variables, and it tells how much these variables are related to each other: \\[ Cov(f(x), g(y)) = E_{x,y \\sim p} [(f(x)-\\mu_x)(f(y)-\\mu_y)] \\] Here, \\(Cov \\rightarrow 0\\) indicates no correlation between the variables, \\(Cov > 0\\) denotes positive correlation and \\(Cov < 0\\) denotes negative correlation. It is worth remembering that covariance is linked to correlation via: \\[ Corr_{x,y} = \\frac{Cov_{x,y}}{\\sigma_x \\sigma_y} \\] Finally, the covariance of a multidimensional vector \\(\\textbf{x} \\in \\mathbb{R}^n\\) is defined as: \\[ Cov_{i,j} = Cov(x_i, x_j), \\qquad Cov_{i,i} = \\sigma^2_i \\] Distributions : some of the most used probability distributions in Machine Learning are listed in the following. 1. Bernoulli : single binary variable \\(x \\in \\{0,1\\}\\) (commonly used to describe the toss of a coin). It is defined as \\[ P(x=1)=\\phi, \\; P(x=0)=1-\\phi, \\; \\phi \\in [0,1] \\] with probability: \\[ P(x)=\\phi^x(1-\\phi)^{1-x} \\] and momentums equal to: \\[ E[x] = 1, \\; \\sigma^2 = \\phi (1-\\phi) \\] 2. Multinoulli (or categorical) : extension of Bernoulli distribution to K different states \\[ \\textbf{P} \\in [0,1]^{K-1}; \\; P_k = 1- \\textbf{1}^T\\textbf{P}, \\; \\textbf{1}^T\\textbf{P} \\leq 1 \\] 3. Gaussian : most popular choice for continuous random variables (most distributions are close to a normal distribution and the central limit theorem states that any sum of indipendent variables is approximately normal) \\[ x \\sim \\mathcal{N}(\\mu, \\sigma^2) \\rightarrow p(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} = \\sqrt{\\frac{\\beta}{2 \\pi}} e^{-\\frac{\\beta(x-\\mu)^2}{2}} \\] where the second definition uses the precision \\(\\beta=\\frac{1}{\\sigma^2} \\in (0, \\infty)\\) to avoid possible division by zero. A third way to parametrize the gaussian probability uses \\(2 \\delta = log \\sigma^2 \\in (-\\infty, \\infty)\\) which has the further benefit to be unbounded and can be easily optimized for during training. which is unbounded (compared to the variance that must be positive) 4. Multivariate Gaussian : extension of Gaussian distribution to a multidimensional vector \\(\\textbf{x} \\in \\mathbb{R}^n\\) \\[ \\textbf{x} \\sim \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol\\Sigma) \\rightarrow p(\\textbf{x}) = \\sqrt{\\frac{1}{(2 \\pi)^n det \\boldsymbol\\Sigma}} e^{-\\frac{1}{2}(\\textbf{x}- \\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{x}- \\boldsymbol\\mu)}= \\sqrt{\\frac{det \\boldsymbol\\beta}{(2 \\pi)^n}} e^{-\\frac{1}{2}(\\textbf{x}- \\boldsymbol\\mu)^T\\boldsymbol\\beta(\\textbf{x}- \\boldsymbol\\mu)} $$ \\] where again \\(\\boldsymbol\\beta =\\boldsymbol\\Sigma^{-1}\\) . In ML applications \\(\\boldsymbol\\beta\\) is generally assumed diagonal (mean-field approximation) or even isotropic ($\\boldsymbol\\beta = \\beta \\textbf{I}_n) 5. Mixture of distributions : any smooth probability density function can be expressed as a weighted sum of simpler distributions \\[ P(x) = \\sum_i P(c=i) P(x | c=i) \\] where \\(c\\) is a categorical variable with Multinoulli distribution and plays the role of a latent variable , a variable that cannot be directly observed but is related to \\(x\\) via the joint distribution: \\[ P(x,c) = P(x | c) P(c), \\; P(x) = \\sum_c P(x|c)P(c) \\] A special case is the so-called Gaussian Mixture where each probability \\(P(x|c=i) \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\) . Information theory In Machine Learning, we are sometimes interested to quantify how much information is contained in a signal or how much two signals (or probability distributions) differ from each other. A large body of literature exists in the context of telecommunications, where it is necessary to study how to transmit signals for a discrete alphabet over a noisy channel. More specifically, a code must be designed so to allow sending the least amount of bits for the most amount of useful information. Extension of such theory to continous variables is also available and more commonly used in the context of ML systems. Self-information : a measure of information in such a way that likely events have low information content, less likely events have higher information content and indipendent events have additive information: \\[ I(x) = - log_eP(x) \\] such that for \\(P(x) \\rightarrow 0\\) (unlikely event), \\(I \\rightarrow \\infty\\) and for \\(P(x) \\rightarrow 1\\) (likely event), \\(I \\rightarrow 0\\) . Shannon entropy : extension of self-information to continous variables, representing the expected amount of information in an event \\(x\\) drawn from a probability $P: \\[ H(x) = E_{x \\sim P} [I(x)] = - E_{x \\sim P} [log_eP(x)] \\] Kullback-Leibler divergence : extension of entropy to 2 variables with probability \\(P\\) and \\(Q\\) , respectively. It is used to measure their distance \\[ D_{KL}(P||Q) = E_{x \\sim P} [log\\frac{P(x)}{Q(x)}] = E_{x \\sim P} [logP(x)-logQ(x)] = E_{x \\sim P} [logP(x)] -E_{x \\sim P}[logQ(x)] \\] which is \\(D_{KL}(P||Q)=0\\) only when \\(P=Q\\) and grows the further away the two probabilities are. Finally, note that this is not a real distance in that \\(D_{KL}(P||Q) \\neq D_{KL}(Q|| P)\\) (non-symmetric), therefore the direction matter and it must be chosen wisely when devising optimization schemes with KL divergence in the loss function as we will discuss in more details later.","title":"Probability refresher"},{"location":"lectures/2_prob/#probability-refresher","text":"Another set of fundamental mathematical tools required to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) In order to develop various machine learning algorithms (especially towards the end of the course when we will focus on generative modelling) we need to familarize with some basic concepts of: mathematical tools from: Probability : mathematical framework to handle uncertain statements; Information Theory : scientific field focused on the quantification of amount of uncertainty in a probability distribution.","title":"Probability refresher"},{"location":"lectures/2_prob/#probability","text":"Random Variable : a variable whose value is unknown, all we know is that it can take on different values with a given probability. It is generally defined by an uppercase letter \\(X\\) , whilst the values it can take are in lowercase letter \\(x\\) . Probability distribution : description of how likely a variable \\(x\\) is, \\(P(x)\\) (or \\(p(x)\\) ). Depending on the type of variable we have: Discrete distributions : \\(P(X)\\) called Probability Mass Function (PMF) and \\(X\\) can take on a discrete number of states N. A classical example is represented by a coin where N=2 and \\(X={0,1}\\) . For a fair coin, \\(P(X=0)=0.5\\) and \\(P(X=1)=0.5\\) . Continuous distributions : \\(p(X)\\) called Probability Density Function (PDF) and \\(X\\) can take on any value from a continuous space (e.g., \\(\\mathbb{R}\\) ). A classical example is represented by the gaussian distribution where \\(x \\in (-\\infty, \\infty)\\) . A probability distribution must satisfy the following conditions: each of the possible states must have probability bounded between 0 (no occurrance) and 1 (certainty of occurcence): \\(\\forall x \\in X, \\; 0 \\leq P(x) \\leq 1\\) (or \\(p(x) \\geq 0\\) , where the upper bound is removed because of the fact that the integration step \\(\\delta x\\) in the second condition can be smaller than 1: \\(p(X=x) \\delta x <=1\\) ); the sum of the probabilities of all possible states must equal to 1: \\(\\sum_x P(X=x)=1\\) (or \\(\\int p(X=x)dx=1\\) ). Joint and Marginal Probabilities : assuming we have a probability distribution acting over a set of variables (e.g., \\(X\\) and \\(Y\\) ) we can define Joint distribution : \\(P(X=x, Y=y)\\) (or \\(p(X=x, Y=y)\\) ); Marginal distribution : \\(P(X=x) = \\sum_{y \\in Y} P(X=x, Y=y)\\) (or \\(p(X=x) = \\int P(X=x, Y=y) dy\\) ), which is the probability spanning one or a subset of the original variables; Conditional Probability : provides us with the probability of an event given the knowledge that another event has already occurred \\[ P(Y=y | X=x) = \\frac{P(X=x, Y=y)}{P(X=x)} \\] This formula can be used recursively to define the joint probability of a number N of variables as product of conditional probabilities (so-called Chain Rule of Probability ) \\[ P(x_1, x_2, ..., x_N) = P(x_1) \\prod_{i=2}^N P(x_i | x_1, x_2, x_{i-1}) \\] Independence and Conditional Independence : Two variables X and Y are said to be indipendent if \\[ P(X=x, Y=y) = P(X=x) P(Y=y) \\] If both variables are conditioned on a third variable Z (i.e., P(X=x, Y=y | Z=z)), they are said to be conditionally independent if \\[ P(X=x, Y=y | Z=z) = P(X=x | Z=z) P(Y=y| Z=z) \\] Bayes Rule : probabilistic way to update our knowledge of a certain phenomenon (called prior) based on a new piece of evidence (called likelihood): \\[ P(x | y) = \\frac{P(y|x) P(x)}{P(y)} \\] where \\(P(y) = \\sum_x P(x, y) = \\sum_x P(y |x) P(x)\\) is called the evidence. In practice it is unfeasible to compute this quantity as it would require evaluating \\(y\\) for any possible combination of \\(x\\) (we will see later how it is possible to devise methods for which \\(P(y)\\) can be ignored). Mean (or Expectation) : Given a function \\(f(x)\\) where \\(x\\) is a stochastic variable with probability \\(P(x)\\) , its average or mean value is defined as follows for the discrete case: \\[ \\mu = E_{x \\sim P} [f(x)] = \\sum_x P(x) f(x) \\] and for the continuos case \\[ \\mu = E_{x \\sim p} [f(x)] = \\int p(x) f(x) dx \\] In most Machine Learning applications we do not have knowledge of the full distribution to evaluate the mean, rather we have access to N equi-probable samples that we assume are drawn from the underlying distribution. We can approximate the mean via the Sample Mean : \\[ \\mu \\approx \\sum_i \\frac{1}{N} f(x_i) \\] Variance (and Covariance) : Given a function \\(f(x)\\) where \\(x\\) is a stochastic variable with probability \\(P(x)\\) , it represents a masure of how much the values of the function vary from the mean: \\[ \\sigma^2 = E_{x \\sim p} [(f(x)-\\mu)^2] \\] Covariance is the extension of the variance to two or more variables, and it tells how much these variables are related to each other: \\[ Cov(f(x), g(y)) = E_{x,y \\sim p} [(f(x)-\\mu_x)(f(y)-\\mu_y)] \\] Here, \\(Cov \\rightarrow 0\\) indicates no correlation between the variables, \\(Cov > 0\\) denotes positive correlation and \\(Cov < 0\\) denotes negative correlation. It is worth remembering that covariance is linked to correlation via: \\[ Corr_{x,y} = \\frac{Cov_{x,y}}{\\sigma_x \\sigma_y} \\] Finally, the covariance of a multidimensional vector \\(\\textbf{x} \\in \\mathbb{R}^n\\) is defined as: \\[ Cov_{i,j} = Cov(x_i, x_j), \\qquad Cov_{i,i} = \\sigma^2_i \\] Distributions : some of the most used probability distributions in Machine Learning are listed in the following. 1. Bernoulli : single binary variable \\(x \\in \\{0,1\\}\\) (commonly used to describe the toss of a coin). It is defined as \\[ P(x=1)=\\phi, \\; P(x=0)=1-\\phi, \\; \\phi \\in [0,1] \\] with probability: \\[ P(x)=\\phi^x(1-\\phi)^{1-x} \\] and momentums equal to: \\[ E[x] = 1, \\; \\sigma^2 = \\phi (1-\\phi) \\] 2. Multinoulli (or categorical) : extension of Bernoulli distribution to K different states \\[ \\textbf{P} \\in [0,1]^{K-1}; \\; P_k = 1- \\textbf{1}^T\\textbf{P}, \\; \\textbf{1}^T\\textbf{P} \\leq 1 \\] 3. Gaussian : most popular choice for continuous random variables (most distributions are close to a normal distribution and the central limit theorem states that any sum of indipendent variables is approximately normal) \\[ x \\sim \\mathcal{N}(\\mu, \\sigma^2) \\rightarrow p(x) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}} = \\sqrt{\\frac{\\beta}{2 \\pi}} e^{-\\frac{\\beta(x-\\mu)^2}{2}} \\] where the second definition uses the precision \\(\\beta=\\frac{1}{\\sigma^2} \\in (0, \\infty)\\) to avoid possible division by zero. A third way to parametrize the gaussian probability uses \\(2 \\delta = log \\sigma^2 \\in (-\\infty, \\infty)\\) which has the further benefit to be unbounded and can be easily optimized for during training. which is unbounded (compared to the variance that must be positive) 4. Multivariate Gaussian : extension of Gaussian distribution to a multidimensional vector \\(\\textbf{x} \\in \\mathbb{R}^n\\) \\[ \\textbf{x} \\sim \\mathcal{N}(\\boldsymbol\\mu, \\boldsymbol\\Sigma) \\rightarrow p(\\textbf{x}) = \\sqrt{\\frac{1}{(2 \\pi)^n det \\boldsymbol\\Sigma}} e^{-\\frac{1}{2}(\\textbf{x}- \\boldsymbol\\mu)^T\\boldsymbol\\Sigma^{-1}(\\textbf{x}- \\boldsymbol\\mu)}= \\sqrt{\\frac{det \\boldsymbol\\beta}{(2 \\pi)^n}} e^{-\\frac{1}{2}(\\textbf{x}- \\boldsymbol\\mu)^T\\boldsymbol\\beta(\\textbf{x}- \\boldsymbol\\mu)} $$ \\] where again \\(\\boldsymbol\\beta =\\boldsymbol\\Sigma^{-1}\\) . In ML applications \\(\\boldsymbol\\beta\\) is generally assumed diagonal (mean-field approximation) or even isotropic ($\\boldsymbol\\beta = \\beta \\textbf{I}_n) 5. Mixture of distributions : any smooth probability density function can be expressed as a weighted sum of simpler distributions \\[ P(x) = \\sum_i P(c=i) P(x | c=i) \\] where \\(c\\) is a categorical variable with Multinoulli distribution and plays the role of a latent variable , a variable that cannot be directly observed but is related to \\(x\\) via the joint distribution: \\[ P(x,c) = P(x | c) P(c), \\; P(x) = \\sum_c P(x|c)P(c) \\] A special case is the so-called Gaussian Mixture where each probability \\(P(x|c=i) \\sim \\mathcal{N}(\\mu_i, \\sigma_i^2)\\) .","title":"Probability"},{"location":"lectures/2_prob/#information-theory","text":"In Machine Learning, we are sometimes interested to quantify how much information is contained in a signal or how much two signals (or probability distributions) differ from each other. A large body of literature exists in the context of telecommunications, where it is necessary to study how to transmit signals for a discrete alphabet over a noisy channel. More specifically, a code must be designed so to allow sending the least amount of bits for the most amount of useful information. Extension of such theory to continous variables is also available and more commonly used in the context of ML systems. Self-information : a measure of information in such a way that likely events have low information content, less likely events have higher information content and indipendent events have additive information: \\[ I(x) = - log_eP(x) \\] such that for \\(P(x) \\rightarrow 0\\) (unlikely event), \\(I \\rightarrow \\infty\\) and for \\(P(x) \\rightarrow 1\\) (likely event), \\(I \\rightarrow 0\\) . Shannon entropy : extension of self-information to continous variables, representing the expected amount of information in an event \\(x\\) drawn from a probability $P: \\[ H(x) = E_{x \\sim P} [I(x)] = - E_{x \\sim P} [log_eP(x)] \\] Kullback-Leibler divergence : extension of entropy to 2 variables with probability \\(P\\) and \\(Q\\) , respectively. It is used to measure their distance \\[ D_{KL}(P||Q) = E_{x \\sim P} [log\\frac{P(x)}{Q(x)}] = E_{x \\sim P} [logP(x)-logQ(x)] = E_{x \\sim P} [logP(x)] -E_{x \\sim P}[logQ(x)] \\] which is \\(D_{KL}(P||Q)=0\\) only when \\(P=Q\\) and grows the further away the two probabilities are. Finally, note that this is not a real distance in that \\(D_{KL}(P||Q) \\neq D_{KL}(Q|| P)\\) (non-symmetric), therefore the direction matter and it must be chosen wisely when devising optimization schemes with KL divergence in the loss function as we will discuss in more details later.","title":"Information theory"},{"location":"lectures/3_gradopt/","text":"Gradient-based optimization After reviewing some of the basic concepts of linear algebra and probability that we will be using during this course, we are now in a position to start our journey in the field of learning algorithms . Any learning algorithm, no matter its level of complexity, is composed of 4 key elements: Dataset : a collection of many examples (sometimes referred to as samples of data points) that represents the experience we wish our machine learning algorithm to learn from. More speficically, the dataset is defined as: $$ \\mathbf{x} = [x_1, x_2, ..., x_{N_f}]^T \\quad \\mathbf{X} = [\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(N_s)}] $$ and $$ \\mathbf{y} = [y_1, y_2, ..., y_{N_t}]^T \\quad \\mathbf{Y} = [\\mathbf{y}^{(1)}, \\mathbf{y}^{(2)}, ..., \\mathbf{y}^{(N_s)}] $$ where \\(N_f\\) and \\(N_t\\) are the number of features and targets for each sample in the dataset, respectively, and \\(N_s\\) is the number of samples. Model : a mathematical relation between the input (or features) and output (or target) or our dataset. It is generally parametrized as function \\(f\\) of a number of free parameters \\(\\theta\\) which we want the learning algorithm to estimate given a task and a measure of performance, and we write it as $$ \\mathbf{y} = f_\\theta(\\mathbf{x}) $$ Loss (and cost) function : quantitative measure of the performance of the learning algorithm, which we wish to minimize (or maximize) in order to make accurate predictions on unseen data. It is written as $$ J_\\theta = \\frac{1}{N_s} \\sum_{j=1}^{N_s} \\mathscr{L} (\\mathbf{y}^{(j)}, f_\\theta(\\mathbf{x}^{(j)})) $$ where \\(\\mathscr{L}\\) is the loss function for each input-output pair and \\(J\\) is the overall cost function. Optimization algorithm : mathematical method that aims to drive down (up) the cost function by modifying its free-parameters \\(\\theta\\) : $$ \\hat{\\theta} = \\underset{\\theta} {\\mathrm{argmin}} \\; J_\\theta $$ Optimization algorithms are generally divided into two main families: gradient-based (or local) and gradient-free (or global). Gradient-based optimization is by far the most popular way to train NNs and will be discussed in more details below. Gradient- and steepest-descent algorithms The simplest of gradient-based methods is the so-called Gradient-descent algorithm. As the name implies, this algorithm uses local gradient information of the functional to minimize/maximize to move towards its global mimimum/maximum as depicted in the figure below. More formally, given a functional \\(J_\\theta\\) and its gradient \\(\\nabla J = \\frac{\\delta J}{\\delta \\theta}\\) , the (minimization) algorithm can be written as: Initialization: choose \\(\\theta \\in \\mathbb{R}\\) For \\(i=0,...N-1\\) ; Compute update direction \\(d_i = -\\nabla J |_{\\theta_i}\\) Estimate step-lenght \\(\\alpha_i\\) Update \\(\\theta_{i+1} = \\theta_{i} + \\alpha_i d_i\\) Note that the maximization version of this algorithm simply switches the sign in the update direction (first equation of the algorithm). Moreover, the proposed algorithm can be easily extended to N-dimensional model vectors \\(\\theta=[\\theta_1, \\theta_2, ..., \\theta_N]\\) by defining the following gradient vector \\(\\nabla J=[\\delta J / \\delta\\theta_1, \\delta J / \\delta\\theta_2, ..., \\delta J/ \\delta\\theta_N]\\) . Step lenght selection The choice of the step-lenght has tremendous impact on the performance of the algorithm and its ability to converge fast (i.e., in a small number of iterations) to the optimal solution. The most used selection rules are: Constant: the step size is fixed to a constant value \\(\\alpha_i=\\hat{\\alpha}\\) . This is the most common situation that we will encounter when training neural networks. In practice, some adaptive schemes based on the evolution of the train (or validation) norm are generally adopted, but we will still refer to this case as costant step size; Exact linesearch: at each iteration, \\(\\alpha_i\\) is chosen such that it minimizes \\(J(\\theta_{i} + \\alpha_i d_i)\\) . This is the most commonly used approach when dealing with linear systems of equations. Backtracking \"Armijo\" linesearch: at each iteration, given a parameter \\(\\mu \\in (0,1)\\) , start with \\(\\alpha_i=1\\) and reduce it by a factor of 2 until the following condition is satisfied: \\(J(\\theta_i) - J(\\theta_{i} + \\alpha_i d_i) \\ge -\\mu \\alpha_i \\nabla J^T d_i\\) Second-order optimization Up until now we have discussed first-order optimization techniques that rely on the ability to evaluate the function \\(J\\) and its gradient \\(\\nabla J\\) . Second-order optimization method go one step beyond in that they use information from both the local slope and curvature of the function \\(J\\) . When a function has small curvature, the function and its tangent line are very similar: the gradient alone is therefore able to provide a good local approximation of the function (i.e., \\(J(\\theta+\\delta \\theta)\\approx J(\\theta) + \\nabla J \\delta \\theta\\) ). On the other hand, if the curvature of the function of large, the function and its tangent line start to differ very quickly away from the linearization point. The gradient alone is not able anymore to provide a good local approximation of the function (i.e., \\(J(\\theta+\\delta \\theta)\\approx J(\\theta) + \\nabla J \\delta \\theta + \\nabla^2 J \\delta \\theta^2\\) ). Let's start again from the one-dimensional case and the well-known Newton's method . This method is generally employed to find the zeros of a function: \\(\\theta: J(\\theta)=0\\) and can be written as: \\[ \\theta_{i+1} = \\theta_i - \\frac{J(\\theta)|_{\\theta_i}}{J'(\\theta)|_{\\theta_i}} \\] which can be easily derived from the Taylor expansion of \\(f(\\theta)\\) around \\(\\theta_{i+1}\\) . If we remember that finding the minimum (or maximum) of a function is equivalent to find the zeros of its first derivative ( \\(\\theta: min_\\theta f(\\theta) \\leftrightarrow \\theta: f'(\\theta)=0\\) ), the Netwon's method can be written as: \\[ \\theta_{i+1} = \\theta_i - \\frac{J'(\\theta)|_{\\theta_i}}{J''(\\theta)|_{\\theta_i}} \\] In order to be able to discuss second-order optimization algorithms for the multi-dimensional case, let's first introduce the notion of Jacobian : \\[\\mathbf{y} = J(\\boldsymbol\\theta) \\rightarrow \\mathbf{J} = \\begin{bmatrix} \\frac{\\partial J_1}{\\partial \\theta_1} & \\frac{\\partial J_1}{\\partial \\theta_2} & ... & \\frac{\\partial J_1}{\\partial \\theta_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial J_N}{\\partial \\theta_1} & \\frac{\\partial J_N}{\\partial \\theta_2} & ... & \\frac{\\partial J_N}{\\partial \\theta_M} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[N \\times M]} \\] Through the notion of Jacobian, we can define the Hessian as the Jacobian of the gradient vector \\[\\mathbf{H} = \\nabla (\\nabla J) = \\begin{bmatrix} \\frac{\\partial J^2}{\\partial \\theta_1^2} & \\frac{\\partial J^2}{\\partial x_1 \\partial \\theta_2} & ... & \\frac{\\partial J^2}{\\partial \\theta_1\\partial \\theta_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial J^2}{\\partial \\theta_M \\partial \\theta_1} & \\frac{\\partial J^2}{\\partial \\theta_M \\partial \\theta_2} & ... & \\frac{\\partial J^2}{\\partial \\theta_M^2} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[M \\times M]} \\] where we note that when \\(J\\) is continuous, \\(\\partial / \\partial \\theta_i \\partial \\theta_j = \\partial / \\partial \\theta_j \\partial \\theta_i\\) , and \\(\\mathbf{H}\\) is symmetric. The Newton method for the multi-dimensional case becomes: \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_i - \\mathbf{H}^{-1}\\nabla J \\] Approximated version of the Gauss-Netwon method have been developed over the years, mostly based on the idea that inverting \\(\\mathbf{H}\\) is sometimes a prohibitive task. Such methods, generally referred to as Quasi-Netwon methods attempt to approximate the Hessian (or its inverse) using the gradient at the current iteration and that of a number of previous iterations. BFGS or its limited memory version L-BFGS are examples of such a kind. Due to their computational cost (as well as the lack of solid theories for their use in conjunction with approximate gradients), these methods are not yet commonly used by the machine learning community to optimize the parameters of NNs in deep learning. Stochastic-gradient descent (SGD) To conclude, we look again and gradient-based iterative solvers and more specifically in the context of finite-sum functionals of the kind that we will encountering when training neural networks: \\[ J_\\theta = \\frac{1}{N_s} \\sum_{i=1}^{N_s} \\mathscr{L} (\\mathbf{y}^{(i)}, f_\\theta(\\mathbf{x}^{(i)})) \\] where the summation is here performed over training data. Batched gradient descent The solvers that we have considered so far are generally referred to as methods as they update the model parameters \\(\\boldsymbol\\theta\\) using the full gradient (i.e., over the entire batch of samples): \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_{i} - \\alpha_i \\nabla J = \\boldsymbol\\theta_{i} - \\frac{\\alpha_i}{N_s} \\sum_{j=1}^{N_s} \\nabla \\mathscr{L}_j \\] A limitation of such an approach is that, if we have a very large number of training samples, the computational cost of computing the full gradient is very high and when some of the samples are similar their gradient contribution is somehow redundant. Stochastic gradient descent In this case we take a completely opposite approach to computing the gradient. More specifically, a single training sample is considered at each iteration: \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_{i} - \\alpha_i \\nabla \\mathscr{L}_j \\] The choice of the training sample \\(j\\) at each iteration is generally completely random and this is repeated once all training data have been used at least once (generally referred to as epoch ). In this case, the gradient may be noisy because the gradient of a single sample is a very rough approximation of the total cost function \\(J\\) : such a high variance of gradients requires lowering the step-size \\(\\alpha\\) leading to slow convergence. Mini-batched gradient descent A more commonly used strategy, that lies in between the batched and stochastic gradient descent algorithms uses batches of training samples to compute the gradient at each iteration. More spefically given a batch of \\(N_b\\) samples, the update formula can be written as: \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_{i} - \\frac{\\alpha_i}{N_b} \\sum_{j=1}^{N_b} \\nabla \\mathscr{L}_j \\] and similarly to the stochastic gradient descent, the batches of data are chosen at random and this is repeated as soon as all data are used once in the training loop. Whilst the choice of the size of the batch depends on many factors (e.g., overall size of the dataset, variety of training samples), common batch sizes in training of NNs are from around 50 to 256 (unless memory requirements kick in leading to even small batch sizes). Finally, I encourage everyone to read the following blog post for a more detailed overview of the optimization algorithms discussed here. Note that in one of our future lectures we will also look again at optimization algorithms and more specifically discuss strategies that allow overcoming some of the limitations of standard SGD in this lecture .","title":"Gradient-based optimization"},{"location":"lectures/3_gradopt/#gradient-based-optimization","text":"After reviewing some of the basic concepts of linear algebra and probability that we will be using during this course, we are now in a position to start our journey in the field of learning algorithms . Any learning algorithm, no matter its level of complexity, is composed of 4 key elements: Dataset : a collection of many examples (sometimes referred to as samples of data points) that represents the experience we wish our machine learning algorithm to learn from. More speficically, the dataset is defined as: $$ \\mathbf{x} = [x_1, x_2, ..., x_{N_f}]^T \\quad \\mathbf{X} = [\\mathbf{x}^{(1)}, \\mathbf{x}^{(2)}, ..., \\mathbf{x}^{(N_s)}] $$ and $$ \\mathbf{y} = [y_1, y_2, ..., y_{N_t}]^T \\quad \\mathbf{Y} = [\\mathbf{y}^{(1)}, \\mathbf{y}^{(2)}, ..., \\mathbf{y}^{(N_s)}] $$ where \\(N_f\\) and \\(N_t\\) are the number of features and targets for each sample in the dataset, respectively, and \\(N_s\\) is the number of samples. Model : a mathematical relation between the input (or features) and output (or target) or our dataset. It is generally parametrized as function \\(f\\) of a number of free parameters \\(\\theta\\) which we want the learning algorithm to estimate given a task and a measure of performance, and we write it as $$ \\mathbf{y} = f_\\theta(\\mathbf{x}) $$ Loss (and cost) function : quantitative measure of the performance of the learning algorithm, which we wish to minimize (or maximize) in order to make accurate predictions on unseen data. It is written as $$ J_\\theta = \\frac{1}{N_s} \\sum_{j=1}^{N_s} \\mathscr{L} (\\mathbf{y}^{(j)}, f_\\theta(\\mathbf{x}^{(j)})) $$ where \\(\\mathscr{L}\\) is the loss function for each input-output pair and \\(J\\) is the overall cost function. Optimization algorithm : mathematical method that aims to drive down (up) the cost function by modifying its free-parameters \\(\\theta\\) : $$ \\hat{\\theta} = \\underset{\\theta} {\\mathrm{argmin}} \\; J_\\theta $$ Optimization algorithms are generally divided into two main families: gradient-based (or local) and gradient-free (or global). Gradient-based optimization is by far the most popular way to train NNs and will be discussed in more details below.","title":"Gradient-based optimization"},{"location":"lectures/3_gradopt/#gradient-and-steepest-descent-algorithms","text":"The simplest of gradient-based methods is the so-called Gradient-descent algorithm. As the name implies, this algorithm uses local gradient information of the functional to minimize/maximize to move towards its global mimimum/maximum as depicted in the figure below. More formally, given a functional \\(J_\\theta\\) and its gradient \\(\\nabla J = \\frac{\\delta J}{\\delta \\theta}\\) , the (minimization) algorithm can be written as: Initialization: choose \\(\\theta \\in \\mathbb{R}\\) For \\(i=0,...N-1\\) ; Compute update direction \\(d_i = -\\nabla J |_{\\theta_i}\\) Estimate step-lenght \\(\\alpha_i\\) Update \\(\\theta_{i+1} = \\theta_{i} + \\alpha_i d_i\\) Note that the maximization version of this algorithm simply switches the sign in the update direction (first equation of the algorithm). Moreover, the proposed algorithm can be easily extended to N-dimensional model vectors \\(\\theta=[\\theta_1, \\theta_2, ..., \\theta_N]\\) by defining the following gradient vector \\(\\nabla J=[\\delta J / \\delta\\theta_1, \\delta J / \\delta\\theta_2, ..., \\delta J/ \\delta\\theta_N]\\) .","title":"Gradient- and steepest-descent algorithms"},{"location":"lectures/3_gradopt/#step-lenght-selection","text":"The choice of the step-lenght has tremendous impact on the performance of the algorithm and its ability to converge fast (i.e., in a small number of iterations) to the optimal solution. The most used selection rules are: Constant: the step size is fixed to a constant value \\(\\alpha_i=\\hat{\\alpha}\\) . This is the most common situation that we will encounter when training neural networks. In practice, some adaptive schemes based on the evolution of the train (or validation) norm are generally adopted, but we will still refer to this case as costant step size; Exact linesearch: at each iteration, \\(\\alpha_i\\) is chosen such that it minimizes \\(J(\\theta_{i} + \\alpha_i d_i)\\) . This is the most commonly used approach when dealing with linear systems of equations. Backtracking \"Armijo\" linesearch: at each iteration, given a parameter \\(\\mu \\in (0,1)\\) , start with \\(\\alpha_i=1\\) and reduce it by a factor of 2 until the following condition is satisfied: \\(J(\\theta_i) - J(\\theta_{i} + \\alpha_i d_i) \\ge -\\mu \\alpha_i \\nabla J^T d_i\\)","title":"Step lenght selection"},{"location":"lectures/3_gradopt/#second-order-optimization","text":"Up until now we have discussed first-order optimization techniques that rely on the ability to evaluate the function \\(J\\) and its gradient \\(\\nabla J\\) . Second-order optimization method go one step beyond in that they use information from both the local slope and curvature of the function \\(J\\) . When a function has small curvature, the function and its tangent line are very similar: the gradient alone is therefore able to provide a good local approximation of the function (i.e., \\(J(\\theta+\\delta \\theta)\\approx J(\\theta) + \\nabla J \\delta \\theta\\) ). On the other hand, if the curvature of the function of large, the function and its tangent line start to differ very quickly away from the linearization point. The gradient alone is not able anymore to provide a good local approximation of the function (i.e., \\(J(\\theta+\\delta \\theta)\\approx J(\\theta) + \\nabla J \\delta \\theta + \\nabla^2 J \\delta \\theta^2\\) ). Let's start again from the one-dimensional case and the well-known Newton's method . This method is generally employed to find the zeros of a function: \\(\\theta: J(\\theta)=0\\) and can be written as: \\[ \\theta_{i+1} = \\theta_i - \\frac{J(\\theta)|_{\\theta_i}}{J'(\\theta)|_{\\theta_i}} \\] which can be easily derived from the Taylor expansion of \\(f(\\theta)\\) around \\(\\theta_{i+1}\\) . If we remember that finding the minimum (or maximum) of a function is equivalent to find the zeros of its first derivative ( \\(\\theta: min_\\theta f(\\theta) \\leftrightarrow \\theta: f'(\\theta)=0\\) ), the Netwon's method can be written as: \\[ \\theta_{i+1} = \\theta_i - \\frac{J'(\\theta)|_{\\theta_i}}{J''(\\theta)|_{\\theta_i}} \\] In order to be able to discuss second-order optimization algorithms for the multi-dimensional case, let's first introduce the notion of Jacobian : \\[\\mathbf{y} = J(\\boldsymbol\\theta) \\rightarrow \\mathbf{J} = \\begin{bmatrix} \\frac{\\partial J_1}{\\partial \\theta_1} & \\frac{\\partial J_1}{\\partial \\theta_2} & ... & \\frac{\\partial J_1}{\\partial \\theta_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial J_N}{\\partial \\theta_1} & \\frac{\\partial J_N}{\\partial \\theta_2} & ... & \\frac{\\partial J_N}{\\partial \\theta_M} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[N \\times M]} \\] Through the notion of Jacobian, we can define the Hessian as the Jacobian of the gradient vector \\[\\mathbf{H} = \\nabla (\\nabla J) = \\begin{bmatrix} \\frac{\\partial J^2}{\\partial \\theta_1^2} & \\frac{\\partial J^2}{\\partial x_1 \\partial \\theta_2} & ... & \\frac{\\partial J^2}{\\partial \\theta_1\\partial \\theta_M} \\\\ ... & ... & ... & ... \\\\ \\frac{\\partial J^2}{\\partial \\theta_M \\partial \\theta_1} & \\frac{\\partial J^2}{\\partial \\theta_M \\partial \\theta_2} & ... & \\frac{\\partial J^2}{\\partial \\theta_M^2} \\\\ \\end{bmatrix} \\in \\mathbb{R}^{[M \\times M]} \\] where we note that when \\(J\\) is continuous, \\(\\partial / \\partial \\theta_i \\partial \\theta_j = \\partial / \\partial \\theta_j \\partial \\theta_i\\) , and \\(\\mathbf{H}\\) is symmetric. The Newton method for the multi-dimensional case becomes: \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_i - \\mathbf{H}^{-1}\\nabla J \\] Approximated version of the Gauss-Netwon method have been developed over the years, mostly based on the idea that inverting \\(\\mathbf{H}\\) is sometimes a prohibitive task. Such methods, generally referred to as Quasi-Netwon methods attempt to approximate the Hessian (or its inverse) using the gradient at the current iteration and that of a number of previous iterations. BFGS or its limited memory version L-BFGS are examples of such a kind. Due to their computational cost (as well as the lack of solid theories for their use in conjunction with approximate gradients), these methods are not yet commonly used by the machine learning community to optimize the parameters of NNs in deep learning.","title":"Second-order optimization"},{"location":"lectures/3_gradopt/#stochastic-gradient-descent-sgd","text":"To conclude, we look again and gradient-based iterative solvers and more specifically in the context of finite-sum functionals of the kind that we will encountering when training neural networks: \\[ J_\\theta = \\frac{1}{N_s} \\sum_{i=1}^{N_s} \\mathscr{L} (\\mathbf{y}^{(i)}, f_\\theta(\\mathbf{x}^{(i)})) \\] where the summation is here performed over training data.","title":"Stochastic-gradient descent (SGD)"},{"location":"lectures/3_gradopt/#batched-gradient-descent","text":"The solvers that we have considered so far are generally referred to as methods as they update the model parameters \\(\\boldsymbol\\theta\\) using the full gradient (i.e., over the entire batch of samples): \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_{i} - \\alpha_i \\nabla J = \\boldsymbol\\theta_{i} - \\frac{\\alpha_i}{N_s} \\sum_{j=1}^{N_s} \\nabla \\mathscr{L}_j \\] A limitation of such an approach is that, if we have a very large number of training samples, the computational cost of computing the full gradient is very high and when some of the samples are similar their gradient contribution is somehow redundant.","title":"Batched gradient descent"},{"location":"lectures/3_gradopt/#stochastic-gradient-descent","text":"In this case we take a completely opposite approach to computing the gradient. More specifically, a single training sample is considered at each iteration: \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_{i} - \\alpha_i \\nabla \\mathscr{L}_j \\] The choice of the training sample \\(j\\) at each iteration is generally completely random and this is repeated once all training data have been used at least once (generally referred to as epoch ). In this case, the gradient may be noisy because the gradient of a single sample is a very rough approximation of the total cost function \\(J\\) : such a high variance of gradients requires lowering the step-size \\(\\alpha\\) leading to slow convergence.","title":"Stochastic gradient descent"},{"location":"lectures/3_gradopt/#mini-batched-gradient-descent","text":"A more commonly used strategy, that lies in between the batched and stochastic gradient descent algorithms uses batches of training samples to compute the gradient at each iteration. More spefically given a batch of \\(N_b\\) samples, the update formula can be written as: \\[ \\boldsymbol\\theta_{i+1} = \\boldsymbol\\theta_{i} - \\frac{\\alpha_i}{N_b} \\sum_{j=1}^{N_b} \\nabla \\mathscr{L}_j \\] and similarly to the stochastic gradient descent, the batches of data are chosen at random and this is repeated as soon as all data are used once in the training loop. Whilst the choice of the size of the batch depends on many factors (e.g., overall size of the dataset, variety of training samples), common batch sizes in training of NNs are from around 50 to 256 (unless memory requirements kick in leading to even small batch sizes). Finally, I encourage everyone to read the following blog post for a more detailed overview of the optimization algorithms discussed here. Note that in one of our future lectures we will also look again at optimization algorithms and more specifically discuss strategies that allow overcoming some of the limitations of standard SGD in this lecture .","title":"Mini-batched gradient descent"},{"location":"lectures/4_autoencoder/","text":"Autoencoders At different stages of this course we have seen the importance of choosing a good set of input features when solving a machine learning problem. We have also discussed how, whilst this may require a great deal of human time and e\ufb00ort to find as it varies from problem to problem, Neural Networks have gained popularity because of their ability to find good representations from raw data (e.g., images). The problem of discovering a good representation directly from an input dataset is known as representation learning . The quintessential example of a representation learning algorithm is the Autoencoder . An autoencoder is the combination of an encoder function \\(e_\\theta\\) , which converts the input data into a di\ufb00erent representation, and a decoder function \\(d_\\theta\\) , which converts the new representation back into the original format. ...","title":"Autoencoders"},{"location":"lectures/4_autoencoder/#autoencoders","text":"At different stages of this course we have seen the importance of choosing a good set of input features when solving a machine learning problem. We have also discussed how, whilst this may require a great deal of human time and e\ufb00ort to find as it varies from problem to problem, Neural Networks have gained popularity because of their ability to find good representations from raw data (e.g., images). The problem of discovering a good representation directly from an input dataset is known as representation learning . The quintessential example of a representation learning algorithm is the Autoencoder . An autoencoder is the combination of an encoder function \\(e_\\theta\\) , which converts the input data into a di\ufb00erent representation, and a decoder function \\(d_\\theta\\) , which converts the new representation back into the original format. ...","title":"Autoencoders"},{"location":"lectures/4_linreg/","text":"Linear and Logistic Regression In the previous lecture we have learned how to optimize a generic loss function \\(J_\\theta\\) by modifying its free parameters \\(\\theta\\) . Whilst this is a very generic framework that can be used for various applications in different scientific field, from now on we will learn how to take advtange of similar algorithms in the context of Machine Learning. Linear regression In preparation to our lecture on Neural Networks, we consider here what is generally referred to as the simplest machine learning model for regression, linear regression . Its simplicity lies in the fact that we will only consider a linear relationship between our inputs and targets: where \\(\\textbf{x}\\) is a training sample with \\(N_f\\) features, \\(\\textbf{w}\\) is a vector of \\(N_f\\) weights and \\(b=w_0\\) is the so-called bias term. The set of trainable parameters is therefore the combination of the weights and bias \\(\\boldsymbol\\theta=[\\textbf{w}, b] \\in \\mathbb{R}^{N_f+1}\\) . Similarly the combination of the training sample and a 1-scalar is defined as \\(\\tilde{\\textbf{x}}=[\\textbf{x}, 1] \\in \\mathbb{R}^{N_f+1}\\) The prediction \\(\\hat{y}\\) is simply obtained by linearly combining the different features of the input vector and adding the bias. Despite its simplicity, linear regression (and more commonly multi-variate linear regression) has been succesfully used in a variety of geoscientific tasks, examples of such a kind are: rock-physics models, where a number of petrophysical parameters (e.g., porosity, shale content, depth) can be linearly regressed in order to predict an elastic parameter of interest (e.g., dry bulk modulus); time-to-depth conversion, where a velocity (or depth) prediction is generally made as a linear combination of two-way traveltime and other parameters such as seismic amplitudes and various derived attributes; filling gaps in petrophysical well logs, where various petrophysical measurements (e.g., GR, NEU, DEN) are regressed to estimate another quantity of interest (e.g., S-wave velocity of DTS) that is not directly available within a certain depth interval. Assuming availability of \\(N_s\\) training samples, the input training matrix and output training vector of a linear regression model is written as: \\[ \\mathbf{X}_{train} = [\\tilde{\\mathbf{x}}^{(1)}, \\tilde{\\mathbf{x}}^{(2)}, ..., \\tilde{\\mathbf{x}}^{(N_s)}] \\in \\mathbb{R}^{N_f+1 \\times N_s}, \\quad \\mathbf{y}_{train} = [y^{(1)}, y^{(2)}, y^{(N_s)}] \\in \\mathbb{R}^{N_s \\times 1} \\] Finally, the model can be compactly written as: \\[ \\hat{\\textbf{y}}_{train} = \\textbf{X}_{train}^T \\boldsymbol\\theta \\] Next, we need to define a metric (i.e., cost function) which we can use to optimize for the free parameters \\(\\boldsymbol\\theta\\) . For regression problems, a common metric of goodness is the L2 norm or MSE (Mean Square Error): \\[ J_\\theta = MSE(\\textbf{y}_{train}, \\hat{\\textbf{y}}_{train}) = || \\textbf{y}_{train} - \\hat{\\textbf{y}}_{train}||_2^2 = \\frac{1}{N_s} \\sum_i^{N_s} (y_{train}^{(i)}-\\hat{y}_{train}^{(i)})^2 \\] Based on our previous lecture on optimization, we need to find the best set of coefficients \\(\\theta\\) that minimize the MSE: \\[ \\hat{\\theta} = min_\\theta J_\\theta \\rightarrow \\theta_{i+1} = \\theta_i - \\alpha \\nabla J_\\theta \\] However, since this is a linear inverse problem we can write the analytical solution of the minimization problem as: \\[ \\hat{\\theta} = (\\textbf{X}_{train}^T \\textbf{X}_{train})^{-1} \\textbf{X}_{train}^T \\textbf{y}_{train} \\] which can be obtained by inverting a \\(N_s \\times N_s\\) matrix. An important observation, which lies at the core of most Machine Learning algorithms, is that once the model is trained on the \\(N_s\\) available input-target pairs, the estimated \\(\\hat{\\theta}\\) coefficients can be used to make inference on any new unseen data: \\[ y_{test} = \\tilde{\\textbf{x}}^T_{test} \\hat{\\theta} \\] To conclude, once a linear regression model has been trained, a variety of measures exist to assess the goodness of the model. Whilst the same metric used for training, the mean-square error, can be used to assess the model performance, other metrics are represented by the Pearson coefficient ( \\(R^2\\) ) and the mean-absolute erorr (MAE). Logistic regression Simply put, logistic regression is an extension of linear regression to the problem of binary classification. Whilst the model used by logistic regression is the same linear model described above, this will be coupled with a nonlinear 'activation' function that enforces the outcome of the entire model to be bounded between 0 and 1 (i.e., a probability). In other words, whilst the input training matrix is the same as that of linear regression, the output training vector becomes: \\[ y_{train} = \\{0, 1\\} \\] A variety of applications of such a simple model can be found in geoscience, one common example is represent by net pay prediction from petrophysical logs. Given a single pair of training samples \\(\\textbf{x}, y\\) , a mathematical model for logistic regression can be compactly written as: \\[ \\hat{y} = f_\\theta(\\textbf{x}) = P(y=1 | \\textbf{x}) \\in (0,1) \\] or in other words, the input vector \\(\\textbf{x}\\) is fed through a nonlinear model \\(f_\\theta\\) whose output is a scalar number between 0 and 1 that represents the probability of the target output to be 1. Considering now a set of \\(N_s\\) training pairs, the model can be explicitely written as: \\[ \\hat{\\textbf{y}}_{train} = f_\\theta(\\textbf{X}_{train}) = \\sigma(\\textbf{X}_{train}^T \\boldsymbol\\theta) \\] where \\(\\sigma\\) is a sigmoid function as shown in figure below: Once again, let's define a cost function that we can use to optimize the model parameters. For binary classification, a common metric of goodness is represented by the so-called binary cross-entropy : \\[ \\mathscr{L}(y_{train}^{(i)}, \\hat{y}_{train}^{(i)}) = -(y_{train}^{(i)} log(\\hat{y}_{train}^{(i)}) + (1-y_{train}^{(i)})) log(1- \\hat{y}_{train}^{(i)})) \\] and \\[ J_\\theta = \\frac{1}{N_s} \\sum_i^{N_s} \\mathscr{L}(y_{train}^{(i)}, \\hat{y}_{train}^{(i)}) \\] Let's gain some intuition onto why this is a good cost function. More specifically, we consider with a drawing the two cases separately. First the case of positive target, \\(y_{train}^{(i)}=1\\) and then the case of negative target, \\(y_{train}^{(i)}=0\\) : Our drawings cleary show the validity of such a cost function in both cases. The further away is the prediction from the true label the higher the resulting cost function. Similar to the case of linear regression, we can now update the model parameters by minimizing the cost function: \\[ \\hat{\\theta} = min_\\theta J_\\theta \\rightarrow \\theta_{i+1} = \\theta_i - \\alpha \\nabla J_\\theta \\] However a major difference arises here. Whilst it is easy to compute the derivative of the MSE with respect to the model parameters \\(\\theta\\) , and even more since the model is linear an analytical solution can be found (as shown above), this is not the case of the cost function of the logistic regression model. The good news here is that there exist a systematic approach to computing the derivative of a composite function (i.e., \\(f(x)=f_N(...f_2(f_1(x)))\\) ), which simply relies on the well-known chain rule of functional analysis. This method is referred to in the mathematical community as Automatic Differentiation (AD), and more likely so as Back-propagation in the ML community. As this lies as the foundation of the training process for neural networks, we will get into details later in the text. At this point suffices to say that if we have a composite function like the one above, its derivative with respect to \\(x\\) can be written as: \\[ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_N}{\\partial f_{N-1}} ... \\frac{\\partial f_2}{\\partial f_1} \\frac{\\partial f_1}{\\partial x} \\] where the derivative is simply the product of a number of derivatives over the chain of operations of the composite function. Note that in practice it is more common to compute this chain rule in reverse order, from left to right in the equation above. Whilst we generally rely on the built-in functionalities of deep learning libraries such as Tensorflow or PyTorch to compute such derivaties, we will perform here a full derivation for the simple case of logistic regression. In order to do so, we introduce a very useful mathamatical tool that we use to keep track of a chain of operations and later know how to evaluate the associated gradient. This tool is usually known as computational graph . More specifically, instead of writing the entire logistic regression model compactely in a single equation, we divide it here into its atomic components: \\[ z = \\textbf{x}^T \\boldsymbol\\theta, \\quad a = \\sigma(z), \\quad \\mathscr{L} = -(y log(a) + (1-y)log(1-a)) \\] such that the derivative of the loss function with respect to the model parameters becomes: \\[ \\frac{\\partial \\mathscr{L} }{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\mathscr{L} }{\\partial a} \\frac{\\partial a }{\\partial z} \\frac{\\partial z}{\\partial \\boldsymbol\\theta} \\] The forward and backward passes (as described in software frameworks like PyTorch) can be visually displayed as follows: Let's start from \\(\\partial \\mathscr{L} / \\partial a\\) : \\[ \\frac{\\partial \\mathscr{L}}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a} = \\frac{-y(1-a) + (1-y)a}{a (1-a)} \\] and \\(\\partial a / \\partial z\\) : \\[ \\frac{\\partial a}{\\partial z} = a(1-a) \\] which we can combine together to obtain a simplified formula for the derivative of the loss function of the output of the weighted summation ( \\(z\\) ) \\[ \\frac{\\partial \\mathscr{L}}{\\partial z} = \\frac{\\partial \\mathscr{L}}{\\partial a} \\frac{\\partial a}{\\partial \\sigma} = -y(1-a) + (1-y)a = a - y = dz \\] Finally we differentiate between the weights and the bias to obtain: \\[ \\frac{\\partial z}{\\partial w_i} = x_i, \\quad \\frac{\\partial z}{\\partial b} = 1 \\] such that: \\[ \\frac{\\partial \\mathscr{L}}{\\partial w_i} = dz \\cdot x_i = dw_i, \\quad \\frac{\\partial \\mathscr{L}}{\\partial b} = dz = db \\] Having found the gradients, we can now update the parameters as discussed above: \\[ w_i \\leftarrow w_i - \\alpha \\frac{\\partial \\mathscr{L}}{\\partial w_i} = w_i - \\alpha dw_i, \\quad b \\leftarrow b - \\alpha \\frac{\\partial \\mathscr{L}}{\\partial b} = b - \\alpha db \\] which can be easily modified in the case of multiple training samples: \\[ w_i \\leftarrow w_i - \\alpha \\sum_{j=1}^{N_s} dw_i^{(j)}, \\quad b \\leftarrow b - \\alpha \\sum_{j=1}^{N_s} db^{(j)} \\] We can now summarize a single step of training for \\(N_s\\) training samples for the logistic regression model: \\(\\textbf{z}=\\textbf{X}_{train}^T \\boldsymbol \\theta\\) \\(\\textbf{a} = \\sigma(\\textbf{z})\\) \\(\\textbf{dz} = \\textbf{a} - \\textbf{y}\\) \\(\\textbf{dw} = \\frac{1}{N_s} \\textbf{X}_{train} \\textbf{dz}\\) \\(db = \\frac{1}{N_s} \\textbf{1}^T \\textbf{dz}\\) \\(\\textbf{w} \\leftarrow \\textbf{w} - \\alpha \\textbf{dw}\\) \\(b \\leftarrow b - \\alpha db\\) To conclude, let's turn our attention into some of the evaluation metrics that are commonly used to assess the performance of a classification model (or classifier). Note that these metrics can be used for the logistic regression model discussed here as well as for other more advanced models discussed later in the course. In general for binary classification we have two possible outcomes (positive/negative or true/false) for both the true labels \\(y\\) and the predicted labels \\(\\hat{y}\\) . We can therefore define 4 scenarios: and a number of complementary metrics (all bounded between 0 and 1) can be defined. Note that no metric is better than the others, the importance of one metric over another is context dependant. Precision : \\(Pr=\\frac{TP}{TP+FP}\\) , percentage of correct positive predictions over the overall positive predictions. This measure is appropriate when minimizing false positives is the focus. In the geoscientific context, this may represent a meaningful metric for applications where the main interest is that of predicting the smallest possible number of false positives, whilst at the same time accepting to miss out on some of positives (false negatives). This could be the case when we want to predict hydrocarbon bearing reservoirs from seismic data, where we know already that we will not be able to drill wells into many of them. It is therefore important that even if we make very few positive predictions these must be accurate, whilst the cost of missing other opportunities is not so high. On the other hand, this measure is blind to the predictions of real positive cases to be chosen to be part of the negative class (false negative); Recall : \\(Rc=\\frac{TP}{TP+FN} = \\frac{TP}{P}\\) , percentage of correct positive predictions over the overall positive occurrences. This measure is appropriate when minimizing false negatives is the focus. An opposite scenario to the one presented above is represented by the case of a classifier trained to predict pressure kicks whilst drilling a well. In this case, we are not really concerned with making a few mistakes where we predict a kick when this is not likely to happen (False Positive); of course, this may slow down the drilling process but it is nowhere near as dramatic as the case in which we do not predict a kick which is going to happen (False Negative); a high recall is therefore what we want, as this is an indication of the fact that the model does not miss out on many positive cases. Of course a model that always provides a positive prediction will have a recall of 1 (FN=0), indication of the fact that a high recall is not always an indication of a good model; Accuracy : \\(Ac=\\frac{TP+TN}{TP+TN+FP+FN}=\\frac{TP+TN}{P+N}\\) , percentage of correct predictions over the total number of cases. This measure combines both error types (in the denominator), it is therefore a more global measure of the quality of the model. F1-Score : \\(2 \\frac{Pr \\cdot Rc}{Pr+Rc}\\) , represents a way to combine precision and recall into a single measure that captures both properties. Finally, a more complete description of the performance of a model is given by the so-called confusion matrix , which for the case of binary classification is just the \\(2 \\times 2\\) table in the figure above. This table can be both unnormalized, where each cell simply contains the number of samples which satisfy the specific combination of real and predicted labels, or normalized over either rows or columns.","title":"Linear and Logistic Regression"},{"location":"lectures/4_linreg/#linear-and-logistic-regression","text":"In the previous lecture we have learned how to optimize a generic loss function \\(J_\\theta\\) by modifying its free parameters \\(\\theta\\) . Whilst this is a very generic framework that can be used for various applications in different scientific field, from now on we will learn how to take advtange of similar algorithms in the context of Machine Learning.","title":"Linear and Logistic Regression"},{"location":"lectures/4_linreg/#linear-regression","text":"In preparation to our lecture on Neural Networks, we consider here what is generally referred to as the simplest machine learning model for regression, linear regression . Its simplicity lies in the fact that we will only consider a linear relationship between our inputs and targets: where \\(\\textbf{x}\\) is a training sample with \\(N_f\\) features, \\(\\textbf{w}\\) is a vector of \\(N_f\\) weights and \\(b=w_0\\) is the so-called bias term. The set of trainable parameters is therefore the combination of the weights and bias \\(\\boldsymbol\\theta=[\\textbf{w}, b] \\in \\mathbb{R}^{N_f+1}\\) . Similarly the combination of the training sample and a 1-scalar is defined as \\(\\tilde{\\textbf{x}}=[\\textbf{x}, 1] \\in \\mathbb{R}^{N_f+1}\\) The prediction \\(\\hat{y}\\) is simply obtained by linearly combining the different features of the input vector and adding the bias. Despite its simplicity, linear regression (and more commonly multi-variate linear regression) has been succesfully used in a variety of geoscientific tasks, examples of such a kind are: rock-physics models, where a number of petrophysical parameters (e.g., porosity, shale content, depth) can be linearly regressed in order to predict an elastic parameter of interest (e.g., dry bulk modulus); time-to-depth conversion, where a velocity (or depth) prediction is generally made as a linear combination of two-way traveltime and other parameters such as seismic amplitudes and various derived attributes; filling gaps in petrophysical well logs, where various petrophysical measurements (e.g., GR, NEU, DEN) are regressed to estimate another quantity of interest (e.g., S-wave velocity of DTS) that is not directly available within a certain depth interval. Assuming availability of \\(N_s\\) training samples, the input training matrix and output training vector of a linear regression model is written as: \\[ \\mathbf{X}_{train} = [\\tilde{\\mathbf{x}}^{(1)}, \\tilde{\\mathbf{x}}^{(2)}, ..., \\tilde{\\mathbf{x}}^{(N_s)}] \\in \\mathbb{R}^{N_f+1 \\times N_s}, \\quad \\mathbf{y}_{train} = [y^{(1)}, y^{(2)}, y^{(N_s)}] \\in \\mathbb{R}^{N_s \\times 1} \\] Finally, the model can be compactly written as: \\[ \\hat{\\textbf{y}}_{train} = \\textbf{X}_{train}^T \\boldsymbol\\theta \\] Next, we need to define a metric (i.e., cost function) which we can use to optimize for the free parameters \\(\\boldsymbol\\theta\\) . For regression problems, a common metric of goodness is the L2 norm or MSE (Mean Square Error): \\[ J_\\theta = MSE(\\textbf{y}_{train}, \\hat{\\textbf{y}}_{train}) = || \\textbf{y}_{train} - \\hat{\\textbf{y}}_{train}||_2^2 = \\frac{1}{N_s} \\sum_i^{N_s} (y_{train}^{(i)}-\\hat{y}_{train}^{(i)})^2 \\] Based on our previous lecture on optimization, we need to find the best set of coefficients \\(\\theta\\) that minimize the MSE: \\[ \\hat{\\theta} = min_\\theta J_\\theta \\rightarrow \\theta_{i+1} = \\theta_i - \\alpha \\nabla J_\\theta \\] However, since this is a linear inverse problem we can write the analytical solution of the minimization problem as: \\[ \\hat{\\theta} = (\\textbf{X}_{train}^T \\textbf{X}_{train})^{-1} \\textbf{X}_{train}^T \\textbf{y}_{train} \\] which can be obtained by inverting a \\(N_s \\times N_s\\) matrix. An important observation, which lies at the core of most Machine Learning algorithms, is that once the model is trained on the \\(N_s\\) available input-target pairs, the estimated \\(\\hat{\\theta}\\) coefficients can be used to make inference on any new unseen data: \\[ y_{test} = \\tilde{\\textbf{x}}^T_{test} \\hat{\\theta} \\] To conclude, once a linear regression model has been trained, a variety of measures exist to assess the goodness of the model. Whilst the same metric used for training, the mean-square error, can be used to assess the model performance, other metrics are represented by the Pearson coefficient ( \\(R^2\\) ) and the mean-absolute erorr (MAE).","title":"Linear regression"},{"location":"lectures/4_linreg/#logistic-regression","text":"Simply put, logistic regression is an extension of linear regression to the problem of binary classification. Whilst the model used by logistic regression is the same linear model described above, this will be coupled with a nonlinear 'activation' function that enforces the outcome of the entire model to be bounded between 0 and 1 (i.e., a probability). In other words, whilst the input training matrix is the same as that of linear regression, the output training vector becomes: \\[ y_{train} = \\{0, 1\\} \\] A variety of applications of such a simple model can be found in geoscience, one common example is represent by net pay prediction from petrophysical logs. Given a single pair of training samples \\(\\textbf{x}, y\\) , a mathematical model for logistic regression can be compactly written as: \\[ \\hat{y} = f_\\theta(\\textbf{x}) = P(y=1 | \\textbf{x}) \\in (0,1) \\] or in other words, the input vector \\(\\textbf{x}\\) is fed through a nonlinear model \\(f_\\theta\\) whose output is a scalar number between 0 and 1 that represents the probability of the target output to be 1. Considering now a set of \\(N_s\\) training pairs, the model can be explicitely written as: \\[ \\hat{\\textbf{y}}_{train} = f_\\theta(\\textbf{X}_{train}) = \\sigma(\\textbf{X}_{train}^T \\boldsymbol\\theta) \\] where \\(\\sigma\\) is a sigmoid function as shown in figure below: Once again, let's define a cost function that we can use to optimize the model parameters. For binary classification, a common metric of goodness is represented by the so-called binary cross-entropy : \\[ \\mathscr{L}(y_{train}^{(i)}, \\hat{y}_{train}^{(i)}) = -(y_{train}^{(i)} log(\\hat{y}_{train}^{(i)}) + (1-y_{train}^{(i)})) log(1- \\hat{y}_{train}^{(i)})) \\] and \\[ J_\\theta = \\frac{1}{N_s} \\sum_i^{N_s} \\mathscr{L}(y_{train}^{(i)}, \\hat{y}_{train}^{(i)}) \\] Let's gain some intuition onto why this is a good cost function. More specifically, we consider with a drawing the two cases separately. First the case of positive target, \\(y_{train}^{(i)}=1\\) and then the case of negative target, \\(y_{train}^{(i)}=0\\) : Our drawings cleary show the validity of such a cost function in both cases. The further away is the prediction from the true label the higher the resulting cost function. Similar to the case of linear regression, we can now update the model parameters by minimizing the cost function: \\[ \\hat{\\theta} = min_\\theta J_\\theta \\rightarrow \\theta_{i+1} = \\theta_i - \\alpha \\nabla J_\\theta \\] However a major difference arises here. Whilst it is easy to compute the derivative of the MSE with respect to the model parameters \\(\\theta\\) , and even more since the model is linear an analytical solution can be found (as shown above), this is not the case of the cost function of the logistic regression model. The good news here is that there exist a systematic approach to computing the derivative of a composite function (i.e., \\(f(x)=f_N(...f_2(f_1(x)))\\) ), which simply relies on the well-known chain rule of functional analysis. This method is referred to in the mathematical community as Automatic Differentiation (AD), and more likely so as Back-propagation in the ML community. As this lies as the foundation of the training process for neural networks, we will get into details later in the text. At this point suffices to say that if we have a composite function like the one above, its derivative with respect to \\(x\\) can be written as: \\[ \\frac{\\partial f}{\\partial x} = \\frac{\\partial f_N}{\\partial f_{N-1}} ... \\frac{\\partial f_2}{\\partial f_1} \\frac{\\partial f_1}{\\partial x} \\] where the derivative is simply the product of a number of derivatives over the chain of operations of the composite function. Note that in practice it is more common to compute this chain rule in reverse order, from left to right in the equation above. Whilst we generally rely on the built-in functionalities of deep learning libraries such as Tensorflow or PyTorch to compute such derivaties, we will perform here a full derivation for the simple case of logistic regression. In order to do so, we introduce a very useful mathamatical tool that we use to keep track of a chain of operations and later know how to evaluate the associated gradient. This tool is usually known as computational graph . More specifically, instead of writing the entire logistic regression model compactely in a single equation, we divide it here into its atomic components: \\[ z = \\textbf{x}^T \\boldsymbol\\theta, \\quad a = \\sigma(z), \\quad \\mathscr{L} = -(y log(a) + (1-y)log(1-a)) \\] such that the derivative of the loss function with respect to the model parameters becomes: \\[ \\frac{\\partial \\mathscr{L} }{\\partial \\boldsymbol\\theta} = \\frac{\\partial \\mathscr{L} }{\\partial a} \\frac{\\partial a }{\\partial z} \\frac{\\partial z}{\\partial \\boldsymbol\\theta} \\] The forward and backward passes (as described in software frameworks like PyTorch) can be visually displayed as follows: Let's start from \\(\\partial \\mathscr{L} / \\partial a\\) : \\[ \\frac{\\partial \\mathscr{L}}{\\partial a} = -\\frac{y}{a} + \\frac{1-y}{1-a} = \\frac{-y(1-a) + (1-y)a}{a (1-a)} \\] and \\(\\partial a / \\partial z\\) : \\[ \\frac{\\partial a}{\\partial z} = a(1-a) \\] which we can combine together to obtain a simplified formula for the derivative of the loss function of the output of the weighted summation ( \\(z\\) ) \\[ \\frac{\\partial \\mathscr{L}}{\\partial z} = \\frac{\\partial \\mathscr{L}}{\\partial a} \\frac{\\partial a}{\\partial \\sigma} = -y(1-a) + (1-y)a = a - y = dz \\] Finally we differentiate between the weights and the bias to obtain: \\[ \\frac{\\partial z}{\\partial w_i} = x_i, \\quad \\frac{\\partial z}{\\partial b} = 1 \\] such that: \\[ \\frac{\\partial \\mathscr{L}}{\\partial w_i} = dz \\cdot x_i = dw_i, \\quad \\frac{\\partial \\mathscr{L}}{\\partial b} = dz = db \\] Having found the gradients, we can now update the parameters as discussed above: \\[ w_i \\leftarrow w_i - \\alpha \\frac{\\partial \\mathscr{L}}{\\partial w_i} = w_i - \\alpha dw_i, \\quad b \\leftarrow b - \\alpha \\frac{\\partial \\mathscr{L}}{\\partial b} = b - \\alpha db \\] which can be easily modified in the case of multiple training samples: \\[ w_i \\leftarrow w_i - \\alpha \\sum_{j=1}^{N_s} dw_i^{(j)}, \\quad b \\leftarrow b - \\alpha \\sum_{j=1}^{N_s} db^{(j)} \\] We can now summarize a single step of training for \\(N_s\\) training samples for the logistic regression model: \\(\\textbf{z}=\\textbf{X}_{train}^T \\boldsymbol \\theta\\) \\(\\textbf{a} = \\sigma(\\textbf{z})\\) \\(\\textbf{dz} = \\textbf{a} - \\textbf{y}\\) \\(\\textbf{dw} = \\frac{1}{N_s} \\textbf{X}_{train} \\textbf{dz}\\) \\(db = \\frac{1}{N_s} \\textbf{1}^T \\textbf{dz}\\) \\(\\textbf{w} \\leftarrow \\textbf{w} - \\alpha \\textbf{dw}\\) \\(b \\leftarrow b - \\alpha db\\) To conclude, let's turn our attention into some of the evaluation metrics that are commonly used to assess the performance of a classification model (or classifier). Note that these metrics can be used for the logistic regression model discussed here as well as for other more advanced models discussed later in the course. In general for binary classification we have two possible outcomes (positive/negative or true/false) for both the true labels \\(y\\) and the predicted labels \\(\\hat{y}\\) . We can therefore define 4 scenarios: and a number of complementary metrics (all bounded between 0 and 1) can be defined. Note that no metric is better than the others, the importance of one metric over another is context dependant. Precision : \\(Pr=\\frac{TP}{TP+FP}\\) , percentage of correct positive predictions over the overall positive predictions. This measure is appropriate when minimizing false positives is the focus. In the geoscientific context, this may represent a meaningful metric for applications where the main interest is that of predicting the smallest possible number of false positives, whilst at the same time accepting to miss out on some of positives (false negatives). This could be the case when we want to predict hydrocarbon bearing reservoirs from seismic data, where we know already that we will not be able to drill wells into many of them. It is therefore important that even if we make very few positive predictions these must be accurate, whilst the cost of missing other opportunities is not so high. On the other hand, this measure is blind to the predictions of real positive cases to be chosen to be part of the negative class (false negative); Recall : \\(Rc=\\frac{TP}{TP+FN} = \\frac{TP}{P}\\) , percentage of correct positive predictions over the overall positive occurrences. This measure is appropriate when minimizing false negatives is the focus. An opposite scenario to the one presented above is represented by the case of a classifier trained to predict pressure kicks whilst drilling a well. In this case, we are not really concerned with making a few mistakes where we predict a kick when this is not likely to happen (False Positive); of course, this may slow down the drilling process but it is nowhere near as dramatic as the case in which we do not predict a kick which is going to happen (False Negative); a high recall is therefore what we want, as this is an indication of the fact that the model does not miss out on many positive cases. Of course a model that always provides a positive prediction will have a recall of 1 (FN=0), indication of the fact that a high recall is not always an indication of a good model; Accuracy : \\(Ac=\\frac{TP+TN}{TP+TN+FP+FN}=\\frac{TP+TN}{P+N}\\) , percentage of correct predictions over the total number of cases. This measure combines both error types (in the denominator), it is therefore a more global measure of the quality of the model. F1-Score : \\(2 \\frac{Pr \\cdot Rc}{Pr+Rc}\\) , represents a way to combine precision and recall into a single measure that captures both properties. Finally, a more complete description of the performance of a model is given by the so-called confusion matrix , which for the case of binary classification is just the \\(2 \\times 2\\) table in the figure above. This table can be both unnormalized, where each cell simply contains the number of samples which satisfy the specific combination of real and predicted labels, or normalized over either rows or columns.","title":"Logistic regression"},{"location":"lectures/4_pca/","text":"PCA Use material in Linear Algebra page45","title":"PCA"},{"location":"lectures/4_pca/#pca","text":"Use material in Linear Algebra page45","title":"PCA"},{"location":"lectures/5_bestpractice/","text":"Best practice in Machine Learning Generalize the loss functions from LinReg/LogReg, Linreg/Reg:MSE, Logreg/Class:softmax, Class multi see 5.5.1 and 6.2.2.1 and 6.2.2.2, 6.2.2.3 - DONE!! Pretty much all from Generalization. Follow chapter 5.2 for capacity, over-underfitting, Regularization, hyperparams etc. and also the course Also need to talk about Performance meausure, TP, FP, Accuracy, Precision etc... look at chapter 5 performance section and nice blog post https://yanndubs.github.io/machine-learning-glossary/","title":"Best practice in Machine Learning"},{"location":"lectures/5_bestpractice/#best-practice-in-machine-learning","text":"Generalize the loss functions from LinReg/LogReg, Linreg/Reg:MSE, Logreg/Class:softmax, Class multi see 5.5.1 and 6.2.2.1 and 6.2.2.2, 6.2.2.3 - DONE!! Pretty much all from Generalization. Follow chapter 5.2 for capacity, over-underfitting, Regularization, hyperparams etc. and also the course Also need to talk about Performance meausure, TP, FP, Accuracy, Precision etc... look at chapter 5 performance section and nice blog post https://yanndubs.github.io/machine-learning-glossary/","title":"Best practice in Machine Learning"},{"location":"lectures/5_nn/","text":"Basics of Neural Networks In this lecture we start our journey in the field of Deep Learning. In order to do so we must first introduce the most commonly used kind of Neural Networks, the so-called Multi-Layer Perceptron (MLP) (also commonly referred to as fully connected (FC) layer). A MLP is a class of feedforward artificial neural networks (ANNs), where the term feedforward refers to the fact the the flow of information moves from left to right. On the other hand, a change in the direction of the flow is introduced as part of the forward pass gives rise to a different family of NNs, so-called Recurrent Neural Networks (they will be subject of future lectures): Perceptron To begin with, we focus on the core building block of a MLP, to so-called Perceptron or Unit. This is nothing really new to us, as it is exactly the same structure that we used to create the logistic regression model, a linear weighting of the element of the input vector followed by a nonlinear activation function. We prefer however to schematic represent it in a slighty different way as this will make it easier later on to drawn MLPs. Mathematically, the action of a percepton can be written compactly as dot-product followed by an element-wise nonlinear activation: \\[ y = \\sigma(\\sum_i w_i x_i + b) = \\sigma(\\sum_i \\textbf{w}^T \\textbf{x} + b) \\] where \\(\\textbf{w} \\in \\mathbb{R}^{N_i}\\) is the vector of weights, \\(b\\) is the bias, and \\(\\sigma\\) is a nonlinear activation function. Note that whilst we used a sigmoid function in the logistic regression model, this can be any differentiable function as we will discuss later in more details. Multi-layer Perceptron Whilst the perceptron model shown above takes as input a vector \\(\\textbf{x} \\in \\mathbb{R}^{N_i}\\) and returns a scalar \\(y\\) , we are now ready to make a step forward where we simply combine multiple perceptrons together to return a vector \\(\\textbf{y} \\in \\mathbb{R}^{N_o}\\) The MLP in figure presents \\(N_i=3\\) inputs and \\(N_o=2\\) outputs. By highlighting in green the original perceptron, we can easily observed that a MLP is simply a composition of \\(N_o\\) perceptrons, which again we can compactly write as a matrix-vector multiplication followed again by an element-wise nonlinear activation: \\[ y_j = \\sigma(\\sum_i w_{ji} x_i + b), \\quad \\textbf{y} = \\sigma(\\textbf{W} \\textbf{x} + \\textbf{b}) \\] where \\(\\textbf{W} \\in \\mathbb{R}^{N_o \\times N_i}\\) is the matrix of weights, \\(\\textbf{b} \\in \\mathbb{R}^{N_o}\\) is a vector of biases. Finally, if we stack multiple MLPs together we obtained what is generally referred to as N-layer NN, where the count of the number of layers does not include the input layer. For example, a 3-layer NN has the following structure where we omit for simplicity the bias terms in the schematic drawing. This figure gives us the oppportunity to introduce some terminology commonly used in the DL community: Input layer : first layer taking the input vector \\(\\textbf{x}\\) as input and returning an intermediate representation \\(\\textbf{z}^{[1]}\\) ; Hidden layers : second to penultimate layers taking as input the previous representation \\(\\textbf{z}^{[i-1]}\\) and returning a new representation \\(\\textbf{z}^{[i]}\\) ; Ouput layer : last layer producing the output of the network \\(\\textbf{y}\\) ; Depth : number of hidden layers (plus output layer); Width : number of units in each hidden layer. Note that we will always use the following notation \\(\\cdot^{(i)[j]}\\) where round brackets are used to refer to a specific training sample and square brackets are used to refer to a specific layer. Activation functions We have just started to appreciate the simplicity of NNs. A Neural Network is nothing more than a stack of linear transformations and nonlinear element-wise activation functions. If such activation functions where omitted, we could combine the various linear transformations together in a single matrix, as the product of N matrices is equivalent to a matrix. Assuming that sigma acts as an identity matrix \\(\\sigma(\\textbf{x})=\\textbf{Ix}=\\textbf{x}\\) , (and omitting biases for simplicity) we get: $$ \\textbf{y} = \\sigma(\\textbf{W}^{[3]}\\sigma(\\textbf{W}^{[2]}\\sigma(\\textbf{W}^{[1]} \\textbf{x}))) = \\textbf{W}^{[3]}\\textbf{W}^{[2]}\\textbf{W}^{[1]}\\textbf{x} = \\textbf{W} \\textbf{x} $$ so no matter how deep is the network, we can always reconduct it to a linear model. Depending on the final activation and loss function, we will therefore have a linear regression or a logistic regression model. We consider here a very simple example to show the importance of nonlinear activations before delving into the details. Let's assume that we wish the learn the XOR (eXclusive OR) boolean logic operator from the following four training samples: \\[ \\textbf{x}^{(1)} = [0, 0] \\rightarrow y^{(1)}=0 \\] \\[ \\textbf{x}^{(2)} = [0, 1] \\rightarrow y^{(2)}=1 \\] \\[ \\textbf{x}^{(3)} = [1, 0] \\rightarrow y^{(3)}=1 \\] \\[ \\textbf{x}^{(4)} = [1, 1] \\rightarrow y^{(4)}=0 \\] Starting from the linear regression model, we can define a matrix \\(\\textbf{X}_{train} = [\\textbf{x}^{(1)}, \\textbf{x}^{(2)}, \\textbf{x}^{(3)}, \\textbf{x}^{(4)}]\\) and a vector \\(\\textbf{y}_{train} = [y^{(1)}, y^{(2)}, y^{(3)}, y^{(4)}]\\) . The linear model becomes: \\[ \\textbf{y}_{train} = \\textbf{X}_{train}^T \\boldsymbol \\theta \\] where the weights \\(\\boldsymbol \\theta\\) are obtained as detailed in the previous section. It can be easily proven that the solution is \\(\\boldsymbol \\theta=[0,0,0.5]\\) , where \\(\\textbf{w}=[0,0]\\) and \\(b=0.5\\) . This means that, no matter the input the output of the linear model will always be equal to \\(0.5\\) ; in other words, the model is unable to distinguish between the true or false outcomes. If instead we introduce a nonlinearity between two weight matrices (i.e., a 2-layer NN), the following combination of weights and biases (taken from the Goodfellow book) will lead to a correct prediction: \\[ \\textbf{W}^{[1]} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1\\end{bmatrix}, \\textbf{W}^{[2]} = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}^T, \\textbf{b}^{[1]} = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, b^{[2]} = 0 \\] Note that in this case the \\(\\sigma=ReLU\\) activation function, which we will introduce in the next section, must be used. Of course, there may be many more combinations of weights and biases that lead to a satisfactory prediction. You can prove this to yourself by initializing the weights and biases randomly and optimizing them by means of a stochastic gradient descent algorith. Having introduced nonlinearites every time after we apply the weight matrices to the vector flowing through the computational graph, the overall set of operations cannot be simply reconducted to a matrix-vector multiplication and allows us to learn highly complex nonlinear mappings between input features and targets. The role of activation functions is however not always straighforward and easy to grasp. Whilst we can say that they help in the learning process, not every function is suitable to this task and in fact some functions may prevent the network from learning at all. In the following we look at the most commonly used activation functions and discuss their origin and why they became popular and useful in Deep Learning: Sigmoid and Tanh : historically these were the most popular activation functions as they are differentiable across the entire domain. In the past, there was in fact a strong belief that gradient descent cannot operate on functions that have singularities; although this is correct from a theoretical point of view it was later proved to be wrong in practice. They are mathematically defined as: $$\\sigma_s(x) = \\frac{1}{1-e^{-x}} $$ and $$\\sigma_t(x) = 2 \\sigma_s(2x) - 1 $$ Whilst still used in various contexts, these activation functions saturate very quickly (i.e., large values are clipped to to 1 and small values are clipped to -1 for tanh or 0 for sigmoid). This leads to the so-called vanishing gradient problem that we will discuss in more details in following lectures; simply put, if we look at the the gradient of both of these functions, it is non-zero only when x is near zero and becomes zero away from it, meaning that if the output of a linear layer is large the gradient of the activation function will be zero and therefore the gradient will stop flowing through backpropagation. This is particularly problematic for deep network as the training of the early layers becomes very slow. ReLU (Rectified Linear Unit): this activation function became very popular in the start of the 21st century and since then it is the mostly commonly used activation function for NN training. It is much closer to a linear activation than the previous two, but introduces a nonlinearity by putting negative inputs to zero, By doing so, the ReLU activation function is a piecewise linear function. This shows that non-differentiable functions can be used in gradient based optimization, mostly because numerically we will hardly (if not never) have an output of a NN layer that is exactly zero when fed as input to the activation. Mathematically speaking, we can write it as: $$ \\sigma_r(x) = max ( 0,x ) = \\begin{cases} x & x\\ge 0, \\quad 0 & x<0 \\end{cases} $$ whilst its derivative is: $$ \\sigma'_{relu}(x) = \\begin{cases} 1 & x\\ge 0, \\quad 0 & x<0 \\end{cases} $$ We can observe that this activation function never saturates, for every value in the positive axis the derivative is always 1. Such a property makes ReLU suitable for large networks as the risk of vanishing gradients is greatly reduced. A downside of ReLU is that the entire negative axis acts as an annhilator preventing information to flow. A strategy to prevent or reduce the occurrences of negative inputs is represented by the initialization of biases to a value slightly greater than zero (e.g., b=0.1). Leaky ReLU (Leaky Rectified Linear Unit): a modified version of the ReLU activation function aimed once again at avoiding zeroing of inputs in the negative axis. This function is identical to the ReLU in the positive axis, whilst another straight line with smaller slope is used in the negative axis: $$ \\sigma'_{l-relu}(x) = max ( 0,x ) + \\alpha min ( 0,x ) = \\begin{cases} x & x\\ge 0, \\quad \\alpha x & x<0 \\end{cases} $$ By doing so, also negative inputs can flow through the computational graph. A variant of L-ReLU, called P-ReLU, allows for the \\(\\alpha\\) parameter to be learned instead of being fixed. Absolute ReLU (Absolute Rectified Linear Unit): a modified version of the ReLU activation function that is symmetric with respect to the \\(x=0\\) axis: $$ \\sigma'_{l-relu}(x) = |x| = \\begin{cases} x & x\\ge 0, \\quad -x & x<0 \\end{cases} $$ Whilst this is not a popular choice in the DL literature, it has been succesfully used in object detection tasks where the features that we wish the NN to extract from the training process are polarity invariant. Cosine, Sine, ... : the use of period functions have recently started to appear in the literature especially in the context of scientific DL (e.g., Physics-informed neural networks). Softmax : this activation function is commony used at the end of the last layer in the context of multi-label classification. However as it takes an input vector of N numbers and converts it into an output vector of probabilities (i.e., N numbers summing to 1), it may also be used as a sort of switch in the internal layers. The following two figures show the different activation functions discussed above and their gradients. Network architecture Up until now we have discussed the key components of a Feedforward Neural Network, the Multi-layer Perceptron. It was mentioned a few times that a NN can be composed of multiple MLPs connected with each other, giving rise to a so-called Deep Neural Network (DNN). The depth and width of the network has been also defined, and we have introduced the convention that a N-layer NN is a network with N-1 hidden layers. A crucial point in the design of a neural network architecture is represented by the choice of such parameters. Whilst no hard rules exist and the creation of a NN architecture is to these days still closer to an art than a systematic science, in the following we provide a number of guidelines that should be followed when approaching the problem of designing a network. For example, as previously discussed, connecting two or more layers without adding a nonlinear activation function in between should be avoided as this part of the network simply behaves as a single linear layer. An important theorem that provide insights into the design of neural networks is the so-called Universal Approximation theorem . This theorem states that: \"...regardless of the function that we are trying to learn, we know that a single MLP with infinite number of units can represent this function. We are however not guaranteed that we can train such a network...\" More specifically, learning can fail for two di\ufb00erent reasons: i) the optimization algorithm used for training may not be able to find the value of the parameters that correspond to the desired function; ii) the training algorithm might choose the wrong function as a result of over\ufb01tting. In practice, experience has shown that deeper networks with fewer units per layer are better both in terms of generalization and robustness to training . This leads us with a trade-off between shallow networks with many units in each layer and deep networks with fewer units in each layer. An empirical trend has been observed between the depth of a network and its accuracy on test data: To summarize, whilst theoretically 1-layer shallow networks can learn any function, it is advisable these days to trade network width with network depth as training deep networks is nowadays feasible both from a theoretical and computational point of view. It is however always best to start small and grow the network in width and depth as the problem requires. We will see in the following lectures that a large network requires a large training data to avoid overfitting; therefore, when working with small to medium size training data it is always best to avoid using very large networks in the first place.","title":"Basics of Neural Networks"},{"location":"lectures/5_nn/#basics-of-neural-networks","text":"In this lecture we start our journey in the field of Deep Learning. In order to do so we must first introduce the most commonly used kind of Neural Networks, the so-called Multi-Layer Perceptron (MLP) (also commonly referred to as fully connected (FC) layer). A MLP is a class of feedforward artificial neural networks (ANNs), where the term feedforward refers to the fact the the flow of information moves from left to right. On the other hand, a change in the direction of the flow is introduced as part of the forward pass gives rise to a different family of NNs, so-called Recurrent Neural Networks (they will be subject of future lectures):","title":"Basics of Neural Networks"},{"location":"lectures/5_nn/#perceptron","text":"To begin with, we focus on the core building block of a MLP, to so-called Perceptron or Unit. This is nothing really new to us, as it is exactly the same structure that we used to create the logistic regression model, a linear weighting of the element of the input vector followed by a nonlinear activation function. We prefer however to schematic represent it in a slighty different way as this will make it easier later on to drawn MLPs. Mathematically, the action of a percepton can be written compactly as dot-product followed by an element-wise nonlinear activation: \\[ y = \\sigma(\\sum_i w_i x_i + b) = \\sigma(\\sum_i \\textbf{w}^T \\textbf{x} + b) \\] where \\(\\textbf{w} \\in \\mathbb{R}^{N_i}\\) is the vector of weights, \\(b\\) is the bias, and \\(\\sigma\\) is a nonlinear activation function. Note that whilst we used a sigmoid function in the logistic regression model, this can be any differentiable function as we will discuss later in more details.","title":"Perceptron"},{"location":"lectures/5_nn/#multi-layer-perceptron","text":"Whilst the perceptron model shown above takes as input a vector \\(\\textbf{x} \\in \\mathbb{R}^{N_i}\\) and returns a scalar \\(y\\) , we are now ready to make a step forward where we simply combine multiple perceptrons together to return a vector \\(\\textbf{y} \\in \\mathbb{R}^{N_o}\\) The MLP in figure presents \\(N_i=3\\) inputs and \\(N_o=2\\) outputs. By highlighting in green the original perceptron, we can easily observed that a MLP is simply a composition of \\(N_o\\) perceptrons, which again we can compactly write as a matrix-vector multiplication followed again by an element-wise nonlinear activation: \\[ y_j = \\sigma(\\sum_i w_{ji} x_i + b), \\quad \\textbf{y} = \\sigma(\\textbf{W} \\textbf{x} + \\textbf{b}) \\] where \\(\\textbf{W} \\in \\mathbb{R}^{N_o \\times N_i}\\) is the matrix of weights, \\(\\textbf{b} \\in \\mathbb{R}^{N_o}\\) is a vector of biases. Finally, if we stack multiple MLPs together we obtained what is generally referred to as N-layer NN, where the count of the number of layers does not include the input layer. For example, a 3-layer NN has the following structure where we omit for simplicity the bias terms in the schematic drawing. This figure gives us the oppportunity to introduce some terminology commonly used in the DL community: Input layer : first layer taking the input vector \\(\\textbf{x}\\) as input and returning an intermediate representation \\(\\textbf{z}^{[1]}\\) ; Hidden layers : second to penultimate layers taking as input the previous representation \\(\\textbf{z}^{[i-1]}\\) and returning a new representation \\(\\textbf{z}^{[i]}\\) ; Ouput layer : last layer producing the output of the network \\(\\textbf{y}\\) ; Depth : number of hidden layers (plus output layer); Width : number of units in each hidden layer. Note that we will always use the following notation \\(\\cdot^{(i)[j]}\\) where round brackets are used to refer to a specific training sample and square brackets are used to refer to a specific layer.","title":"Multi-layer Perceptron"},{"location":"lectures/5_nn/#activation-functions","text":"We have just started to appreciate the simplicity of NNs. A Neural Network is nothing more than a stack of linear transformations and nonlinear element-wise activation functions. If such activation functions where omitted, we could combine the various linear transformations together in a single matrix, as the product of N matrices is equivalent to a matrix. Assuming that sigma acts as an identity matrix \\(\\sigma(\\textbf{x})=\\textbf{Ix}=\\textbf{x}\\) , (and omitting biases for simplicity) we get: $$ \\textbf{y} = \\sigma(\\textbf{W}^{[3]}\\sigma(\\textbf{W}^{[2]}\\sigma(\\textbf{W}^{[1]} \\textbf{x}))) = \\textbf{W}^{[3]}\\textbf{W}^{[2]}\\textbf{W}^{[1]}\\textbf{x} = \\textbf{W} \\textbf{x} $$ so no matter how deep is the network, we can always reconduct it to a linear model. Depending on the final activation and loss function, we will therefore have a linear regression or a logistic regression model. We consider here a very simple example to show the importance of nonlinear activations before delving into the details. Let's assume that we wish the learn the XOR (eXclusive OR) boolean logic operator from the following four training samples: \\[ \\textbf{x}^{(1)} = [0, 0] \\rightarrow y^{(1)}=0 \\] \\[ \\textbf{x}^{(2)} = [0, 1] \\rightarrow y^{(2)}=1 \\] \\[ \\textbf{x}^{(3)} = [1, 0] \\rightarrow y^{(3)}=1 \\] \\[ \\textbf{x}^{(4)} = [1, 1] \\rightarrow y^{(4)}=0 \\] Starting from the linear regression model, we can define a matrix \\(\\textbf{X}_{train} = [\\textbf{x}^{(1)}, \\textbf{x}^{(2)}, \\textbf{x}^{(3)}, \\textbf{x}^{(4)}]\\) and a vector \\(\\textbf{y}_{train} = [y^{(1)}, y^{(2)}, y^{(3)}, y^{(4)}]\\) . The linear model becomes: \\[ \\textbf{y}_{train} = \\textbf{X}_{train}^T \\boldsymbol \\theta \\] where the weights \\(\\boldsymbol \\theta\\) are obtained as detailed in the previous section. It can be easily proven that the solution is \\(\\boldsymbol \\theta=[0,0,0.5]\\) , where \\(\\textbf{w}=[0,0]\\) and \\(b=0.5\\) . This means that, no matter the input the output of the linear model will always be equal to \\(0.5\\) ; in other words, the model is unable to distinguish between the true or false outcomes. If instead we introduce a nonlinearity between two weight matrices (i.e., a 2-layer NN), the following combination of weights and biases (taken from the Goodfellow book) will lead to a correct prediction: \\[ \\textbf{W}^{[1]} = \\begin{bmatrix} 1 & 1 \\\\ 1 & 1\\end{bmatrix}, \\textbf{W}^{[2]} = \\begin{bmatrix} 1 \\\\ -2 \\end{bmatrix}^T, \\textbf{b}^{[1]} = \\begin{bmatrix} 0 \\\\ -1 \\end{bmatrix}, b^{[2]} = 0 \\] Note that in this case the \\(\\sigma=ReLU\\) activation function, which we will introduce in the next section, must be used. Of course, there may be many more combinations of weights and biases that lead to a satisfactory prediction. You can prove this to yourself by initializing the weights and biases randomly and optimizing them by means of a stochastic gradient descent algorith. Having introduced nonlinearites every time after we apply the weight matrices to the vector flowing through the computational graph, the overall set of operations cannot be simply reconducted to a matrix-vector multiplication and allows us to learn highly complex nonlinear mappings between input features and targets. The role of activation functions is however not always straighforward and easy to grasp. Whilst we can say that they help in the learning process, not every function is suitable to this task and in fact some functions may prevent the network from learning at all. In the following we look at the most commonly used activation functions and discuss their origin and why they became popular and useful in Deep Learning: Sigmoid and Tanh : historically these were the most popular activation functions as they are differentiable across the entire domain. In the past, there was in fact a strong belief that gradient descent cannot operate on functions that have singularities; although this is correct from a theoretical point of view it was later proved to be wrong in practice. They are mathematically defined as: $$\\sigma_s(x) = \\frac{1}{1-e^{-x}} $$ and $$\\sigma_t(x) = 2 \\sigma_s(2x) - 1 $$ Whilst still used in various contexts, these activation functions saturate very quickly (i.e., large values are clipped to to 1 and small values are clipped to -1 for tanh or 0 for sigmoid). This leads to the so-called vanishing gradient problem that we will discuss in more details in following lectures; simply put, if we look at the the gradient of both of these functions, it is non-zero only when x is near zero and becomes zero away from it, meaning that if the output of a linear layer is large the gradient of the activation function will be zero and therefore the gradient will stop flowing through backpropagation. This is particularly problematic for deep network as the training of the early layers becomes very slow. ReLU (Rectified Linear Unit): this activation function became very popular in the start of the 21st century and since then it is the mostly commonly used activation function for NN training. It is much closer to a linear activation than the previous two, but introduces a nonlinearity by putting negative inputs to zero, By doing so, the ReLU activation function is a piecewise linear function. This shows that non-differentiable functions can be used in gradient based optimization, mostly because numerically we will hardly (if not never) have an output of a NN layer that is exactly zero when fed as input to the activation. Mathematically speaking, we can write it as: $$ \\sigma_r(x) = max ( 0,x ) = \\begin{cases} x & x\\ge 0, \\quad 0 & x<0 \\end{cases} $$ whilst its derivative is: $$ \\sigma'_{relu}(x) = \\begin{cases} 1 & x\\ge 0, \\quad 0 & x<0 \\end{cases} $$ We can observe that this activation function never saturates, for every value in the positive axis the derivative is always 1. Such a property makes ReLU suitable for large networks as the risk of vanishing gradients is greatly reduced. A downside of ReLU is that the entire negative axis acts as an annhilator preventing information to flow. A strategy to prevent or reduce the occurrences of negative inputs is represented by the initialization of biases to a value slightly greater than zero (e.g., b=0.1). Leaky ReLU (Leaky Rectified Linear Unit): a modified version of the ReLU activation function aimed once again at avoiding zeroing of inputs in the negative axis. This function is identical to the ReLU in the positive axis, whilst another straight line with smaller slope is used in the negative axis: $$ \\sigma'_{l-relu}(x) = max ( 0,x ) + \\alpha min ( 0,x ) = \\begin{cases} x & x\\ge 0, \\quad \\alpha x & x<0 \\end{cases} $$ By doing so, also negative inputs can flow through the computational graph. A variant of L-ReLU, called P-ReLU, allows for the \\(\\alpha\\) parameter to be learned instead of being fixed. Absolute ReLU (Absolute Rectified Linear Unit): a modified version of the ReLU activation function that is symmetric with respect to the \\(x=0\\) axis: $$ \\sigma'_{l-relu}(x) = |x| = \\begin{cases} x & x\\ge 0, \\quad -x & x<0 \\end{cases} $$ Whilst this is not a popular choice in the DL literature, it has been succesfully used in object detection tasks where the features that we wish the NN to extract from the training process are polarity invariant. Cosine, Sine, ... : the use of period functions have recently started to appear in the literature especially in the context of scientific DL (e.g., Physics-informed neural networks). Softmax : this activation function is commony used at the end of the last layer in the context of multi-label classification. However as it takes an input vector of N numbers and converts it into an output vector of probabilities (i.e., N numbers summing to 1), it may also be used as a sort of switch in the internal layers. The following two figures show the different activation functions discussed above and their gradients.","title":"Activation functions"},{"location":"lectures/5_nn/#network-architecture","text":"Up until now we have discussed the key components of a Feedforward Neural Network, the Multi-layer Perceptron. It was mentioned a few times that a NN can be composed of multiple MLPs connected with each other, giving rise to a so-called Deep Neural Network (DNN). The depth and width of the network has been also defined, and we have introduced the convention that a N-layer NN is a network with N-1 hidden layers. A crucial point in the design of a neural network architecture is represented by the choice of such parameters. Whilst no hard rules exist and the creation of a NN architecture is to these days still closer to an art than a systematic science, in the following we provide a number of guidelines that should be followed when approaching the problem of designing a network. For example, as previously discussed, connecting two or more layers without adding a nonlinear activation function in between should be avoided as this part of the network simply behaves as a single linear layer. An important theorem that provide insights into the design of neural networks is the so-called Universal Approximation theorem . This theorem states that: \"...regardless of the function that we are trying to learn, we know that a single MLP with infinite number of units can represent this function. We are however not guaranteed that we can train such a network...\" More specifically, learning can fail for two di\ufb00erent reasons: i) the optimization algorithm used for training may not be able to find the value of the parameters that correspond to the desired function; ii) the training algorithm might choose the wrong function as a result of over\ufb01tting. In practice, experience has shown that deeper networks with fewer units per layer are better both in terms of generalization and robustness to training . This leads us with a trade-off between shallow networks with many units in each layer and deep networks with fewer units in each layer. An empirical trend has been observed between the depth of a network and its accuracy on test data: To summarize, whilst theoretically 1-layer shallow networks can learn any function, it is advisable these days to trade network width with network depth as training deep networks is nowadays feasible both from a theoretical and computational point of view. It is however always best to start small and grow the network in width and depth as the problem requires. We will see in the following lectures that a large network requires a large training data to avoid overfitting; therefore, when working with small to medium size training data it is always best to avoid using very large networks in the first place.","title":"Network architecture"},{"location":"lectures/6_nn/","text":"More on Neural Networks In this lecture, we will delve into some more advanced topics associated to the creation and training of deep neural networks. Backpropagation First of all, once a neural network architecture has been defined for the problem at and, we need a method that can learn the best set of free parameters of such nonlinear function represented as \\(f_\\theta\\) . More specifically, we want to initialize the network with some random weights and biases (we will soon discuss how such initialization can be performed) and use the training data at hand to improve our weights and biases in order to minimize a certain loss function. Whilst this can be easily done by means of gradient based optimizers like those presented in Lecture 3, a key ingredient that we need to provide to such algorithms is represented by the gradient of the loss function with respect to each and every weight and bias paramtets. We have already alluded at a technique that can do so whilst discussing a simple logistic regression model. This is generally referred to by the ML community as back-propagation and more broadly by the mathematical community as Reverse Automatic Differentiation . Let's start by taking the same schematic diagram used for the logistic regression example and generalize it to a N-layer NN: The main change here, which we will need to discuss in details, is the fact that in the forward pass we feed the input into a stack of linear layers prior to computing the loss function. The backpropagation does need to be able to keep track of the chain of operations (i.e., computational graph) and traverse it back. However, as already done for the logistic regression model, all we need to do is to write the entire chain of operations as a chain of atomic ones that we can then easily traverse back. Let's do this for the network above and a single training sample \\(\\textbf{x}\\) : \\[ \\textbf{z}^{[1]} = \\textbf{W}^{[1]}\\textbf{x} + \\textbf{b}^{[1]}, \\quad \\textbf{a}^{[1]} = \\sigma(z^{[1]}), \\] \\[ \\textbf{z}^{[2]} = \\textbf{W}^{[2]}\\textbf{a}^{[1]} + \\textbf{b}^{[2]}, \\quad \\textbf{a}^{[2]} = \\sigma(z^{[2]}), \\] \\[ \\textbf{z}^{[3]} = \\textbf{W}^{[3]}\\textbf{a}^{[2]} + \\textbf{b}^{[3]}, \\quad a^{[3]} = \\sigma(z^{[3]}), \\] \\[ l = \\mathscr{L}(y,a^{[3]}). \\] Given such a chain of operations, we are now able to find the derivatives of the loss function with respect to any of the weights or biases. As an example we consider here \\(\\partial l / \\partial \\textbf{W}^{[2]}\\) : \\[ \\frac{\\partial l}{\\partial \\textbf{W}^{[2]}} = \\frac{\\partial l}{\\partial a^{[3]}} \\frac{\\partial a^{[3]}}{\\partial \\textbf{z}^{[3]}} \\frac{\\partial \\textbf{z}^{[3]}}{\\partial \\textbf{a}^{[2]}} \\frac{\\partial \\textbf{a}^{[2]}}{\\partial \\textbf{z}^{[2]}} \\frac{\\partial \\textbf{z}^{[2]}}{\\partial \\textbf{W}^{[2]}} \\] Assuming for simplicity that the binary cross-entropy and sigmoid functions are used here as loss and activation functions, respectively: \\[ \\frac{\\partial l}{\\partial a^{[3]}} \\frac{\\partial a^{[3]}}{\\partial z^{[3]}} = a^{[3]} - y \\] \\[ \\frac{\\partial z^{[3]}}{\\partial \\textbf{a}^{[2]}} = \\textbf{W}^{[3]} \\] \\[ \\frac{\\partial \\textbf{a}^{[2]}}{\\partial \\textbf{z}^{[2]}} = \\textbf{a}^{[2]}(1-\\textbf{a}^{[2]}) \\] \\[ \\frac{\\partial \\textbf{z}^{[2]}}{\\partial \\textbf{W}^{[2]}} = \\textbf{a}^{[1]} \\] which put together: \\[ \\frac{\\partial l}{\\partial \\textbf{W}^{[2]}} = [(\\textbf{a}^{[2]}(1-\\textbf{a}^{[2]})) \\cdot \\textbf{W}^{[3]T}(a^{[3]} - y)] \\textbf{a}^{[1]T} \\] where \\(\\cdot\\) is used to refer to element-wise products. Similar results can be obtained for the bias vector and for both weights and biases in the other layers as depicted in the figure below for a 2-layer NN: To conclude, the backpropagation equations in the diagram above are now generalized for the case of \\(N_s\\) training samples \\(\\textbf{X} \\in \\mathbb{R}^{N \\times N_s}\\) and a generic activation function \\(\\sigma\\) whose derivative is denoted as \\(\\sigma'\\) . Here we still assume an output of dimensionality one -- \\(\\textbf{Y} \\in \\mathbb{R}^{1 \\times N_s}\\) : \\[ \\textbf{dZ}^{[2]}=\\textbf{A}^{[2]}-\\textbf{Y} \\qquad (\\textbf{A}^{[2]},\\textbf{dZ}^{[2]} \\in \\mathbb{R}^{1 \\times N_s}) \\] \\[ \\textbf{dW}^{[2]}= \\frac{1}{N_s} \\textbf{dZ}^{[2]}\\textbf{A}^{[1]T} \\qquad (\\textbf{A}^{[1]} \\in \\mathbb{R}^{N^{[1]} \\times N_s}) \\] \\[ db^{[2]}= \\frac{1}{N_s} \\sum_i \\textbf{dZ}_{:,i}^{[2]} \\] \\[ \\textbf{dZ}^{[1]}=\\textbf{W}^{[2]^T}\\textbf{dZ}^{[2]} \\cdot \\sigma'(\\textbf{Z}^{[1]}) \\qquad (\\textbf{dZ}^{[1]} \\in \\mathbb{R}^{N^{[1]} \\times N_s}) \\] \\[ \\textbf{dW}^{[1]}= \\frac{1}{N_s} \\textbf{dZ}^{[1]}\\textbf{X}^T \\] \\[ \\textbf{db}^{[1]}= \\frac{1}{N_s} \\sum_i \\textbf{dZ}_{:,i}^{[1]} \\] Initialization Neural networks are hihgly nonlinear functions. The associated cost function used in the training process in order to optimize the network weights and biases is therefore non-convex and contains several local minima and saddle points. A key component in non-convex optimization is represented by the starting guess of the parameters to optimize, which in the context of deep learning is identified by initialization of weights and biases. Whilst a proper initialization has been shown to be key to a succesful training of deep train NNs, this is a very active area of research as initialization strategies are so far mostly based on heuristic arguments and experience. Zero initialization First of all, let's highlight a bad initialization choice that can compromise the training no matter the architecture of the network and other hyperparamters. A common choice in standard optimization in the absence of any strong prior information is to initalize all the paramters to zero: if we decide to follow such a strategy when training a NN, we will soon realize that training is stagnant due to the so called symmetry problem (also referred to as symmetric gradients ). Note that a similar situation arises also if we choose a constant values for weights and biases (e.g., \\(c^{[1]}\\) for all the weights and biases in the first layer and \\(c^{[2]}\\) for all the weights and biases in the second layer): Let's take a look at this with an example: Since the activations are constant vectors, back-propagation produces constant updates for the weights (and biases), leading to weights and biases to never lose the initial symmetry. Random initialization A more appropriate way to initialize the weights of a neural network is to sample their values from random distributions, for example: $$ w_{ij}^{[.]} \\sim \\mathcal{N}(0, 0.01) $$ where the choice of the variance is based on the following trade-off: too small variance leads to the vanishing gradient problem (i.e., slow training), whilst too high variance leads to the exploding gradient problem (i.e., unstable training). On the other hand, for the biases we can use zero or a constant value. If you remember, we have already mentioned this when discussing the ReLU activation function: a good strategy to limit the amount of negative values as input to this activation function is to choose a small constant bias (e.g., \\(b=0.1\\) ). Whilst this approach provides a good starting point for stable training of neural networks, more advanced initialization strategies have been proposed in the literature: Uniform : the weights are initialized with uniform distributions whose variance depend on the number of units in the layer: $$ w_{ij}^{[k]} \\sim \\mathcal{U}(-1/\\sqrt{N^{[k]}}, 1/\\sqrt{N^{[k]}}) $$ or $$ w_{ij}^{[k]} \\sim \\mathcal{U}(-\\sqrt{6/(N^{[k-1]}+N^{[k]})}, \\sqrt{6/(N^{[k-1]}+N^{[k]})}) $$ This strategy is commonly used with FC layers. Xavier : the weights are initialized with normal distributions whose variance depend on the number of units in the layer: $$ w_{ij}^{[k]} \\sim \\mathcal{N}(0, 1/N^{[k]}) $$ This strategy ensures that the variance remains the same across the layers. Xavier initialization is very popular especially in layers using Tanh activations. He : the weights are initialized with normal distributions whose variance depend on the number of units in the layer: $$ w_{ij}^{[k]} \\sim \\mathcal{N}(0, 2/N^{[k]}) $$ This strategy ensures that the variance remains the same across the layers. Xavier initialization is very popular especially in layers using ReLU activations. Finally, if you are interest to learn more about initialization I reccomend reading (and reproducing) the following blog posts: 1 and 2 . WHY NN/DEEP LEARNING TOOK OFF Finally lets remark why theories are all from '80 but until early 2000 NN were not popular (niche field) \"The core ideas behind modern feedforward networks have not changed sub-stantially since the 1980s. The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of the improvement in neuralnetwork performance from 1986 to 2015 can be attributed to two factors. First,larger datasets have reduced the degree to which statistical generalization is achallenge for neural networks. Second, neural networks have become much larger,because of more powerful computers and better software infrastructure\" Plus a few algorithmic changes: - \"One of these algorithmic changes was the replacement of mean squared errorwith the cross-entropy family of loss functions. Mean squared error was popular inthe 1980s and 1990s but was gradually replaced by cross-entropy losses and the principle of maximum likelihood as ideas spread between the statistics community and the machine learning community. The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, whichhad previously Su\ufb00ered from saturation and slow learning when using the meansquared error loss\" --> NOTE for regression ML with gaussianity assumption on p(y|x) is still MSE, but only for this edge case! \"change that has greatly improved the performanceof feedforward networks was the replacement of sigmoid hidden units with piecewiselinear hidden units, such as recti\ufb01ed linear units. Recti\ufb01cation using themax{0, z}function was introduced in early neural network models and dates back at least as faras the cognitron and neocognitron (Fukushima, 1975, 1980)... This began to change in about 2009. Jarrett et al. (2009)observed that \u201cusing a rectifying nonlinearity is the single most important factorin improving the performance of a recognition system,\u201d --> LEARN TO CHALLANGE STATUS QUO, SOMETIMES UNDERSTANDING OF PROBLEMS CAN CHANGE OR NEW EXTERNAL FACTORS (EG MORE DATA) MAKE SOMETHING THAT WAS WORSE BECOME BETTER... SIMILAR STORY IN FWI FOR GEOPHYSCISTS Also add mixture density 'network' (example of petroelastic with facies - Andrews paper) - https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca","title":"More on Neural Networks"},{"location":"lectures/6_nn/#more-on-neural-networks","text":"In this lecture, we will delve into some more advanced topics associated to the creation and training of deep neural networks.","title":"More on Neural Networks"},{"location":"lectures/6_nn/#backpropagation","text":"First of all, once a neural network architecture has been defined for the problem at and, we need a method that can learn the best set of free parameters of such nonlinear function represented as \\(f_\\theta\\) . More specifically, we want to initialize the network with some random weights and biases (we will soon discuss how such initialization can be performed) and use the training data at hand to improve our weights and biases in order to minimize a certain loss function. Whilst this can be easily done by means of gradient based optimizers like those presented in Lecture 3, a key ingredient that we need to provide to such algorithms is represented by the gradient of the loss function with respect to each and every weight and bias paramtets. We have already alluded at a technique that can do so whilst discussing a simple logistic regression model. This is generally referred to by the ML community as back-propagation and more broadly by the mathematical community as Reverse Automatic Differentiation . Let's start by taking the same schematic diagram used for the logistic regression example and generalize it to a N-layer NN: The main change here, which we will need to discuss in details, is the fact that in the forward pass we feed the input into a stack of linear layers prior to computing the loss function. The backpropagation does need to be able to keep track of the chain of operations (i.e., computational graph) and traverse it back. However, as already done for the logistic regression model, all we need to do is to write the entire chain of operations as a chain of atomic ones that we can then easily traverse back. Let's do this for the network above and a single training sample \\(\\textbf{x}\\) : \\[ \\textbf{z}^{[1]} = \\textbf{W}^{[1]}\\textbf{x} + \\textbf{b}^{[1]}, \\quad \\textbf{a}^{[1]} = \\sigma(z^{[1]}), \\] \\[ \\textbf{z}^{[2]} = \\textbf{W}^{[2]}\\textbf{a}^{[1]} + \\textbf{b}^{[2]}, \\quad \\textbf{a}^{[2]} = \\sigma(z^{[2]}), \\] \\[ \\textbf{z}^{[3]} = \\textbf{W}^{[3]}\\textbf{a}^{[2]} + \\textbf{b}^{[3]}, \\quad a^{[3]} = \\sigma(z^{[3]}), \\] \\[ l = \\mathscr{L}(y,a^{[3]}). \\] Given such a chain of operations, we are now able to find the derivatives of the loss function with respect to any of the weights or biases. As an example we consider here \\(\\partial l / \\partial \\textbf{W}^{[2]}\\) : \\[ \\frac{\\partial l}{\\partial \\textbf{W}^{[2]}} = \\frac{\\partial l}{\\partial a^{[3]}} \\frac{\\partial a^{[3]}}{\\partial \\textbf{z}^{[3]}} \\frac{\\partial \\textbf{z}^{[3]}}{\\partial \\textbf{a}^{[2]}} \\frac{\\partial \\textbf{a}^{[2]}}{\\partial \\textbf{z}^{[2]}} \\frac{\\partial \\textbf{z}^{[2]}}{\\partial \\textbf{W}^{[2]}} \\] Assuming for simplicity that the binary cross-entropy and sigmoid functions are used here as loss and activation functions, respectively: \\[ \\frac{\\partial l}{\\partial a^{[3]}} \\frac{\\partial a^{[3]}}{\\partial z^{[3]}} = a^{[3]} - y \\] \\[ \\frac{\\partial z^{[3]}}{\\partial \\textbf{a}^{[2]}} = \\textbf{W}^{[3]} \\] \\[ \\frac{\\partial \\textbf{a}^{[2]}}{\\partial \\textbf{z}^{[2]}} = \\textbf{a}^{[2]}(1-\\textbf{a}^{[2]}) \\] \\[ \\frac{\\partial \\textbf{z}^{[2]}}{\\partial \\textbf{W}^{[2]}} = \\textbf{a}^{[1]} \\] which put together: \\[ \\frac{\\partial l}{\\partial \\textbf{W}^{[2]}} = [(\\textbf{a}^{[2]}(1-\\textbf{a}^{[2]})) \\cdot \\textbf{W}^{[3]T}(a^{[3]} - y)] \\textbf{a}^{[1]T} \\] where \\(\\cdot\\) is used to refer to element-wise products. Similar results can be obtained for the bias vector and for both weights and biases in the other layers as depicted in the figure below for a 2-layer NN: To conclude, the backpropagation equations in the diagram above are now generalized for the case of \\(N_s\\) training samples \\(\\textbf{X} \\in \\mathbb{R}^{N \\times N_s}\\) and a generic activation function \\(\\sigma\\) whose derivative is denoted as \\(\\sigma'\\) . Here we still assume an output of dimensionality one -- \\(\\textbf{Y} \\in \\mathbb{R}^{1 \\times N_s}\\) : \\[ \\textbf{dZ}^{[2]}=\\textbf{A}^{[2]}-\\textbf{Y} \\qquad (\\textbf{A}^{[2]},\\textbf{dZ}^{[2]} \\in \\mathbb{R}^{1 \\times N_s}) \\] \\[ \\textbf{dW}^{[2]}= \\frac{1}{N_s} \\textbf{dZ}^{[2]}\\textbf{A}^{[1]T} \\qquad (\\textbf{A}^{[1]} \\in \\mathbb{R}^{N^{[1]} \\times N_s}) \\] \\[ db^{[2]}= \\frac{1}{N_s} \\sum_i \\textbf{dZ}_{:,i}^{[2]} \\] \\[ \\textbf{dZ}^{[1]}=\\textbf{W}^{[2]^T}\\textbf{dZ}^{[2]} \\cdot \\sigma'(\\textbf{Z}^{[1]}) \\qquad (\\textbf{dZ}^{[1]} \\in \\mathbb{R}^{N^{[1]} \\times N_s}) \\] \\[ \\textbf{dW}^{[1]}= \\frac{1}{N_s} \\textbf{dZ}^{[1]}\\textbf{X}^T \\] \\[ \\textbf{db}^{[1]}= \\frac{1}{N_s} \\sum_i \\textbf{dZ}_{:,i}^{[1]} \\]","title":"Backpropagation"},{"location":"lectures/6_nn/#initialization","text":"Neural networks are hihgly nonlinear functions. The associated cost function used in the training process in order to optimize the network weights and biases is therefore non-convex and contains several local minima and saddle points. A key component in non-convex optimization is represented by the starting guess of the parameters to optimize, which in the context of deep learning is identified by initialization of weights and biases. Whilst a proper initialization has been shown to be key to a succesful training of deep train NNs, this is a very active area of research as initialization strategies are so far mostly based on heuristic arguments and experience.","title":"Initialization"},{"location":"lectures/6_nn/#zero-initialization","text":"First of all, let's highlight a bad initialization choice that can compromise the training no matter the architecture of the network and other hyperparamters. A common choice in standard optimization in the absence of any strong prior information is to initalize all the paramters to zero: if we decide to follow such a strategy when training a NN, we will soon realize that training is stagnant due to the so called symmetry problem (also referred to as symmetric gradients ). Note that a similar situation arises also if we choose a constant values for weights and biases (e.g., \\(c^{[1]}\\) for all the weights and biases in the first layer and \\(c^{[2]}\\) for all the weights and biases in the second layer): Let's take a look at this with an example: Since the activations are constant vectors, back-propagation produces constant updates for the weights (and biases), leading to weights and biases to never lose the initial symmetry.","title":"Zero initialization"},{"location":"lectures/6_nn/#random-initialization","text":"A more appropriate way to initialize the weights of a neural network is to sample their values from random distributions, for example: $$ w_{ij}^{[.]} \\sim \\mathcal{N}(0, 0.01) $$ where the choice of the variance is based on the following trade-off: too small variance leads to the vanishing gradient problem (i.e., slow training), whilst too high variance leads to the exploding gradient problem (i.e., unstable training). On the other hand, for the biases we can use zero or a constant value. If you remember, we have already mentioned this when discussing the ReLU activation function: a good strategy to limit the amount of negative values as input to this activation function is to choose a small constant bias (e.g., \\(b=0.1\\) ). Whilst this approach provides a good starting point for stable training of neural networks, more advanced initialization strategies have been proposed in the literature: Uniform : the weights are initialized with uniform distributions whose variance depend on the number of units in the layer: $$ w_{ij}^{[k]} \\sim \\mathcal{U}(-1/\\sqrt{N^{[k]}}, 1/\\sqrt{N^{[k]}}) $$ or $$ w_{ij}^{[k]} \\sim \\mathcal{U}(-\\sqrt{6/(N^{[k-1]}+N^{[k]})}, \\sqrt{6/(N^{[k-1]}+N^{[k]})}) $$ This strategy is commonly used with FC layers. Xavier : the weights are initialized with normal distributions whose variance depend on the number of units in the layer: $$ w_{ij}^{[k]} \\sim \\mathcal{N}(0, 1/N^{[k]}) $$ This strategy ensures that the variance remains the same across the layers. Xavier initialization is very popular especially in layers using Tanh activations. He : the weights are initialized with normal distributions whose variance depend on the number of units in the layer: $$ w_{ij}^{[k]} \\sim \\mathcal{N}(0, 2/N^{[k]}) $$ This strategy ensures that the variance remains the same across the layers. Xavier initialization is very popular especially in layers using ReLU activations. Finally, if you are interest to learn more about initialization I reccomend reading (and reproducing) the following blog posts: 1 and 2 .","title":"Random initialization"},{"location":"lectures/6_nn/#why-nndeep-learning-took-off","text":"Finally lets remark why theories are all from '80 but until early 2000 NN were not popular (niche field) \"The core ideas behind modern feedforward networks have not changed sub-stantially since the 1980s. The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of the improvement in neuralnetwork performance from 1986 to 2015 can be attributed to two factors. First,larger datasets have reduced the degree to which statistical generalization is achallenge for neural networks. Second, neural networks have become much larger,because of more powerful computers and better software infrastructure\" Plus a few algorithmic changes: - \"One of these algorithmic changes was the replacement of mean squared errorwith the cross-entropy family of loss functions. Mean squared error was popular inthe 1980s and 1990s but was gradually replaced by cross-entropy losses and the principle of maximum likelihood as ideas spread between the statistics community and the machine learning community. The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, whichhad previously Su\ufb00ered from saturation and slow learning when using the meansquared error loss\" --> NOTE for regression ML with gaussianity assumption on p(y|x) is still MSE, but only for this edge case! \"change that has greatly improved the performanceof feedforward networks was the replacement of sigmoid hidden units with piecewiselinear hidden units, such as recti\ufb01ed linear units. Recti\ufb01cation using themax{0, z}function was introduced in early neural network models and dates back at least as faras the cognitron and neocognitron (Fukushima, 1975, 1980)... This began to change in about 2009. Jarrett et al. (2009)observed that \u201cusing a rectifying nonlinearity is the single most important factorin improving the performance of a recognition system,\u201d --> LEARN TO CHALLANGE STATUS QUO, SOMETIMES UNDERSTANDING OF PROBLEMS CAN CHANGE OR NEW EXTERNAL FACTORS (EG MORE DATA) MAKE SOMETHING THAT WAS WORSE BECOME BETTER... SIMILAR STORY IN FWI FOR GEOPHYSCISTS Also add mixture density 'network' (example of petroelastic with facies - Andrews paper) - https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca","title":"WHY NN/DEEP LEARNING TOOK OFF"}]}