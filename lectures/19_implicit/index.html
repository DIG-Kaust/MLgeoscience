
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Implicit neural networks - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#implicit-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Implicit neural networks
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_INN/" class="md-nav__link">
        Invertible Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Implicit neural networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Implicit neural networks
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fixed-point-iterations" class="md-nav__link">
    Fixed point iterations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    Further reading
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fixed-point-iterations" class="md-nav__link">
    Fixed point iterations
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#further-reading" class="md-nav__link">
    Further reading
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="implicit-neural-networks">Implicit neural networks</h1>
<p>Neural networks consists of a sequence of consecutive operations that are typically defined explicitly. An explicit operation is one that computes the output directly
from a sequence of explicit operations applied to the input. A simple example is a feed-forward MLP, where the transition from one layer to the next is done by the following sequence 
of operations</p>
<div class="arithmatex">\[
\begin{aligned}
z_i &amp; = W_iz_{i-1} + b_i \\
a_i &amp; = \sigma_i(z_i)
\end{aligned}
\]</div>
<p>Additionally one could add operations like batch normalization and max pooling, all of which are given explicitly. Alternatively, two variables can be related
via an implicit equation. A simple example of an explicit function versus an implicit function is <span class="arithmatex">\(y = x^2\)</span> versus <span class="arithmatex">\(x^2 + y^2 = 1\)</span>. From the second example it 
becomes clear why implicit functions are sometimes favorable, since the implicit function <span class="arithmatex">\(x^2 + y^2 = 1\)</span> has an explicit counterpart with two equations, namely
<span class="arithmatex">\(y = \sqrt{1 - x^2}\)</span> and <span class="arithmatex">\(y = -\sqrt{1 - x^2}\)</span>. In a more abstract fashion, we can write an explicit equation as</p>
<div class="arithmatex">\[
  y = f(x),
\]</div>
<p>and an implicit equation as</p>
<div class="arithmatex">\[
  f(x, y) = 0.
\]</div>
<p>Neural networks can be defined implicitly as well through the concept of implicit layers and were introduced under the name <em>Deep Equilibrium Models (DEQ)</em>. This concept 
is a bit abstract but the nice thing about this paradigm is that the memory requirements for deep networks are constant. To understand this concept we need to cover
two fundamental concepts:</p>
<ul>
<li>Implicit functions and the implicit function theorem. Taking derivatives of explicit functions is easy, since we have an explicit relation of the output with respect
to the input, and we can compute <span class="arithmatex">\(\frac{dy}{dx}\)</span> in a straightforward manner. However, if <span class="arithmatex">\(y\)</span> is only given through <span class="arithmatex">\(f(x, y) = 0\)</span> then computing the derivative is 
less straightforward.</li>
<li>Fixed point iterations. Fixed point iterations are iterations of the form <span class="arithmatex">\(x_{k+1} = \mathcal{F}(x_k)\)</span> and we call a vector <span class="arithmatex">\(x_{\star}\)</span> a <em>fixed point of <span class="arithmatex">\(\mathcal{F}\)</span></em> 
if <span class="arithmatex">\(x_{\star} = \mathcal{F}(x_{\star})\)</span>. DEQs are based on the idea that the layers of a neural network will eventually reach a fixed point.</li>
</ul>
<h2 id="fixed-point-iterations">Fixed point iterations</h2>
<p>Consider the following fixed point iteration</p>
<div class="arithmatex">\[
  z_{k+1} = \tanh(Wz_k + b + x).
\]</div>
<p>This is essentially repeated application of one layer of a neural network with weight matrix <span class="arithmatex">\(W\)</span>, bias <span class="arithmatex">\(b\)</span>, some input <span class="arithmatex">\(x\)</span> and activation function <span class="arithmatex">\(\tanh\)</span>. Assuming for now that 
a fixed point actually exists we iterate until convergence, i.e. <span class="arithmatex">\(z_{\star} = \mathcal{F}(z_{\star})\)</span> up to some tolerance. Alternatively, we can write the above equation as </p>
<div class="arithmatex">\[
  z - \tanh(Wz + b + x) = 0,
\]</div>
<p>where the function is now implicitly defined. Defining</p>
<div class="arithmatex">\[
  g(x, z) := z - \tanh(Wz + b + x),
\]</div>
<p>the goal is now to solve the root finding problem</p>
<div class="arithmatex">\[
  g(x, z_{\star}(x)) = 0,
\]</div>
<p>where <span class="arithmatex">\(z_{\star}(x)\)</span> denotes the solution depending on <span class="arithmatex">\(x\)</span>. Let the solution to this problem be given by <span class="arithmatex">\(z_{\star}(x)\)</span> and assume we want 
to compute <span class="arithmatex">\(\frac{dz_{\star}(x)}{dx}\)</span> (note that we could choose to differentiate through any parameter, for example the weight matrix, 
but this is just for illustrative purposes). Since we only have access to <span class="arithmatex">\(z_{\star}\)</span> through the equation <span class="arithmatex">\(g(x, z_{\star}(x)) = 0\)</span> need to 
differentiate through this equation to obtain <span class="arithmatex">\(\frac{dz_{\star}(x)}{dx}\)</span>. This yields:</p>
<div class="arithmatex">\[
  \frac{\partial}{\partial x}g(x, z_{\star}(x)) = \frac{\partial g(x, z_{\star})}{\partial x} +  \frac{\partial g(x, z_{\star})}{\partial z_{\star}}\frac{\partial z_{\star}(x)}{\partial x} = 0
\]</div>
<p>This equation allows us to solve for <span class="arithmatex">\(\frac{dz_{\star}(x)}{dx}\)</span> as follows</p>
<div class="arithmatex">\[
  \frac{\partial z_{\star}(x)}{\partial x} = -\left(\frac{\partial g(x, z_{\star})}{\partial z_{\star}}\right)^{-1}\frac{\partial g(x, z_{\star})}{\partial x}
\]</div>
<p>The main question here is whether existence is guaranteed. The <em>implicit function theorem</em> states that if a fixed point exists and the function 
<span class="arithmatex">\(g\)</span> is differentiable with non-singular Jacobian around <span class="arithmatex">\(z_{\star}\)</span> there exists a unique function <span class="arithmatex">\(z_{\star}(x)\)</span>. The key point here is that 
one can differentiate through <span class="arithmatex">\(z_{\star}\)</span> without needing to differentiate through the solver used to obtain the fixed point. This saves a huge amount of 
memory that would otherwise be needed in order to perform backpropagation. </p>
<p>This observation has led to the development of the <em>Deep Equilibrium Network</em>. This network has the following structure:</p>
<div class="arithmatex">\[
\begin{aligned}
z_1 &amp; = 0 \\
z_i &amp; = \sigma_i(Wz_i + Ux + b_i), \quad i=1,\ldots, k \\
h(x) &amp; = W_kz_k + b_k
\end{aligned}
\]</div>
<p>As we can see, DEQs apply a fixed point iteration to a single layer of a neural network. The question is whether this fixed point iteration
actually converges: It could also blow-up or oscillate. It turns out that in general the fixed point iteration converges. As you can probably guess
at this point, the fixed point iteration is solved using implicit differentiation, thereby bypassing the need to store any information necessary for 
the backward pass. This way one can build an extremely deep network. If we now want to update the weights of the neural network we need to evaluate the 
partial derivative with respect to <span class="arithmatex">\(W\)</span>. Given that <span class="arithmatex">\(z_{\star}\)</span> is a fixed point we have</p>
<div class="arithmatex">\[
  z_{\star} = f(x, z_{\star}) \: \Leftrightarrow \: \frac{\partial z_{\star}}{\partial W} = \frac{\partial f(x, z_{\star})}{\partial W}
\]</div>
<p>Computing <span class="arithmatex">\(\frac{\partial f(x, z_{\star})}{\partial W}\)</span> via implicit differentiation and rearranging terms gives</p>
<div class="arithmatex">\[
  \frac{\partial z_{\star}}{\partial W} =  \left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)^{-1}\frac{\partial f(x, z_{\star})}{\partial W}
\]</div>
<p>Backpropagation actually implements the transpose of this expression, i.e.:</p>
<div class="arithmatex">\[
  \left(\frac{\partial z_{\star}}{\partial W}\right)^Ty =  \left(\frac{\partial f(x, z_{\star})}{\partial W}\right)^T\left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)^{-T}y,
\]</div>
<p>where <span class="arithmatex">\(y\)</span> is some vector we apply the gradient to. Evaluating the gradient is now a two-step process:</p>
<ul>
<li>Evaluate <span class="arithmatex">\(\left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)^{-1}y\)</span>. Since this matrix tends to be large we do not evaluate
the inverse directly, but rather solve the linear system 
$$
  y = \left(I - \frac{\partial f(x, z_{\star})}{\partial z_{\star}}\right)g \quad \Leftrightarrow \quad g = \frac{\partial f(x, z_{\star})}{\partial z_{\star}}g + y.
$$</li>
<li>Compute 
$$
  \left(\frac{\partial z_{\star}}{\partial W}\right)^Ty = \left(\frac{\partial f(x, z_{\star})}{\partial W}\right)^Tg
$$</li>
</ul>
<p>So far we have considered a rather simple model for the DEQ. We have assumed a constant weight <span class="arithmatex">\(W\)</span> accross the layers and have assumed a simple
feed-forward model. It turns out that a feed-forward neural network with constant weights accross the layers is actually equivalent to a neural network
with a layer-dependent matrix, which is summarized in the following theorem by <a href="https://arxiv.org/pdf/1909.01377.pdf">Bai et al., 2019</a>:</p>
<p>Consider a traditional <span class="arithmatex">\(L\)</span>-layer MLP</p>
<div class="arithmatex">\[
  z_{i+1} = \sigma_{i}(W_iz_i + b_i), \quad i=0,\ldots,L-1, \quad z_0 = x.
\]</div>
<p>This network is equivalent to the following weight-tied network of equivalent depth:</p>
<div class="arithmatex">\[
  \tilde{z}_{i+1} = \tilde{\sigma}(W_zz_i + \tilde{b} + Ux), \quad i=0, \ldots, L-1, \quad \tilde{z}_{0} = (0, \ldots, 0)^T
\]</div>
<p>We prove the theorem for the case <span class="arithmatex">\(L = 4\)</span>, but it extends to general <span class="arithmatex">\(L\)</span>. Define the matrices</p>
<div class="arithmatex">\[
  W_z = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}, \:
U = \begin{bmatrix} W_0 \\ 0 \\ 0 \\ 0 \end{bmatrix}, \: \tilde{b} = \begin{bmatrix} b_0 \\ b_1 \\ b_2 \\ b_3 \end{bmatrix}, \: \tilde{\sigma} = \begin{bmatrix} \sigma_0 \\ \sigma_1 \\ \sigma_2 \\ \sigma_3 \end{bmatrix}.
\]</div>
<p>Then after one iteration we have</p>
<div class="arithmatex">\[
  \tilde{z}_1 = \tilde{\sigma}(W_z\tilde{z}_0 + Ux + \tilde{b}) = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(b_1) \\ \sigma_2(b_2) \\ \sigma_3(b_3) \end{bmatrix} 
= \begin{bmatrix} z_0 \\ \sigma_1(b_1) \\ \sigma_2(b_2) \\ \sigma_3(b_3) \end{bmatrix}.
\]</div>
<p>For the second iteration we have</p>
<div class="arithmatex">\[
  W_z\tilde{z_1} = \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}\begin{bmatrix} z_0 \\ \sigma_1(b_1) \\ \sigma_2(b_2) \\ \sigma_3(b_3) \end{bmatrix} = \begin{bmatrix} 0 \\ W_1z_0 \\ W_2\sigma_2(b_2) \\ W_3\sigma_2(b_2) \end{bmatrix}
\]</div>
<p>and hence</p>
<div class="arithmatex">\[
  \tilde{z}_{2} = \tilde{\sigma}(W_zz_1 + \tilde{b} + Ux) = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(W_1z_0 + b_1) \\ \sigma_2(W_2\sigma_1(b_1) + b_2) \\ \sigma_3(W_3\sigma_2(b_2) + b_3) \end{bmatrix} 
= \begin{bmatrix} z_0 \\ z_1 \\ \sigma_2(W_2\sigma_1(b_1) + b_2) \\ \sigma_3(W_3\sigma_2(b_2) + b_3) \end{bmatrix}. 
\]</div>
<p>Similarly, for the next layer we obtain</p>
<div class="arithmatex">\[
  \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}\begin{bmatrix} z_0 \\ z_1 \\ \sigma_2(W_2\sigma_1(b_1) + b_2) \\ \sigma_3(W_3\sigma_2(b_2) + b_3) \end{bmatrix} = \begin{bmatrix} 0 \\ W_1z_0 \\ W_2z_1 \\ W_3\sigma_2(W_2\sigma_1(b_1) + b_2) \end{bmatrix}
\]</div>
<p>which leads to </p>
<div class="arithmatex">\[
  \tilde{z}_3 = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(W_1z_0 + b_1) \\ \sigma_2(W_2z_1 + b_2) \\ \sigma_3(W_3\sigma_2(W_2\sigma_1(b_1) + b_2) + b_3) \end{bmatrix} = \begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ \sigma_3(W_3\sigma_2(W_2\sigma_1(b_1) + b_2) + b_3) \end{bmatrix}
\]</div>
<p>Then, finally,</p>
<div class="arithmatex">\[
  \begin{bmatrix} 0 &amp; 0 &amp; 0 &amp; 0 \\ W_1 &amp; 0  &amp; 0 &amp; 0 \\ 0 &amp; W_2 &amp; 0 &amp; 0 \\ 0 &amp; 0 &amp; W_{3} &amp; 0 \end{bmatrix}\begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ \sigma_3(W_3\sigma_2(W_2\sigma_1(b_1) + b_2) + b_3) \end{bmatrix} = \begin{bmatrix} 0 \\ W_1z_1 \\ W_2z_2 \\ W_3z_3 \end{bmatrix}
\]</div>
<p>and hence</p>
<div class="arithmatex">\[
  \tilde{z}_4 = \begin{bmatrix} \sigma_0(W_0x + b_0) \\ \sigma_1(W_1z_0 + b_1) \\ \sigma_2(W_2z_1 + b_2) \\ \sigma_3(W_3z_2 + b_3) \end{bmatrix} = \begin{bmatrix} z_1 \\ z_2 \\ z_3 \\ z_4 \end{bmatrix}.
\]</div>
<p>Moreover, note that we have only used a single layer DEQ as opposed to the multi-layer architecture that is typical for powerful neural networks. However,
any deep neural network can be represented as a deep neural network. The argument is as follows. Assume that construct a two-layer network <span class="arithmatex">\(g_2(g_1(x))\)</span>. This can 
be posed a single layer DEQ using the following relation:</p>
<div class="arithmatex">\[
  f(z, x) = f\left( \begin{bmatrix} z_1 \\ z_2 \end{bmatrix}, x\right) = \begin{bmatrix} g(x) \\ g(z_1) \end{bmatrix}
\]</div>
<p>That is, the complexity of the extra layer can simply be added by concatenating the two layers to make a single layer neural network. The same argument
holds for stacking DEQs: a single DEQ can model any number of stacked DEQs.</p>
<p>Finally, we can increase the complexity of the DEQ by substituting the simple feed-forward neural network with any sequence of operations, including
convolutions, normalizations, grouping and skip connections. </p>
<h2 id="further-reading">Further reading</h2>
<p>These notes are essentially a summary of the following tutorial, specifically chapters 1 and 4:</p>
<ul>
<li><a href="http://implicit-layers-tutorial.org/">Implicit layer tutorial</a></li>
</ul>
<p>Below is the paper introducing Deep Equilibrium Models (DEQ):</p>
<ul>
<li><a href="https://arxiv.org/pdf/1909.01377.pdf">Bai et al., 2019</a></li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../18_INN/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Invertible Neural Networks" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Invertible Neural Networks
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>