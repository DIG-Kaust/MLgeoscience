
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.2.8">
    
    
      
        <title>More on Neural Networks - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.cb6bc1d0.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.39b8e14a.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#more-on-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      <div class="md-header-nav__ellipsis">
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            
              More on Neural Networks
            
          </span>
        </div>
      </div>
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
      
      <label class="md-nav__link" for="nav-4">
        Lectures
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="nav-4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../10_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../11_vae/" class="md-nav__link">
        VAE
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../12_mdn/" class="md-nav__link">
        Mixture-density networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../1_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../2_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../2_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../3_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_autoencoder/" class="md-nav__link">
        Autoencoders
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_pca/" class="md-nav__link">
        PCA
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../5_bestpractice/" class="md-nav__link">
        Best practice in Machine Learning
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../5_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          More on Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        More on Neural Networks
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#backpropagation" class="md-nav__link">
    Backpropagation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    Initialization
  </a>
  
    <nav class="md-nav" aria-label="Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-initialization" class="md-nav__link">
    Zero initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    Random initialization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-nndeep-learning-took-off" class="md-nav__link">
    WHY NN/DEEP LEARNING TOOK OFF
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#backpropagation" class="md-nav__link">
    Backpropagation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    Initialization
  </a>
  
    <nav class="md-nav" aria-label="Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-initialization" class="md-nav__link">
    Zero initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    Random initialization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-nndeep-learning-took-off" class="md-nav__link">
    WHY NN/DEEP LEARNING TOOK OFF
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="more-on-neural-networks">More on Neural Networks</h1>
<p>In this lecture, we will delve into some more advanced topics associated to the creation and training of deep 
neural networks.</p>
<h2 id="backpropagation">Backpropagation</h2>
<p>First of all, once a neural network architecture has been defined for the problem at and, we need a method
that can learn the best set of free parameters of such nonlinear function represented as <span class="arithmatex">\(f_\theta\)</span>.</p>
<p>More specifically, we want to initialize the network with some random weights and biases (we will soon discuss how
such initialization can be performed) and use the training data at hand to improve our weights and biases in order
to minimize a certain loss function. Whilst this can be easily done by means of gradient based optimizers like those
presented in Lecture 3, a key ingredient that we need to provide to such algorithms is represented by the gradient
of the loss function with respect to each and every weight and bias paramtets. </p>
<p>We have already alluded at a technique that can do so whilst discussing a simple logistic regression model. This
is generally referred to by the ML community as <em>back-propagation</em> and more broadly by the mathematical community
as <em>Reverse Automatic Differentiation</em>. Let's start by taking the same schematic diagram used for the logistic regression
example and generalize it to a N-layer NN:</p>
<p><img alt="BACKPROP_NN" src="../figs/backprop_nn.png" /></p>
<p>The main change here, which we will need to discuss in details, is the fact that in the forward pass 
we feed the input into a stack of linear layers prior to computing the loss function. The backpropagation
does need to be able to keep track of the chain of operations (i.e., computational graph) and traverse it
back. However, as already done for the logistic regression model, all we need to do is to write the entire
chain of operations as a chain of atomic ones that we can then easily traverse back. Let's do this for
the network above and a single training sample <span class="arithmatex">\(\textbf{x}\)</span>:</p>
<div class="arithmatex">\[
\textbf{z}^{[1]} = \textbf{W}^{[1]}\textbf{x} + \textbf{b}^{[1]}, \quad
\textbf{a}^{[1]} = \sigma(z^{[1]}),
\]</div>
<div class="arithmatex">\[
\textbf{z}^{[2]} = \textbf{W}^{[2]}\textbf{a}^{[1]} + \textbf{b}^{[2]}, \quad
\textbf{a}^{[2]} = \sigma(z^{[2]}),
\]</div>
<div class="arithmatex">\[
\textbf{z}^{[3]} = \textbf{W}^{[3]}\textbf{a}^{[2]} + \textbf{b}^{[3]}, \quad
a^{[3]} = \sigma(z^{[3]}),
\]</div>
<div class="arithmatex">\[
l = \mathscr{L}(y,a^{[3]}).
\]</div>
<p>Given such a chain of operations, we are now able to find the derivatives of the loss function with
respect to any of the weights or biases. As an example we consider here <span class="arithmatex">\(\partial l / \partial \textbf{W}^{[2]}\)</span>:</p>
<div class="arithmatex">\[
\frac{\partial l}{\partial \textbf{W}^{[2]}} = \frac{\partial l}{\partial a^{[3]}} \frac{\partial a^{[3]}}{\partial \textbf{z}^{[3]}}
\frac{\partial \textbf{z}^{[3]}}{\partial \textbf{a}^{[2]}} \frac{\partial \textbf{a}^{[2]}}{\partial \textbf{z}^{[2]}} 
\frac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}}
\]</div>
<p>Assuming for simplicity that the binary cross-entropy and sigmoid functions are used here as loss and activation functions, respectively:</p>
<div class="arithmatex">\[
\frac{\partial l}{\partial a^{[3]}} \frac{\partial a^{[3]}}{\partial z^{[3]}} = a^{[3]} - y
\]</div>
<div class="arithmatex">\[
\frac{\partial z^{[3]}}{\partial \textbf{a}^{[2]}} = \textbf{W}^{[3]}
\]</div>
<div class="arithmatex">\[
\frac{\partial \textbf{a}^{[2]}}{\partial \textbf{z}^{[2]}} = \textbf{a}^{[2]}(1-\textbf{a}^{[2]})
\]</div>
<div class="arithmatex">\[
\frac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = \textbf{a}^{[1]}
\]</div>
<p>which put together:</p>
<div class="arithmatex">\[
\frac{\partial l}{\partial \textbf{W}^{[2]}} = [(\textbf{a}^{[2]}(1-\textbf{a}^{[2]})) \cdot \textbf{W}^{[3]T}(a^{[3]} - y)] \textbf{a}^{[1]T}
\]</div>
<p>where <span class="arithmatex">\(\cdot\)</span> is used to refer to element-wise products. Similar results can be obtained for the bias vector
and for both weights and biases in the other layers as depicted in the figure below for a 2-layer NN:</p>
<p><img alt="BACKPROP_NN1" src="../figs/backprop_nn1.png" /></p>
<p>To conclude, the backpropagation equations in the diagram above are now generalized for the case 
of <span class="arithmatex">\(N_s\)</span> training samples <span class="arithmatex">\(\textbf{X} \in \mathbb{R}^{N \times N_s}\)</span> and a generic activation function
<span class="arithmatex">\(\sigma\)</span> whose derivative is denoted as <span class="arithmatex">\(\sigma'\)</span>. Here we still assume an output
of dimensionality one -- <span class="arithmatex">\(\textbf{Y} \in \mathbb{R}^{1 \times N_s}\)</span>:</p>
<div class="arithmatex">\[
\textbf{dZ}^{[2]}=\textbf{A}^{[2]}-\textbf{Y} \qquad (\textbf{A}^{[2]},\textbf{dZ}^{[2]} \in \mathbb{R}^{1 \times N_s})
\]</div>
<div class="arithmatex">\[
\textbf{dW}^{[2]}= \frac{1}{N_s} \textbf{dZ}^{[2]}\textbf{A}^{[1]T} \qquad (\textbf{A}^{[1]} \in \mathbb{R}^{N^{[1]} \times N_s})
\]</div>
<div class="arithmatex">\[
db^{[2]}= \frac{1}{N_s} \sum_i \textbf{dZ}_{:,i}^{[2]}
\]</div>
<div class="arithmatex">\[
\textbf{dZ}^{[1]}=\textbf{W}^{[2]^T}\textbf{dZ}^{[2]} \cdot \sigma'(\textbf{Z}^{[1]})  \qquad (\textbf{dZ}^{[1]} \in \mathbb{R}^{N^{[1]} \times N_s})
\]</div>
<div class="arithmatex">\[
\textbf{dW}^{[1]}= \frac{1}{N_s} \textbf{dZ}^{[1]}\textbf{X}^T
\]</div>
<div class="arithmatex">\[
\textbf{db}^{[1]}= \frac{1}{N_s} \sum_i \textbf{dZ}_{:,i}^{[1]}
\]</div>
<h2 id="initialization">Initialization</h2>
<p>Neural networks are hihgly nonlinear functions. The associated cost function used in the training
process in order to optimize the network weights and biases is therefore non-convex and contains
several local minima and saddle points.</p>
<p>A key component in non-convex optimization is represented by the starting guess of the parameters
to optimize, which in the context of deep learning is identified by initialization of weights and biases.
Whilst a proper initialization has been shown to be key to a succesful training of deep train NNs, 
this is a very active area of research as initialization strategies are so far mostly based on heuristic 
arguments and experience.</p>
<h3 id="zero-initialization">Zero initialization</h3>
<p>First of all, let's highlight a bad initialization choice that can compromise the training no matter the 
architecture of the network and other hyperparamters. A common choice in standard optimization in the absence
of any strong prior information is to initalize all the paramters to zero: if we decide to follow such a strategy
when training a NN, we will soon realize that training is stagnant due to the so called <em>symmetry problem</em>
(also referred to as <em>symmetric gradients</em>). Note that a similar situation arises also if we 
choose a constant values for weights and biases (e.g., <span class="arithmatex">\(c^{[1]}\)</span> for all the weights and biases in the first layer and 
<span class="arithmatex">\(c^{[2]}\)</span> for all the weights and biases in the second layer):</p>
<p>Let's take a look at this with an example:</p>
<p><img alt="ZEROINIT" src="../figs/zeroinit.png" /></p>
<p>Since the activations are constant vectors, back-propagation produces constant updates for the weights (and biases),
leading to weights and biases to never lose the initial symmetry.</p>
<h3 id="random-initialization">Random initialization</h3>
<p>A more appropriate way to initialize the weights of a neural network is to sample their
values from random distributions, for example:
$$
w_{ij}^{[.]} \sim \mathcal{N}(0, 0.01)
$$
where the choice of the variance is based on the following trade-off: too small variance leads to the 
vanishing gradient problem (i.e., slow training), whilst too high variance leads to the 
exploding gradient problem (i.e., unstable training). On the other hand, for the biases we can use zero or a constant value. If you remember, we have already
mentioned this when discussing the ReLU activation function: a good strategy to limit the amount of
negative values as input to this activation function is to choose a small constant bias (e.g., <span class="arithmatex">\(b=0.1\)</span>).</p>
<p>Whilst this approach provides a good starting point for stable training of neural networks, more advanced
initialization strategies have been proposed in the literature:</p>
<ul>
<li>
<p><strong>Uniform</strong>: the weights are initialized with uniform distributions whose variance depend on the
  number of units in the layer:
  $$
  w_{ij}^{[k]} \sim \mathcal{U}(-1/\sqrt{N^{[k]}}, 1/\sqrt{N^{[k]}})
  $$
  or
  $$
  w_{ij}^{[k]} \sim \mathcal{U}(-\sqrt{6/(N^{[k-1]}+N^{[k]})}, \sqrt{6/(N^{[k-1]}+N^{[k]})})
  $$
  This strategy is commonly used with FC layers.</p>
</li>
<li>
<p><strong>Xavier</strong>: the weights are initialized with normal distributions whose variance depend on the
  number of units in the layer:
  $$
  w_{ij}^{[k]} \sim \mathcal{N}(0, 1/N^{[k]})
  $$
  This strategy ensures that the variance remains the same across the layers. Xavier initialization
  is very popular especially in layers using Tanh activations.</p>
</li>
<li>
<p><strong>He</strong>: the weights are initialized with normal distributions whose variance depend on the
  number of units in the layer:
  $$
  w_{ij}^{[k]} \sim \mathcal{N}(0, 2/N^{[k]})
  $$
  This strategy ensures that the variance remains the same across the layers. Xavier initialization
  is very popular especially in layers using ReLU activations.</p>
</li>
</ul>
<p>Finally, if you are interest to learn more about initialization I reccomend reading (and reproducing)
the following blog posts: <a href="https://medium.com/@safrin1128/weight-initialization-in-neural-network-inspired-by-andrew-ng-e0066dc4a566">1</a>
and <a href="https://www.deeplearning.ai/ai-notes/initialization/">2</a>.</p>
<h2 id="why-nndeep-learning-took-off">WHY NN/DEEP LEARNING TOOK OFF</h2>
<p>Finally lets remark why theories are all from '80 but until early 2000 NN were not popular (niche field)</p>
<p>"The core ideas behind modern feedforward networks have not changed sub-stantially since the 1980s. 
The same back-propagation algorithm and the same approaches to gradient descent are still in use. Most of 
the improvement in neuralnetwork performance from 1986 to 2015 can be attributed to two factors. First,larger 
datasets have reduced the degree to which statistical generalization is achallenge for neural networks. Second, neural 
networks have become much larger,because of more powerful computers and better software infrastructure"</p>
<p>Plus a few algorithmic changes:
- "One of these algorithmic changes was the replacement of mean squared errorwith the cross-entropy family of loss functions. 
Mean squared error was popular inthe 1980s and 1990s but was gradually replaced by cross-entropy losses and the principle 
of maximum likelihood as ideas spread between the statistics community and the machine learning community. 
The use of cross-entropy losses greatly improved the performance of models with sigmoid and softmax outputs, whichhad previously 
Suﬀered from saturation and slow learning when using the meansquared error loss" --&gt; NOTE for regression ML with gaussianity assumption
on p(y|x) is still MSE, but only for this edge case!</p>
<ul>
<li>"change that has greatly improved the performanceof feedforward networks was the replacement of sigmoid hidden units 
  with piecewiselinear hidden units, such as rectiﬁed linear units. Rectiﬁcation using themax{0, z}function was 
  introduced in early neural network models and dates back at least as faras the cognitron and neocognitron (Fukushima, 1975, 1980)...
  This began to change in about 2009. Jarrett et al. (2009)observed that “using a rectifying nonlinearity is the single most 
  important factorin improving the performance of a recognition system,” --&gt; LEARN TO CHALLANGE STATUS QUO, SOMETIMES UNDERSTANDING OF
  PROBLEMS CAN CHANGE OR NEW EXTERNAL FACTORS (EG MORE DATA) MAKE SOMETHING THAT WAS WORSE BECOME BETTER... SIMILAR STORY IN FWI FOR GEOPHYSCISTS</li>
</ul>
<p>Also add mixture density 'network' (example of petroelastic with facies - Andrews paper) - https://towardsdatascience.com/a-hitchhikers-guide-to-mixture-density-networks-76b435826cca</p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../5_nn/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Basics of Neural Networks
              </div>
            </div>
          </a>
        
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.18f0862e.min.js"></script>
      <script src="../../assets/javascripts/bundle.994580cf.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.9c0e82ba.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>