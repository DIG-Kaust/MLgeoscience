
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Linear and Logistic Regression - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#linear-and-logistic-regression" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Linear and Logistic Regression
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../READMEcurvenotelocal/" class="md-nav__link">
        READMEcurvenotelocal
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Linear and Logistic Regression
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Linear and Logistic Regression
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#linear-regression" class="md-nav__link">
    Linear regression
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#logistic-regression" class="md-nav__link">
    Logistic regression
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_INN/" class="md-nav__link">
        Invertible Neural Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#linear-regression" class="md-nav__link">
    Linear regression
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#logistic-regression" class="md-nav__link">
    Logistic regression
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="linear-and-logistic-regression">Linear and Logistic Regression</h1>
<p>In the previous lecture we have learned how to optimize a generic loss function <span class="arithmatex">\(J_\theta\)</span> 
by modifying its free parameters <span class="arithmatex">\(\theta\)</span>. Whilst this is a very generic framework that can be used for 
various applications in different scientific field, from now on we will learn how to take advtange of
similar algorithms in the context of Machine Learning.</p>
<h2 id="linear-regression">Linear regression</h2>
<p>In preparation to our lecture on Neural Networks, here we consider the simplest machine learning model for regression, <em>linear regression</em>. Its simplicity lies in the fact that we will only consider a linear relationship between our inputs and targets:</p>
<p><img alt="LIN REG" src="../figs/reg_model.png" /></p>
<p>where <span class="arithmatex">\(\textbf{x}\)</span> is a training sample with <span class="arithmatex">\(N_f\)</span> features, <span class="arithmatex">\(\textbf{w}\)</span> is a vector of <span class="arithmatex">\(N_f\)</span> weights and <span class="arithmatex">\(b=w_0\)</span> is the
so-called bias term. The set of trainable parameters is therefore the combination of the weights and bias 
<span class="arithmatex">\(\boldsymbol\theta=[\textbf{w}, b] \in \mathbb{R}^{N_f+1}\)</span>. Similarly, the combination of the training sample and a 1-scalar is defined as 
<span class="arithmatex">\(\tilde{\textbf{x}}=[\textbf{x}, 1] \in \mathbb{R}^{N_f+1}\)</span>
The prediction <span class="arithmatex">\(\hat{y}\)</span> is simply obtained by linearly 
combining the different features of the input vector and adding the bias.</p>
<p>Despite its simplicity, linear regression (and more commonly multi-variate linear regression) has been successfully used in 
a variety of geoscientific tasks, examples of such a kind are:</p>
<ul>
<li>
<p>rock-physics models, where a number of petrophysical parameters (e.g., porosity, shale content, depth) can be linearly regressed 
  in order to predict an elastic parameter of interest (e.g., dry bulk modulus);</p>
</li>
<li>
<p>time-to-depth conversion, where a velocity (or depth) prediction is generally made as a linear combination of two-way traveltime
  and other parameters such as seismic amplitudes and various derived attributes;</p>
</li>
<li>
<p>filling gaps in petrophysical well logs, where various petrophysical measurements (e.g., GR, NEU, DEN) are regressed to 
  estimate another quantity of interest (e.g., S-wave velocity of DTS) that is not directly available within a certain depth interval.</p>
</li>
</ul>
<p>Assuming availability of <span class="arithmatex">\(N_s\)</span> training samples, the input training matrix and output training vector of a linear regression 
model is written as:</p>
<div class="arithmatex">\[
\mathbf{X}_{train} = [\tilde{\mathbf{x}}^{(1)}, \tilde{\mathbf{x}}^{(2)}, ..., \tilde{\mathbf{x}}^{(N_s)}] \in \mathbb{R}^{N_f+1 \times N_s}, \quad
\mathbf{y}_{train} = [y^{(1)}, y^{(2)}, y^{(N_s)}] \in \mathbb{R}^{N_s \times 1}
\]</div>
<p>Finally, the <strong>model</strong> can be compactly written as:</p>
<div class="arithmatex">\[
\hat{\textbf{y}}_{train} = \textbf{X}_{train}^T \boldsymbol\theta
\]</div>
<p>Next, we need to define a metric (i.e., cost function) which we can use to optimize for the free parameters <span class="arithmatex">\(\boldsymbol\theta\)</span>.
For regression problems, a common metric of goodness is the L2 norm or MSE (Mean Square Error):</p>
<div class="arithmatex">\[
J_\theta = MSE(\textbf{y}_{train}, \hat{\textbf{y}}_{train}) = \frac{1}{N_s} || \textbf{y}_{train} - \hat{\textbf{y}}_{train}||_2^2 = 
\frac{1}{N_s} \sum_i^{N_s} (y_{train}^{(i)}-\hat{y}_{train}^{(i)})^2
\]</div>
<p>Based on our previous lecture on optimization, we need to find the best set of coefficients <span class="arithmatex">\(\theta\)</span> that minimizes the MSE:</p>
<div class="arithmatex">\[
\hat{\theta} = min_\theta  J_\theta \rightarrow \theta_{i+1} = \theta_i - \alpha \nabla J_\theta
\]</div>
<p>However, since this is a linear inverse problem we can write the analytical solution of the minimization problem as:</p>
<div class="arithmatex">\[
\hat{\theta} = (\textbf{X}_{train}^T \textbf{X}_{train})^{-1} \textbf{X}_{train}^T \textbf{y}_{train}
\]</div>
<p>which can be obtained by inverting a <span class="arithmatex">\(N_s \times N_s\)</span> matrix. </p>
<p>An important observation, which lies at the core of most Machine Learning algorithms, is that once the model is trained 
on the <span class="arithmatex">\(N_s\)</span> available input-target pairs, the estimated <span class="arithmatex">\(\hat{\theta}\)</span> coefficients can be used to make <em>inference</em> on any new unseen data:</p>
<div class="arithmatex">\[
y_{test} = \tilde{\textbf{x}}^T_{test} \hat{\theta}
\]</div>
<p>To conclude, once a linear regression model has been trained, a variety of measures exist to assess the goodness of the model. Whilst the same 
metric used for training, the mean-square error, can be used to assess the model performance, other metrics are represented by the Pearson coefficient 
(<span class="arithmatex">\(R^2\)</span>) and the mean-absolute error (MAE).</p>
<h2 id="logistic-regression">Logistic regression</h2>
<p>Simply put, logistic regression is an extension of linear regression to the problem of binary classification. Whilst the model used
by logistic regression is the same linear model described above, this will be coupled with a nonlinear 'activation' function that enforces the
outcome of the entire model to be bounded between 0 and 1 (i.e., a probability). In other words, whilst the input training matrix is the same
as that of linear regression, the output training vector becomes:</p>
<div class="arithmatex">\[
y_{train} = \{0, 1\}
\]</div>
<p>A variety of applications of such a simple model can be found in geoscience, one common example is represent by net pay prediction 
from petrophysical logs.</p>
<p>Given a single pair of training samples <span class="arithmatex">\(\textbf{x}, y\)</span>, a mathematical <strong>model</strong> for logistic regression can be compactly written as:</p>
<div class="arithmatex">\[
\hat{y} = f_\theta(\textbf{x}) = P(y=1 | \textbf{x}) \in (0,1)
\]</div>
<p>or in other words, the input vector <span class="arithmatex">\(\textbf{x}\)</span> is fed through a nonlinear model <span class="arithmatex">\(f_\theta\)</span> whose output is a scalar number between 0 and 1 that 
represents the probability of the target output to be 1.</p>
<p>Considering now a set of <span class="arithmatex">\(N_s\)</span> training pairs, the model can be explicitly written as:</p>
<div class="arithmatex">\[
\hat{\textbf{y}}_{train} = f_\theta(\textbf{X}_{train}) = \sigma(\textbf{X}_{train}^T \boldsymbol\theta)
\]</div>
<p>where <span class="arithmatex">\(\sigma\)</span> is a sigmoid function as shown in figure below:</p>
<p><img alt="SIGMOID" src="../figs/sigmoid.png" /></p>
<p>Once again, let's define a cost function that we can use to optimize the model parameters. For binary classification, a common metric of goodness
is represented by the so-called <em>binary cross-entropy</em>:</p>
<div class="arithmatex">\[
\mathscr{L}(y_{train}^{(i)}, \hat{y}_{train}^{(i)}) = -(y_{train}^{(i)} log(\hat{y}_{train}^{(i)}) + 
(1-y_{train}^{(i)})) log(1- \hat{y}_{train}^{(i)}))
\]</div>
<p>and</p>
<div class="arithmatex">\[
J_\theta = \frac{1}{N_s} \sum_i^{N_s} \mathscr{L}(y_{train}^{(i)}, \hat{y}_{train}^{(i)})
\]</div>
<p>Let's gain some intuition onto why this is a good cost function. More specifically, we consider with a drawing the two cases separately.
First the case of positive target, <span class="arithmatex">\(y_{train}^{(i)}=1\)</span> </p>
<p><img alt="LOGISTIC POSITIVE" src="../figs/logistic_pos.png" /></p>
<p>and then the case of negative target, <span class="arithmatex">\(y_{train}^{(i)}=0\)</span>:</p>
<p><img alt="LOGISTIC NEGATIVE" src="../figs/logistic_neg.png" /></p>
<p>Our drawings clearly show the validity of such a cost function in both cases. The further away is the prediction from the true label the higher the
resulting cost function. Similar to the case of linear regression, we can now update the model parameters by minimizing the cost function:</p>
<div class="arithmatex">\[
\hat{\theta} = min_\theta  J_\theta \rightarrow \theta_{i+1} = \theta_i - \alpha \nabla J_\theta
\]</div>
<p>However a major difference arises here. Whilst it is easy to compute the derivative of the MSE with respect to the model parameters <span class="arithmatex">\(\theta\)</span>, and 
even more since the model is linear an analytical solution can be found (as shown above), this is not the case of the cost function of the logistic
regression model.</p>
<p>The good news here is that there exist a systematic approach to computing the derivative of a composite function (i.e., <span class="arithmatex">\(f(x)=f_N(...f_2(f_1(x)))\)</span>), which
simply relies on the well-known <em>chain rule</em> of functional analysis. This method is referred to in the mathematical community as Automatic Differentiation (AD),
and more likely so as Back-propagation in the ML community. As this lies as the foundation of the training process for neural networks, we will get into details 
later in the text. At this point, it suffices to say that if we have a composite function like the one above, its derivative with respect to <span class="arithmatex">\(x\)</span> can be written as:</p>
<div class="arithmatex">\[
\frac{\partial f}{\partial x} = \frac{\partial f_N}{\partial f_{N-1}} ... \frac{\partial f_2}{\partial f_1} \frac{\partial f_1}{\partial x}
\]</div>
<p>where the derivative is simply the product of all derivatives over the chain of operations of the composite function.
Note that in practice it is more common to compute this chain rule in reverse order, from left to right in the equation above.</p>
<p>We generally rely on the built-in functionalities of deep learning libraries such as Tensorflow or PyTorch to compute such derivaties, we will perform here a full derivation for the simple case of logistic regression. In order to do so, we introduce a very useful mathamatical tool that we use to keep track of a chain of operations and later, we know how to evaluate the associated gradient. This tool is usually known as <em>computational graph</em>. More specifically, instead of writing the entire logistic regression model compactly in a single equation, we divide it here into its atomic components:</p>
<div class="arithmatex">\[
z = \textbf{x}^T \boldsymbol\theta, \quad
a = \sigma(z), \quad \mathscr{L} = -(y log(a) + (1-y)log(1-a))
\]</div>
<p>such that the derivative of the loss function with respect to the model parameters becomes:</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L} }{\partial \boldsymbol\theta} = \frac{\partial \mathscr{L} }{\partial a}
\frac{\partial a }{\partial z} \frac{\partial z}{\partial \boldsymbol\theta}
\]</div>
<p>The forward and backward passes (as described in software frameworks like PyTorch) can be visually displayed as follows:</p>
<p><img alt="BACKPROP" src="../figs/backprop.png" /></p>
<p>Let's start from <span class="arithmatex">\(\partial \mathscr{L} / \partial a\)</span>:</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial a} = -\frac{y}{a} + \frac{1-y}{1-a} = \frac{-y(1-a) + (1-y)a}{a (1-a)}
\]</div>
<p>and <span class="arithmatex">\(\partial a / \partial z\)</span>:</p>
<div class="arithmatex">\[
\frac{\partial a}{\partial z} = a(1-a)
\]</div>
<p>which we can combine together to obtain a simplified formula for the derivative of the loss function of the output of the weighted summation (<span class="arithmatex">\(z\)</span>)</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial z} = \frac{\partial \mathscr{L}}{\partial a} \frac{\partial a}{\partial \sigma} = 
-y(1-a) + (1-y)a = a - y = dz
\]</div>
<p>Finally we differentiate between the weights and the bias to obtain:</p>
<div class="arithmatex">\[
\frac{\partial z}{\partial w_i} = x_i, \quad \frac{\partial z}{\partial b} = 1
\]</div>
<p>such that:</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial w_i} = dz \cdot x_i = dw_i, \quad \frac{\partial \mathscr{L}}{\partial b} = dz = db
\]</div>
<p>Having found the gradients, we can now update the parameters as discussed above:</p>
<div class="arithmatex">\[
w_i \leftarrow w_i - \alpha \frac{\partial \mathscr{L}}{\partial w_i} = w_i - \alpha dw_i, \quad
b \leftarrow b - \alpha \frac{\partial \mathscr{L}}{\partial b} = b - \alpha db
\]</div>
<p>which can be easily modified in the case of multiple training samples:</p>
<div class="arithmatex">\[
w_i \leftarrow  w_i - \alpha \sum_{j=1}^{N_s} dw_i^{(j)}, \quad
b \leftarrow b - \alpha \sum_{j=1}^{N_s} db^{(j)}
\]</div>
<p>We can now summarize a single step of training for <span class="arithmatex">\(N_s\)</span> training samples for the logistic regression model:</p>
<p><span class="arithmatex">\(\textbf{z}=\textbf{X}_{train}^T \boldsymbol \theta\)</span></p>
<p><span class="arithmatex">\(\textbf{a} = \sigma(\textbf{z})\)</span></p>
<p><span class="arithmatex">\(\textbf{dz} = \textbf{a} - \textbf{y}\)</span></p>
<p><span class="arithmatex">\(\textbf{dw} = \frac{1}{N_s} \textbf{X}_{train} \textbf{dz}\)</span></p>
<p><span class="arithmatex">\(db = \frac{1}{N_s} \textbf{1}^T \textbf{dz}\)</span></p>
<p><span class="arithmatex">\(\textbf{w} \leftarrow \textbf{w} - \alpha \textbf{dw}\)</span></p>
<p><span class="arithmatex">\(b \leftarrow b - \alpha db\)</span></p>
<p>To conclude, let's turn our attention into some of the <em>evaluation metrics</em> that are commonly used to assess the performance of a 
classification model (or classifier). Note that these metrics can be used for the logistic regression model discussed here as well as for
other more advanced models discussed later in the course. </p>
<p>In general for binary classification we have two possible outcomes (positive/negative or true/false) for both the true labels <span class="arithmatex">\(y\)</span> and the 
predicted labels <span class="arithmatex">\(\hat{y}\)</span>. We can therefore define 4 scenarios:</p>
<p><img alt="CLASSMETRICS" src="../figs/classification_metrics.png" /></p>
<p>and a number of complementary metrics (all bounded between 0 and 1) can be defined. Note that no metric is better than the others, the importance of one metric over another is context dependant.</p>
<ol>
<li><strong>Precision</strong>: <span class="arithmatex">\(Pr=\frac{TP}{TP+FP}\)</span>, percentage of correct positive predictions over the overall positive predictions. This measure is 
   appropriate when minimizing false positives is the focus. In the geoscientific context, 
   this may represent a meaningful metric for applications where the main interest is that of predicting the smallest possible number of false positives, 
   whilst at the same time accepting to miss out on some of positives (false negatives). This could be the case when we want to predict hydrocarbon bearing reservoirs from 
   seismic data, where we know already that we will not be able to drill wells into many of them. It is therefore important that even if we make very few
   positive predictions these must be accurate, whilst the cost of missing other opportunities is not so high. On the other hand, this measure is blind to the
   predictions of real positive cases to be chosen to be part of the negative class (false negative);</li>
<li><strong>Recall</strong>: <span class="arithmatex">\(Rc=\frac{TP}{TP+FN} = \frac{TP}{P}\)</span>, percentage of correct positive predictions over the overall positive occurrences. This measure is
   appropriate when minimizing false negatives is the focus. An opposite scenario to the one presented above is represented by the case of a classifier trained to predict pressure kicks whilst drilling a well. 
   In this case, we are not really concerned with making a few mistakes where we predict a kick when this is not likely to happen (False Positive); 
   of course, this may slow down the drilling process but it is nowhere near as dramatic as the case in which we do not predict a kick which is going to happen (False Negative); 
   a high recall is therefore what we want, as this is an indication of the fact that the model does not miss out on many positive cases. 
   Of course, a model that always provides a positive prediction will have a recall of 1 (FN=0), indication of the fact that a high recall is not 
   always an indication of a good model;</li>
<li><strong>Accuracy</strong>: <span class="arithmatex">\(Ac=\frac{TP+TN}{TP+TN+FP+FN}=\frac{TP+TN}{P+N}\)</span>, percentage of correct predictions over the total number of cases. 
   This measure combines both error types (in the denominator), it is therefore a more global measure of the quality of the model.</li>
<li><strong>F1-Score</strong>: <span class="arithmatex">\(2 \frac{Pr \cdot Rc}{Pr+Rc}\)</span>, represents a way to combine precision and recall into a single measure that captures both properties.</li>
</ol>
<p>Finally, a more complete description of the performance of a model is given by the so-called <em>confusion matrix</em>, which for the case of binary classification
is just the <span class="arithmatex">\(2 \times 2\)</span> table in the figure above. This table can be both unnormalized, where each cell simply contains the number of samples which satisfy
the specific combination of real and predicted labels, or normalized over either rows or columns.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../03_gradopt/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Gradient-based optimization" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Gradient-based optimization
            </div>
          </div>
        </a>
      
      
        
        <a href="../05_nn/" class="md-footer__link md-footer__link--next" aria-label="Next: Basics of Neural Networks" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Basics of Neural Networks
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>