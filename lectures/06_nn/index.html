
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.2.8">
    
    
      
        <title>More on Neural Networks - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.cb6bc1d0.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.39b8e14a.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#more-on-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      <div class="md-header-nav__ellipsis">
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            
              More on Neural Networks
            
          </span>
        </div>
      </div>
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
      
      <label class="md-nav__link" for="nav-4">
        Lectures
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="nav-4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          More on Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        More on Neural Networks
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#backpropagation" class="md-nav__link">
    Backpropagation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    Initialization
  </a>
  
    <nav class="md-nav" aria-label="Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-initialization" class="md-nav__link">
    Zero initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    Random initialization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-deep-learning-took-off-in-the-last-century" class="md-nav__link">
    Why Deep Learning took off in the last century
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-likelihood-estimators" class="md-nav__link">
    Maximum likelihood estimators
  </a>
  
    <nav class="md-nav" aria-label="Maximum likelihood estimators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    Regression
  </a>
  
    <nav class="md-nav" aria-label="Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-regression" class="md-nav__link">
    Linear regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-layer-perceptron-regression" class="md-nav__link">
    Multi-layer perceptron regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    Classification
  </a>
  
    <nav class="md-nav" aria-label="Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#binary-classification" class="md-nav__link">
    Binary classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-label-classification" class="md-nav__link">
    Multi-label classification
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../11_vae/" class="md-nav__link">
        VAE
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../12_gan/" class="md-nav__link">
        GANs
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_autoencoder/" class="md-nav__link">
        Autoencoders
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_pca/" class="md-nav__link">
        PCA
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#backpropagation" class="md-nav__link">
    Backpropagation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#initialization" class="md-nav__link">
    Initialization
  </a>
  
    <nav class="md-nav" aria-label="Initialization">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#zero-initialization" class="md-nav__link">
    Zero initialization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#random-initialization" class="md-nav__link">
    Random initialization
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-deep-learning-took-off-in-the-last-century" class="md-nav__link">
    Why Deep Learning took off in the last century
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#maximum-likelihood-estimators" class="md-nav__link">
    Maximum likelihood estimators
  </a>
  
    <nav class="md-nav" aria-label="Maximum likelihood estimators">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#regression" class="md-nav__link">
    Regression
  </a>
  
    <nav class="md-nav" aria-label="Regression">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#linear-regression" class="md-nav__link">
    Linear regression
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-layer-perceptron-regression" class="md-nav__link">
    Multi-layer perceptron regression
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#classification" class="md-nav__link">
    Classification
  </a>
  
    <nav class="md-nav" aria-label="Classification">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#binary-classification" class="md-nav__link">
    Binary classification
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#multi-label-classification" class="md-nav__link">
    Multi-label classification
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="more-on-neural-networks">More on Neural Networks</h1>
<p>In this lecture, we will delve into some more advanced topics associated to the creation and training of deep 
neural networks.</p>
<h2 id="backpropagation">Backpropagation</h2>
<p>First of all, once a neural network architecture has been defined for the problem at hand, we need a method
that can learn the best set of free parameters of such nonlinear function represented as <span class="arithmatex">\(f_\theta\)</span>.</p>
<p>More specifically, we want to initialize the network with some random weights and biases (we will soon discuss how
such initialization can be performed) and use the training data at hand to improve our weights and biases in order
to minimize a certain loss function. Whilst this can be easily done by means of gradient based optimizers like those
presented in Lecture 3, a key ingredient that we need to provide to such algorithms is represented by the gradient
of the loss function with respect to each and every weight and bias parameters. </p>
<p>We have already alluded at a technique that can do so whilst discussing a simple logistic regression model. This
is generally referred to by the ML community as <em>back-propagation</em> and more broadly by the mathematical community
as <em>Reverse Automatic Differentiation</em>. Let's start by taking the same schematic diagram used for the logistic regression
example and generalize it to a N-layer NN:</p>
<p><img alt="BACKPROP_NN" src="../figs/backprop_nn.png" /></p>
<p>The main difference here, which we will need to discuss in details, is the fact that in the forward pass 
we feed the input into a stack of linear layers prior to computing the loss function. The backpropagation
does need to be able to keep track of the chain of operations (i.e., computational graph) and traverse it
back. However, as already done for the logistic regression model, all we need to do is to write the entire
chain of operations as a chain of atomic ones that we can then easily traverse back. Let's do this for
the network above and a single training sample <span class="arithmatex">\(\textbf{x}\)</span>:</p>
<div class="arithmatex">\[
\textbf{z}^{[1]} = \textbf{W}^{[1]}\textbf{x} + \textbf{b}^{[1]}, \quad
\textbf{a}^{[1]} = \sigma(z^{[1]}),
\]</div>
<div class="arithmatex">\[
\textbf{z}^{[2]} = \textbf{W}^{[2]}\textbf{a}^{[1]} + \textbf{b}^{[2]}, \quad
\textbf{a}^{[2]} = \sigma(z^{[2]}),
\]</div>
<div class="arithmatex">\[
\textbf{z}^{[3]} = \textbf{W}^{[3]}\textbf{a}^{[2]} + \textbf{b}^{[3]}, \quad
a^{[3]} = \sigma(z^{[3]}),
\]</div>
<div class="arithmatex">\[
l = \mathscr{L}(y,a^{[3]}).
\]</div>
<p>Given such a chain of operations, we are now able to find the derivatives of the loss function with
respect to any of the weights or biases. As an example we consider here <span class="arithmatex">\(\partial l / \partial \textbf{W}^{[2]}\)</span>:</p>
<div class="arithmatex">\[
\frac{\partial l}{\partial \textbf{W}^{[2]}} = \frac{\partial l}{\partial a^{[3]}} \frac{\partial a^{[3]}}{\partial \textbf{z}^{[3]}}
\frac{\partial \textbf{z}^{[3]}}{\partial \textbf{a}^{[2]}} \frac{\partial \textbf{a}^{[2]}}{\partial \textbf{z}^{[2]}} 
\frac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}}
\]</div>
<p>Assuming for simplicity that the binary cross-entropy and sigmoid functions are used here as the loss and activation functions, respectively:</p>
<div class="arithmatex">\[
\frac{\partial l}{\partial a^{[3]}} \frac{\partial a^{[3]}}{\partial z^{[3]}} = a^{[3]} - y
\]</div>
<div class="arithmatex">\[
\frac{\partial z^{[3]}}{\partial \textbf{a}^{[2]}} = \textbf{W}^{[3]}
\]</div>
<div class="arithmatex">\[
\frac{\partial \textbf{a}^{[2]}}{\partial \textbf{z}^{[2]}} = \textbf{a}^{[2]}(1-\textbf{a}^{[2]})
\]</div>
<div class="arithmatex">\[
\frac{\partial \textbf{z}^{[2]}}{\partial \textbf{W}^{[2]}} = \textbf{a}^{[1]}
\]</div>
<p>which put together:</p>
<div class="arithmatex">\[
\frac{\partial l}{\partial \textbf{W}^{[2]}} = [(\textbf{a}^{[2]}(1-\textbf{a}^{[2]})) \cdot \textbf{W}^{[3]T}(a^{[3]} - y)] \textbf{a}^{[1]T}
\]</div>
<p>where <span class="arithmatex">\(\cdot\)</span> is used to refer to element-wise products. Similar results can be obtained for the bias vector
and for both weights and biases in the other layers as depicted in the figure below for a 2-layer NN:</p>
<p><img alt="BACKPROP_NN1" src="../figs/backprop_nn1.png" /></p>
<p>To conclude, the backpropagation equations in the diagram above are now generalized for the case 
of <span class="arithmatex">\(N_s\)</span> training samples <span class="arithmatex">\(\textbf{X} \in \mathbb{R}^{N \times N_s}\)</span> and a generic activation function
<span class="arithmatex">\(\sigma\)</span> whose derivative is denoted as <span class="arithmatex">\(\sigma'\)</span>. Here we still assume an output
of dimensionality one -- <span class="arithmatex">\(\textbf{Y} \in \mathbb{R}^{1 \times N_s}\)</span>:</p>
<div class="arithmatex">\[
\textbf{dZ}^{[2]}=\textbf{A}^{[2]}-\textbf{Y} \qquad (\textbf{A}^{[2]},\textbf{dZ}^{[2]} \in \mathbb{R}^{1 \times N_s})
\]</div>
<div class="arithmatex">\[
\textbf{dW}^{[2]}= \frac{1}{N_s} \textbf{dZ}^{[2]}\textbf{A}^{[1]T} \qquad (\textbf{A}^{[1]} \in \mathbb{R}^{N^{[1]} \times N_s})
\]</div>
<div class="arithmatex">\[
db^{[2]}= \frac{1}{N_s} \sum_i \textbf{dZ}_{:,i}^{[2]}
\]</div>
<div class="arithmatex">\[
\textbf{dZ}^{[1]}=\textbf{W}^{[2]^T}\textbf{dZ}^{[2]} \cdot \sigma'(\textbf{Z}^{[1]})  \qquad (\textbf{dZ}^{[1]} \in \mathbb{R}^{N^{[1]} \times N_s})
\]</div>
<div class="arithmatex">\[
\textbf{dW}^{[1]}= \frac{1}{N_s} \textbf{dZ}^{[1]}\textbf{X}^T
\]</div>
<div class="arithmatex">\[
\textbf{db}^{[1]}= \frac{1}{N_s} \sum_i \textbf{dZ}_{:,i}^{[1]}
\]</div>
<h2 id="initialization">Initialization</h2>
<p>Neural networks are highly nonlinear functions. The associated cost function used in the training
process in order to optimize the network weights and biases is therefore non-convex and contains
several local minima and saddle points.</p>
<p>A key component in non-convex optimization is represented by the starting guess of the parameters
to optimize, which in the context of deep learning is identified by initialization of weights and biases.
Whilst a proper initialization has been shown to be key to a succesful training of deep train NNs, 
this is a very active area of research as initialization strategies are so far mostly based on heuristic 
arguments and experience.</p>
<h3 id="zero-initialization">Zero initialization</h3>
<p>First of all, let's highlight a bad choice of initialization that can compromise the training no matter the 
architecture of the network and other hyperparamters. A common choice in standard optimization in the absence
of any strong prior information is to initalize all the paramters to zero: if we decide to follow such a strategy
when training a NN, we will soon realize that training is stagnant due to the so called <em>symmetry problem</em>
(also referred to as <em>symmetric gradients</em>). Note that a similar situation arises also if we 
choose a constant values for weights and biases (e.g., <span class="arithmatex">\(c^{[1]}\)</span> for all the weights and biases in the first layer and 
<span class="arithmatex">\(c^{[2]}\)</span> for all the weights and biases in the second layer):</p>
<p>Let's take a look at this with an example:</p>
<p><img alt="ZEROINIT" src="../figs/zeroinit.png" /></p>
<p>Since the activations are constant vectors, back-propagation produces constant updates for the weights (and biases),
leading to weights and biases to never lose the initial symmetry.</p>
<h3 id="random-initialization">Random initialization</h3>
<p>A more appropriate way to initialize the weights of a neural network is to sample their
values from random distributions, for example:
$$
w_{ij}^{[.]} \sim \mathcal{N}(0, 0.01)
$$
where the choice of the variance is based on the following trade-off: too small variance leads to the 
vanishing gradient problem (i.e., slow training), whilst too high variance leads to the 
exploding gradient problem (i.e., unstable training). On the other hand, for the biases we can use zero or a constant value. If you remember, we have already
mentioned this when discussing the ReLU activation function: a good strategy to limit the amount of
negative values as input to this activation function is to choose a small constant bias (e.g., <span class="arithmatex">\(b=0.1\)</span>).</p>
<p>Whilst this approach provides a good starting point for stable training of neural networks, more advanced
initialization strategies have been proposed in the literature:</p>
<ul>
<li>
<p><strong>Uniform</strong>: the weights are initialized with uniform distributions whose variance depend on the
  number of units in the layer:
  $$
  w_{ij}^{[k]} \sim \mathcal{U}(-1/\sqrt{N^{[k]}}, 1/\sqrt{N^{[k]}})
  $$
  or
  $$
  w_{ij}^{[k]} \sim \mathcal{U}(-\sqrt{6/(N^{[k-1]}+N^{[k]})}, \sqrt{6/(N^{[k-1]}+N^{[k]})})
  $$
  This strategy is commonly used with FC layers.</p>
</li>
<li>
<p><strong>Xavier</strong>: the weights are initialized with normal distributions whose variance depend on the
  number of units in the layer:
  $$
  w_{ij}^{[k]} \sim \mathcal{N}(0, 1/N^{[k]})
  $$
  This strategy ensures that the variance remains the same across the layers. Xavier initialization
  is very popular especially in layers using Tanh activations.</p>
</li>
<li>
<p><strong>He</strong>: the weights are initialized with normal distributions whose variance depend on the
  number of units in the layer:
  $$
  w_{ij}^{[k]} \sim \mathcal{N}(0, 2/N^{[k]})
  $$
  This strategy ensures that the variance remains the same across the layers. He initialization
  is very popular especially in layers using ReLU activations.</p>
</li>
</ul>
<h2 id="why-deep-learning-took-off-in-the-last-century">Why Deep Learning took off in the last century</h2>
<p>Before moving onto the last topic of this lecture, a unified statistical view of loss functions in deep learning, 
let's try to answer a question that many of you may ask: <em>what makes NNs so popular these days and why deep learning took off in the 
last decade?</em></p>
<p>By now, we have made ourself familar with the concept of neural networks, learned about its basic building block (the so-called perceptron) and
how by simply horizontally and vertically stacking multiple percetrons we can create universal function approximators that can be trained to learn
very complex nonlinear relationships between inputs and targets (provided availability of a large enough amount of training data). The process
of creating and training NNs relies on the following four key ingredients:</p>
<ul>
<li><em>linear algebra operations</em>: matrix-vector and matrix-matrix multiplications (at least within the context of FC networks);</li>
<li><em>activations</em>: nonlinear functions that enable the learning of complex nonlinear mappings;</li>
<li><em>loss functions</em>: functions that can be used to evaluate the goodness of the model in terms of predicting targets from inputs; </li>
<li><em>learning algorithms</em>: optimization methods that can produce the best weights and biases using gradient information;</li>
</ul>
<p>Eventually, most of the underlying theory of NNs was already fairly mature in 
70s and 80s; nevertheless, until the early 2000, research in the field of artificial neural networks was still considered a 
niche domain mostly theoretical and with little practical implications. So, what did lead to the renaissance of Deep Learning?</p>
<p>Two key factors in the popularity and success of Neural Networks growth are undoubtedly:</p>
<ul>
<li><em>larger datasets</em>: with the growth of the internet and social media, a digital revolution has started since the beginning of the
  new century, where datasets of ever increasing size can be easily sourced. This applies both to images and text as well as audio 
  and video content.</li>
<li><em>larger networks</em>: with the emergence of new hardware technology such as GPUs, training large deep networks is nowadays possible,
  not only for large corporations like Google or Microsoft but also in Academia or for small- and medium-size enterprises that 
  want to leverage their data to make data-driven business decisions.</li>
</ul>
<p>Alongside the data and harwdare revolution, a number of important algorithmic discoveries have also led to faster, more robust 
training of NNs making this process easier and more accessible to domain scientists in a variety of scientific fields. Some of them
have been already discussed, but we wish here to put more emphasis on them:</p>
<ul>
<li><em>MSE --&gt; Cross-entropy</em>: whilst in the past the mean square error (MSE) loss was used for pretty much every task, nowadays classification
  or semantic segmentation tasks are more commonly solved by means of Cross-entropy loss functions. This shift in training strategy is mostly
  due to the fact that the ML community and the statistical community got closer to each other in the last two decades, which lead to 
  the development of strong statistical foundations in the theory of deep learning;</li>
<li><em>Sigmoid --&gt; ReLU</em>: whilst continuous, differentiable activation functions used to be a must in the past mostly due to the belief that
  gradient descent algorithms (and back-propagation) needs these kind of functions to behave correctly, it is now clear that this constraint
  can be greatly related. Piece-wise linear activation functions like ReLU are nowadays not only used but pretty much the de-facto standard
  for hidden layers in deep neural networks. Jarrett et al. (2009) observed that <em>"using a rectifying nonlinearity is the single most 
  important factor in improving the performance of a recognition system"</em>.</li>
</ul>
<h2 id="maximum-likelihood-estimators">Maximum likelihood estimators</h2>
<p>To conclude, we would like to revisit the loss functions already introduced in the context of linear and logistic 
regression models and introduce some other loss functions that are commonly employed to train neural networks.</p>
<p>However, whilst so far we have chosen different loss functions for each task (regression vs. classification) without really
providing a statistical motivation of such choices, in this section we will instead try to define a common framework based on the concept of
Maximum Likelihood Estimations (MLE).</p>
<p>Let's start by considering a set of samples drawn from the true (but unknown) distribution:</p>
<div class="arithmatex">\[
\mathbf{X} = \{ \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, ..., \mathbf{x}^{(N_s)} \} \sim p_{data}(\mathbf{X}) 
\]</div>
<p>Second, a parametric family of probability distribution is defined:</p>
<div class="arithmatex">\[
p_{model}(\mathbf{X}; \theta) 
\]</div>
<p>This distribution maps any vector <span class="arithmatex">\(\mathbf{x}\)</span> to a real number and is generally referred to as the 
likelihood function. Its free parameters <span class="arithmatex">\(\theta\)</span> must be chosen
such that this probability distribution is as close as possible to the true one.</p>
<p>As an example, if we consider a multi-variate gaussian distribution with uncorrelated members, the
free parameters become <span class="arithmatex">\(\theta = \{ \boldsymbol \mu, \sigma\}\)</span> and the probability density function
becomes:</p>
<div class="arithmatex">\[
p_{model}(\mathbf{x}; \{ \boldsymbol \mu, \sigma\}) = 
\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{||\mathbf{x} - \boldsymbol \mu||_2^2}{2 \sigma^2}}
\]</div>
<p>We can now define the MLE as follows:</p>
<div class="arithmatex">\[
\theta_{ML} = \underset{\theta} {\mathrm{argmax}} \; p_{model}(\mathbf{X}; \theta) 
\]</div>
<p>Assuming now statistical independence between the samples <span class="arithmatex">\(\mathbf{x}^{(i)}\)</span>, the equation above can
be rewritten as:</p>
<div class="arithmatex">\[
\begin{aligned}
\theta_{ML} &amp;= \underset{\theta} {\mathrm{argmax}} \; \prod_{i=1}^{N_s} p_{model}(\mathbf{x}^{(i)}; \theta) \\
&amp;= \underset{\theta} {\mathrm{argmax}} \; \sum_{i=1}^{N_s} log(p_{model}(\mathbf{x}^{(i)}; \theta)) \\
&amp;\approx \underset{\theta} {\mathrm{argmax}} \; E_{\mathbf{x} \sim p_{data}} [ log(p_{model}(\mathbf{x}; \theta))] \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; - E_{\mathbf{x} \sim p_{data}} [ log(p_{model}(\mathbf{x}; \theta))]
\end{aligned}
\]</div>
<p>Simply put, maximizing the parametric probability density 
function is shown to be equivalent to <em>minimizing the negative log likelihood</em> of the same distribution.
An optimization problem must be therefore solved to find the most suitable free parameters. Going back
to the multi-variate gaussian example, let's assume we are interested to estimate the mean (whilst we keep the
variance fixed):</p>
<div class="arithmatex">\[
\begin{aligned}
\boldsymbol \mu_{ML} &amp;= \underset{\boldsymbol \mu} {\mathrm{argmin}} \; 
- \sum_{i=1}^{N_s} log \Big( \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{||\mathbf{x}^{(i)} - \boldsymbol \mu||_2^2}{2 \sigma^2}} \Big) \\
&amp;= \underset{\boldsymbol \mu} {\mathrm{argmin}} \; \sum_{i=1}^{N_s} \frac{||\mathbf{x}^{(i)} - \boldsymbol \mu||_2^2}{2 \sigma^2}
\end{aligned}
\]</div>
<p>Computing the gradient and imposing it to be zero gives us the point estimate of <span class="arithmatex">\(\boldsymbol \mu_{ML}\)</span>:</p>
<div class="arithmatex">\[ \frac{\partial -\sum_i log p}{\partial \boldsymbol \mu} = 0 \rightarrow \sum_{i=1}^{N_s} (\mathbf{x}^{(i)} - \boldsymbol \mu) = 0 
\rightarrow \boldsymbol \mu_{ML} = \frac{1}{N_s} \sum_{i=1}^{N_s} \mathbf{x}^{(i)}
\]</div>
<p>which is nothing more than the well-known <em>sample mean</em> of the distribution.</p>
<p>In order to apply the same framework to learning problems like thoose arising in DL, the ML estimation is now 
extended to the case of conditional probabilities where we are given a set of training pairs <span class="arithmatex">\((\mathbf{x}, y)^{(i)}\)</span>:</p>
<div class="arithmatex">\[
\begin{aligned}
\theta_{ML} &amp;= \underset{\theta} {\mathrm{argmax}} \; p_{model}(Y | \mathbf{X}; \theta) \\
&amp;= ... \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; - E_{\mathbf{x},y \sim p_{data}} [ log(p_{model}(y|\mathbf{x}; \theta))]
\end{aligned}
\]</div>
<h3 id="regression">Regression</h3>
<h4 id="linear-regression">Linear regression</h4>
<p>Let's first apply this framework to a simple linear regression problem. Here, under the assumption 
of gaussian noise, the likelihood can be written as a multi-variate gaussian distribution:</p>
<div class="arithmatex">\[
y = \tilde{\mathbf{x}}^T \boldsymbol \theta + \mathbf{n}  \sim \mathcal{N}(\hat{y} = \tilde{\mathbf{x}}^T \boldsymbol \theta, \sigma)
\]</div>
<p>Plugging this distribution into the negative log likelihood expression, we obtain:</p>
<div class="arithmatex">\[
\boldsymbol \theta_{ML} = \underset{\boldsymbol \theta} {\mathrm{argmin}} \; \sum_{i=1}^{N_s} 
\frac{||\hat{y}^{(i)} - y^{(i)}||_2^2}{2\sigma^2} = \frac{N_s}{2\sigma^2} MSE(\textbf{y}_{train}, \hat{\textbf{y}}_{train})\\
\]</div>
<p>This cost function can be seen to be a rescaled version of the MSE function previously introduced 
as the loss function for the linear regression model. Note however, that this model is not only more rigorous from
a statistical point of view but provides also a natural way to handle training samples with different confidence. By using
sample-dependant scaling factors (<span class="arithmatex">\(\sigma^{(i)}\)</span>), different samples can be chosen to contribute more or less to the
training process.</p>
<h4 id="multi-layer-perceptron-regression">Multi-layer perceptron regression</h4>
<p>A very similar derivation can be performed for a neural network composed by one or more MLPs. Eventually we simply need
to swap the previously linearly predicted output <span class="arithmatex">\(\hat{y}=\tilde{\mathbf{x}}^T \boldsymbol \theta\)</span> with a new
output produced by the chosen nonlinear functional <span class="arithmatex">\(\hat{y}=f_\theta(\mathbf{x})\)</span>. </p>
<p>In conclusion, we must remember that the MSE loss function, commonly used for regression 
tasks in ML and DL, is a MLE in disguise.</p>
<h3 id="classification">Classification</h3>
<h4 id="binary-classification">Binary classification</h4>
<p>In statistical learning, a Bernoulli distribution is commonly used for the task of binary (i.e., 2 label) 
classification:</p>
<div class="arithmatex">\[
P(y)= \phi y + (1-\phi)(1-y)
\]</div>
<p>where <span class="arithmatex">\(y\)</span> is the outcome and <span class="arithmatex">\(\phi\)</span> is its probability that we wish to learn by means of a model 
(i.e., logistic regression or MLP). Moreover, as we wish to learn a probability this value must be bound between
0 and 1; this can be easily achieved by feeding the output of the model into a sigmoid function <span class="arithmatex">\(\sigma\)</span>:</p>
<div class="arithmatex">\[
\hat{y} = \sigma (f_\theta(\mathbf{x}))
\]</div>
<p>Put together:</p>
<div class="arithmatex">\[
\begin{aligned}
\boldsymbol \theta_{ML} &amp;= \underset{\boldsymbol \theta} {\mathrm{argmin}} \; -\sum_{i=1}^{N_s} log(p_{model}(y^{(i)}|\mathbf{x}^{(i)}; \theta) \\
&amp;= -\sum_{i=1}^{N_s} y^{(i)} log \hat{y}^{(i)} + (1-y^{(i)}) log (1-\hat{y}^{(i)})
\end{aligned}
\]</div>
<p>which is the same loss function that we have introduced and discussed in details in the context of logistic
regression. </p>
<p>Once again, we note how we have here simply defined a MLE for a classification task and obtained
the well-know binary cross-entropy loss function.</p>
<h4 id="multi-label-classification">Multi-label classification</h4>
<p>An extension of binary classification, multi-label classification aims at producing an estimate of the most likely class
within a set of <span class="arithmatex">\(N_c\)</span> classes. </p>
<p>The combination of a Bernoulli distribution and sigmoid activation used for the binary classifier 
is here replaced by a Multinoulli distribution and softmax activation, where the latter is defined as follows:</p>
<div class="arithmatex">\[
\hat{\mathbf{y}} = \sigma(\mathbf{x}) =\frac{e^\mathbf{x}}{\sum_{i=1}^{N_c} e^{x_i}}
\]</div>
<p>A property of such activation function is that it takes as input a vector of numbers (sometimes called <em>logits</em>)) and 
produces as output a vector of probabilities (i.e., <span class="arithmatex">\(y_i&gt;0\)</span> and <span class="arithmatex">\(\sum_{i=1}^{N_c} y_i=1\)</span>).</p>
<p>Put together:</p>
<div class="arithmatex">\[
\begin{aligned}
\boldsymbol \theta_{ML} &amp;= \underset{\boldsymbol \theta} {\mathrm{argmin}} \; -\sum_{i=1}^{N_s} log(p_{model}(y^{(i)}|\mathbf{x}^{(i)}; \theta)) \\
&amp;= -\sum_{i=1}^{N_s} \sum_{j=1}^{N_c} y_j^{(i)} log \hat{y}_j^{(i)}
\end{aligned}
\]</div>
<p>where the true labels <span class="arithmatex">\(\mathbf{y}^{(i)}\)</span> are one-hot encoded vectors (i.e., <span class="arithmatex">\(y_{j=j_{true}}^{(i)}=1\)</span> and 
<span class="arithmatex">\(y_{j \neq j_{true}}^{(i)}=0\)</span>).</p>
<p>To conclude, let's try to get more insights into why ML estimators work so succesfully. In order to do so, we start
by defining a measure of similarity between the two distributions of interest:</p>
<ul>
<li>empirical distribution of the data: <span class="arithmatex">\(p_{data}(\mathbf{X})\)</span></li>
<li>parametric model distribution: <span class="arithmatex">\(p_{model}(\mathbf{X}; \theta)\)</span></li>
</ul>
<p>This can be achieved by means of the previously introduced Kullback-Leibler divergence, which we can write as follows:</p>
<div class="arithmatex">\[
D_{KL}(p_{data}||p_{model})  = E_{x \sim p_{data}} [log p_{data}(\mathbf{x}) - p_{model}(\mathbf{x})]  
\]</div>
<p>Since we are interested to estimate the free-parameters <span class="arithmatex">\(\theta\)</span> such that the model distribution matches that of the data,
an equivalent optimization problem can be written with the help of the KL divergence:</p>
<div class="arithmatex">\[
\begin{aligned}
\theta_{KL} &amp;= \underset{\theta} {\mathrm{argmin}} \; D_{KL}(p_{data}||p_{model}) \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; - E_{\mathbf{x} \sim p_{data}} [ log(p_{model}(\mathbf{x}; \theta))]
\end{aligned}
\]</div>
<p>where the data probability has been removed in the second term since it is independent of <span class="arithmatex">\(\theta\)</span>. We can conclude
that <span class="arithmatex">\(\theta_{KL}=\theta_{ML}\)</span> and therefore minimizing the KL divergence between the model and data distributions
is the same as maximizing their cross-entropy (as done by the ML estimator).</p>
<h2 id="additional-readings">Additional readings</h2>
<ul>
<li>If you are interested to learn more about network initialization, I recommend reading (and reproducing)
the following blog posts: <a href="https://medium.com/@safrin1128/weight-initialization-in-neural-network-inspired-by-andrew-ng-e0066dc4a566">1</a>
and <a href="https://www.deeplearning.ai/ai-notes/initialization/">2</a>.</li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../05_nn/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Basics of Neural Networks
              </div>
            </div>
          </a>
        
        
          <a href="../07_bestpractice/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Best practices in the training of Machine Learning models
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.18f0862e.min.js"></script>
      <script src="../../assets/javascripts/bundle.994580cf.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.9c0e82ba.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>