
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Dimensionality reduction - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#dimensionality-reduction" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Dimensionality reduction
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../READMEcurvenotelocal/" class="md-nav__link">
        READMEcurvenotelocal
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Dimensionality reduction
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Dimensionality reduction
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#principal-component-analysis-pca" class="md-nav__link">
    Principal Component Analysis (PCA)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other-linear-dimensionality-reduction-techniques" class="md-nav__link">
    Other linear dimensionality reduction techniques
  </a>
  
    <nav class="md-nav" aria-label="Other linear dimensionality reduction techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#independent-component-analysis-ica" class="md-nav__link">
    Independent Component Analysis (ICA)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-coding-or-dictionary-learning" class="md-nav__link">
    Sparse Coding (or Dictionary Learning)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencoders" class="md-nav__link">
    Autoencoders
  </a>
  
    <nav class="md-nav" aria-label="Autoencoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#applications" class="md-nav__link">
    Applications
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#undercomplete-vs-overcomplete-aes" class="md-nav__link">
    Undercomplete vs. Overcomplete AEs
  </a>
  
    <nav class="md-nav" aria-label="Undercomplete vs. Overcomplete AEs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse-autoencoders" class="md-nav__link">
    Sparse AutoEncoders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contractive-autoencoders" class="md-nav__link">
    Contractive AutoEncoders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#denoising-autoencoders" class="md-nav__link">
    Denoising AutoEncoders
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_INN/" class="md-nav__link">
        Invertible Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19_implicit/" class="md-nav__link">
        Implicit neural networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#principal-component-analysis-pca" class="md-nav__link">
    Principal Component Analysis (PCA)
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#other-linear-dimensionality-reduction-techniques" class="md-nav__link">
    Other linear dimensionality reduction techniques
  </a>
  
    <nav class="md-nav" aria-label="Other linear dimensionality reduction techniques">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#independent-component-analysis-ica" class="md-nav__link">
    Independent Component Analysis (ICA)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#sparse-coding-or-dictionary-learning" class="md-nav__link">
    Sparse Coding (or Dictionary Learning)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#autoencoders" class="md-nav__link">
    Autoencoders
  </a>
  
    <nav class="md-nav" aria-label="Autoencoders">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#applications" class="md-nav__link">
    Applications
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#undercomplete-vs-overcomplete-aes" class="md-nav__link">
    Undercomplete vs. Overcomplete AEs
  </a>
  
    <nav class="md-nav" aria-label="Undercomplete vs. Overcomplete AEs">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#sparse-autoencoders" class="md-nav__link">
    Sparse AutoEncoders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#contractive-autoencoders" class="md-nav__link">
    Contractive AutoEncoders
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#denoising-autoencoders" class="md-nav__link">
    Denoising AutoEncoders
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="dimensionality-reduction">Dimensionality reduction</h1>
<p>Up until now we have mostly focused on one family of Machine Learning methods, so-called <em>Supervised learning</em>. Whilst this
is by far the most popular application in Deep Learning and the one that has reported greater success in the last decade,
another family of methods that is becoming more and more popular falls under the umbrella of so-called <em>Unsupervised learning</em>.</p>
<p>When labelled data are scarce, or it is difficult to have access to ground truth labels (e.g., in geoscience), unsupervised learning
can represent an appealing alternative to find patterns in data. Unsupervised learning comes in different flavours. 
For example let's imagine grouping a set of unlabelled data into a number of buckets and then analyze them 
one-by-one knowing that the samples within each bucket are more similar to each other than others in the dataset: this is a form of 
unsupervised learning called <em>clustering</em>. The flavour that we are going to discuss in 
more details in the following is however referred to as <em>Dimensionality reduction</em>. Simply stated dimensionality reduction 
can be described as:</p>
<p>Take <span class="arithmatex">\(N_s\)</span> training samples <span class="arithmatex">\(\mathbf{x}^{(i)} \in \mathbb{R}^{N_f}\)</span>, (<span class="arithmatex">\(i=1,2,...N_s\)</span>),</p>
<p>Find a smaller representation <span class="arithmatex">\(\mathbf{c}^{(i)} \in \mathbb{R}^{N_l}\)</span> (<span class="arithmatex">\(N_l&lt;&lt;N_f\)</span>) whilst making the 
smallest possible reconstruction error.</p>
<p>If you previously studied how data are stored in a computer transmitted via cable (or air), you may recall that this 
is the very same objective of <em>data compression</em>. For this reason, nowadays we can build on a vast body of literature
when designing effective dimensionality reduction techniques. What it is however slowly becoming more and more evident is
the fact that by identifying representative low-dimensional (also called <em>latent</em>) spaces from a set of data samples living
in a much richer space, we can implicitly extract useful features to be later used in subsequent tasks of supervised learning.
This two-steps approach is becoming very popular these days especially in fields of science that lack vast amount of labelled data
as a way to take advantage as much as possible of unlabelled samples and then being able to fine-tune supervised models using
small amounts of labelled data.</p>
<p>Before we consider a number of different approaches to dimensionality reduction, let's write the problem in a common mathematical form. 
Given a number of training samples <span class="arithmatex">\(\mathbf{x}^{(i)}\)</span>, we wish to identify:</p>
<ul>
<li>encoder: <span class="arithmatex">\(\mathbf{c}^{(i)} = e(\mathbf{x}^{(i)})\)</span></li>
<li>decoder: <span class="arithmatex">\(\hat{\mathbf{x}}^{(i)} = d(\mathbf{c}^{(i)})\)</span></li>
</ul>
<p>such that:</p>
<div class="arithmatex">\[
\hat{e},\hat{d} = \underset{e,d} {\mathrm{argmin}} \; \frac{1}{N_s}\sum_i \mathscr{L}(\mathbf{x}^{(i)}, d(e(\mathbf{x}^{(i)})))
\]</div>
<h2 id="principal-component-analysis-pca">Principal Component Analysis (PCA)</h2>
<p>The simplest approach to dimensionality reduction uses linear operators for the encoder:</p>
<ul>
<li>encoder: <span class="arithmatex">\(\mathbf{c}^{(i)} = \mathbf{E}\mathbf{x}^{(i)}\)</span></li>
<li>decoder: <span class="arithmatex">\(\hat{\mathbf{x}}^{(i)} = \mathbf{D}\mathbf{c}^{(i)}\)</span></li>
</ul>
<p>where <span class="arithmatex">\(\mathbf{E}_{[N_l \times N_f]}\)</span> and <span class="arithmatex">\(\mathbf{D}_{[N_f \times N_l]}\)</span>. PCA aims to find representative
features that are linear combinations of the columns of the encoder (i.e., <span class="arithmatex">\(\mathbf{c}=\sum_{i=1}^{N_f} \mathbf{E}_{:,i} x_i\)</span>)
such that the projection of these new features onto the original space (<span class="arithmatex">\(\mathbf{D}\mathbf{c}\)</span>) is as close as possible to the
original sample <span class="arithmatex">\(\mathbf{x}\)</span>. In other words, we want to find the best linear subspace of the original space that minimizes the
reconstruction error defined here as the squared Euclidean norm (<span class="arithmatex">\(\mathscr{L}=||.||^2_2\)</span>).</p>
<p>Defining a unique pair of matrices (<span class="arithmatex">\(\mathbf{E},\mathbf{D}\)</span>) is however not possible without imposing further constraints. In the
PCA derivation we must assume that the columns of <span class="arithmatex">\(\mathbf{D}\)</span> are orthonormal:</p>
<div class="arithmatex">\[
\mathbf{D}^T \mathbf{D} = \mathbf{I}_{N_l}
\]</div>
<p>By making such a strong assumption we can easily see that</p>
<div class="arithmatex">\[
\hat{\mathbf{x}}^{(i)} = \mathbf{D}\mathbf{E}\mathbf{x}^{(i)}=\mathbf{D}\mathbf{D}^T\mathbf{x}^{(i)} \quad (\mathbf{E}=\mathbf{D}^T)
\]</div>
<p>is the choice of encoder-decoder that minimizes the reconstruction error. Let's now prove to ourselves that this is the case for 
a single training sample:</p>
<div class="arithmatex">\[
\hat{\mathbf{c}} = \underset{\mathbf{c}} {\mathrm{argmin}} \; ||\mathbf{x}-d(\mathbf{c})||_2^2
\]</div>
<p>where for the moment we do not specify the decoder and simply call it <span class="arithmatex">\(d\)</span>. Let's first expand the loss function</p>
<div class="arithmatex">\[
\begin{aligned}
||\mathbf{x}-d(\mathbf{c})||_2^2 &amp;= (\mathbf{x}-d(\mathbf{c}))^T (\mathbf{x}-d(\mathbf{c})) \\
&amp;= \mathbf{x}^T \mathbf{x} - \mathbf{x}^Td(\mathbf{c}) - d(\mathbf{c})^T \mathbf{x} + d(\mathbf{c})^T d(\mathbf{c})\\
&amp;= \mathbf{x}^T \mathbf{x} - 2 \mathbf{x}^Td(\mathbf{c}) + d(\mathbf{c})^T d(\mathbf{c})\\
\end{aligned}
\]</div>
<p>where we can ignore the first term given it does not depend on <span class="arithmatex">\(\mathbf{c}\)</span>. At this point let's consider the special
case of <span class="arithmatex">\(d()=\mathbf{D}\)</span>, which gives:</p>
<div class="arithmatex">\[
\begin{aligned}
||\mathbf{x}-d(\mathbf{c})||_2^2 &amp;= \mathbf{c}^T \mathbf{D}^T \mathbf{D} \mathbf{c} - 2 \mathbf{x}^T \mathbf{D} \mathbf{c} \\
&amp;= \mathbf{c}^T \mathbf{I}_{N_l} \mathbf{c} - 2 \mathbf{x}^T \mathbf{D} \mathbf{c}
\end{aligned}
\]</div>
<p>Finally we compute the derivative of the loss function over <span class="arithmatex">\(\mathbf{c}\)</span>:</p>
<div class="arithmatex">\[
\frac{\partial J}{\partial \mathbf{c}} = 0 \rightarrow 2 \mathbf{c}^T - 2 \mathbf{x}^T \mathbf{D} = 0 \rightarrow \mathbf{c} = \mathbf{D}^T \mathbf{x}
\]</div>
<p>where we have obtained that <span class="arithmatex">\(\mathbf{E} = \mathbf{D}^T\)</span>.</p>
<p>At this point we know what is the optimal linear encoder-decoder pair with respect to the MSE loss. However, we do not have a specific form
for the matrix <span class="arithmatex">\(\mathbf{D}\)</span> itself. In order to identify the entries of the decoder matrix, we need to set up another optimization problem, this 
time directly for <span class="arithmatex">\(\mathbf{D}\)</span>:</p>
<div class="arithmatex">\[
\hat{\mathbf{D}} = \underset{\mathbf{D}} {\mathrm{argmin}} \; ||\mathbf{X}-\mathbf{D}\mathbf{D}^T \mathbf{X}||_F 
\quad s.t. \; \mathbf{D}^T \mathbf{D} = \mathbf{I}_{N_l}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{X}_{[N_f \times N_s]}\)</span> is the training sample matrix. To simplify our derivation let's consider the case of
<span class="arithmatex">\(N_l=1\)</span>; the result can then be easily generalized for any choice of <span class="arithmatex">\(N_l=1\)</span>. Let's write</p>
<div class="arithmatex">\[
\begin{aligned}
\hat{\mathbf{d}} &amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; ||\mathbf{X}-\mathbf{d}\mathbf{d}^T \mathbf{X}||_F 
\quad s.t. \; \mathbf{d}^T \mathbf{d} = 1 \\
&amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; ||\bar{\mathbf{X}}-\bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T||_F 
\quad s.t. \; \mathbf{d}^T \mathbf{d} = 1 \quad (\bar{\mathbf{X}}=\mathbf{X}^T) \\
&amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; Tr((\bar{\mathbf{X}}-\bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T)^T(\bar{\mathbf{X}}-\bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T))
\quad s.t. \; \mathbf{d}^T \mathbf{d} = 1\\
&amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; Tr(\bar{\mathbf{X}}^T \bar{\mathbf{X}} - \bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T - 
\mathbf{d}\mathbf{d}^T \bar{\mathbf{X}}^T \bar{\mathbf{X}} + \mathbf{d}\mathbf{d}^T\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T)
\quad s.t. \; \mathbf{d}^T \mathbf{d} = 1\\
&amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; -2 Tr(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T) + Tr(\mathbf{d}\mathbf{d}^T\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T) \quad s.t. \; \mathbf{d}^T \mathbf{d} = 1\\
&amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; -2 Tr(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T) + Tr(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T \mathbf{d}\mathbf{d}^T) \quad s.t. \; \mathbf{d}^T \mathbf{d} = 1\\
&amp;= \underset{\mathbf{d}} {\mathrm{argmin}} \; -Tr(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T) \quad s.t. \; \mathbf{d}^T \mathbf{d} = 1\\
&amp;= \underset{\mathbf{d}} {\mathrm{argmax}} \; Tr(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d}\mathbf{d}^T) = Tr(\mathbf{d}^T \bar{\mathbf{X}}^T \bar{\mathbf{X}}\mathbf{d})  \quad s.t. \; \mathbf{d}^T \mathbf{d} = 1\\
\end{aligned}
\]</div>
<p>where in 6 we use the fact that <span class="arithmatex">\(\mathbf{d}^T \mathbf{d} = 1\)</span>. The solution of this maximization problem is represented by the 
eigenvector of <span class="arithmatex">\(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\)</span> associated to the largest eigenvalue (or the <span class="arithmatex">\(N_l\)</span> largest eigenvalues for the 
general case).</p>
<p>We can therefore conclude that PCA is defined as:</p>
<ul>
<li>Take <span class="arithmatex">\(N_s\)</span> training samples <span class="arithmatex">\(\mathbf{x}^{(i)} \in \mathbb{R}^{N_f}\)</span>, (<span class="arithmatex">\(i=1,2,...N_s\)</span>),</li>
<li>Compute the matrix <span class="arithmatex">\(\bar{\mathbf{X}}_{[N_s \times N_f]}\)</span></li>
<li>Compute the SVD of <span class="arithmatex">\(\bar{\mathbf{X}}\)</span> (i.e., eigenvalues and eigenvectors of the sample covariance matrix <span class="arithmatex">\(\bar{\mathbf{X}}^T \bar{\mathbf{X}}\)</span>)</li>
<li>Form <span class="arithmatex">\(\mathbf{D}\)</span> composed by the eigenvector associated with the <span class="arithmatex">\(N_l\)</span> largest eigenvalues.</li>
<li>Compute <span class="arithmatex">\(\mathbf{c}=\mathbf{D}^T \mathbf{x}\)</span> and <span class="arithmatex">\(\hat{\mathbf{x}}=\mathbf{D} \mathbf{c}\)</span>.</li>
</ul>
<p>More in general, it is also worth remembering that if the training data is not zero-mean, PCA can be slightly modified to take that into account:</p>
<div class="arithmatex">\[
\mathbf{c}=\mathbf{D}^T (\mathbf{x}-\boldsymbol\mu \; and \; \hat{\mathbf{x}}=\mathbf{D} \mathbf{c}+\boldsymbol\mu$.
\]</div>
<p>where <span class="arithmatex">\(\boldsymbol\mu\)</span> is the sample mean.</p>
<p>To conclude, let's try to provide some additional geometrical intuition of how PCA works in practice. Once again,
let's recall the covariance matrix that we form and create SVD on:</p>
<div class="arithmatex">\[
\mathbf{C}_x=E_\mathbf{x} [(\mathbf{x}-\boldsymbol\mu) (\mathbf{x}-\boldsymbol\mu)^T]
\]</div>
<p>The eigenvalues <span class="arithmatex">\(\lambda_i\)</span> of <span class="arithmatex">\(\mathbf{C}_x\)</span> relate to the variance of the dataset <span class="arithmatex">\(\mathbf{X}\)</span> in the direction of the 
associated eigenvector <span class="arithmatex">\(\mathbf{v}_i\)</span> as follows (we use a 2d example for simplicity):</p>
<p><img alt="PCA" src="../figs/pca.png" /></p>
<p>so we observe that the first direction of PCA (i.e. <span class="arithmatex">\(\mathbf{v}_1\)</span>) is the one that best minimizes the 
reconstruction error (i.e., <span class="arithmatex">\(\sum_i d_{i,1}\)</span>). In multiple dimensions, the eigenvectors are organized in order of reconstruction 
error of the projected data points from smallest to largest.</p>
<h2 id="other-linear-dimensionality-reduction-techniques">Other linear dimensionality reduction techniques</h2>
<p>Whilst PCA is very popular for its simplicity (both of understanding and implementation), other techniques for linead dimensionality
reduction exist. As some of them has been shown during the years to be very powerful and better suited to find representative latent
representations from data, we will briefly look at them here.</p>
<h3 id="independent-component-analysis-ica">Independent Component Analysis (ICA)</h3>
<p>ICA aims to separate a signal into many underlying signals that are scaled and added together to reproduce the original one:</p>
<div class="arithmatex">\[
\mathbf{x} = \sum_i c_i \mathbf{w}_i = \mathbf{Wc} 
\]</div>
<p>where in this case <span class="arithmatex">\(\mathbf{c}\)</span> has the same dimensionality of <span class="arithmatex">\(\mathbf{x}\)</span>. This model is in fact commonly used for blind source separation
of mixed signals. Despite it is strictly speaking not a dimensionality reduction technique, we discuss it here due to its ability of finding
representative bases that combined together can explain a set of data.</p>
<p>Once again, the problem is in need for extra constraints for us to be able to find a solution. In this case the assumption made of
the <span class="arithmatex">\(\mathbf{w}_i\)</span> signals is as follows:</p>
<p>Signals <span class="arithmatex">\(\mathbf{w}_i\)</span> must be statistically independent from each other and non-gaussian</p>
<p>A solution to this problem can be obtained finding the pair (<span class="arithmatex">\(\mathbf{W}, \mathbf{c}\)</span>) which maximises non-gaussianity (i.e., minimizes 
normalized sample kurtosis) or minimizes mutual information (MI). Whilst we don't discuss here in details how to achieve such solution,
it is worth pointing out that this requires solving a nonlinear inverse problem as <span class="arithmatex">\(\mathbf{W}\)</span> relates in a nonlinear manner to kurtosis or MI. </p>
<h3 id="sparse-coding-or-dictionary-learning">Sparse Coding (or Dictionary Learning)</h3>
<p>Sparse coding is another heavily studied model for dimensionality reduction. The general idea has origin in a large body of 
work carried out in other areas of applied mathematics where hand-crafted transformations (e.g., wavelets) have been identified
to nicely represent data of different kind (e.g., images, sounds, seismic recordings) in a very sparse fashion. Here <em>sparse</em>
refers to the fact that the transformed signal can be represented by a vector with many zeros and just few non-zero entries.</p>
<p>In this context, however, the transformation is represented by a matrix <span class="arithmatex">\(\mathbf{W}\)</span>, whose entries are once again learned directly
from the available training data. This is achieved by imposing a strong condition on the probability distribution associated with 
the latent vector <span class="arithmatex">\(\mathbf{c}\)</span>:</p>
<div class="arithmatex">\[
p(\mathbf{c}) \approx \text{Laplace, Cauchy, Factorized t-student}
\]</div>
<p>in other words, a fat tailed distribution, whose samples are therefore sparse. By making such an assumption, no closed form
solution exist like in the PCA case. Instead, the training process is set up with the following goals in mind:</p>
<ol>
<li>Find sparsest latent representation during the encoding phase</li>
<li>Find a decoder that provides the smallest reconstruction error</li>
</ol>
<p>which mathematically can be written as:</p>
<p>$$
\begin{aligned}
\hat{\mathbf{W}}, \hat{\mathbf{c}} &amp;= \underset{\mathbf{W}, \mathbf{c}} {\mathrm{argmax}} p(\mathbf{c}|\mathbf{x}) 
&amp;= \underset{\mathbf{W}, \mathbf{c}} {\mathrm{argmin}} \beta ||\mathbf{x}-\mathbf{W}\mathbf{c}||_2^2 +\lambda ||\mathbf{c}||_1
\end{aligned}
$$
where <span class="arithmatex">\(\beta\)</span>, <span class="arithmatex">\(\lambda\)</span> are directly related to the parameters of the posterior distribution that we wish to maximize. This
functional can be minimized in an alternating fashion, first for <span class="arithmatex">\(\mathbf{W}\)</span>, then for <span class="arithmatex">\(\mathbf{x}\)</span>, and so on and so forth.</p>
<p>Finally, once the training process is over and <span class="arithmatex">\(\hat{\mathbf{W}}\)</span> is available, it is worth noting that sparse coding does require
solving a sparsity-promoting inverse problem for any new training sample <span class="arithmatex">\(\mathbf{x}\)</span> in order to find its best
representation <span class="arithmatex">\(\hat{\mathbf{c}}\)</span>. Nevertheless, despite the higher cost compared to for example PCA, sparse coding has shown 
great promise in both data compression and representation learning, the latter when coupled with down-the-line supervised tasks.</p>
<h2 id="autoencoders">Autoencoders</h2>
<p>Finally, we turn our attention onto nonlinear dimensionality reduction models. We should know by now that nonlinear mappings (like
those performed by NNs) may be much more powerful than their linear counterpart is used to our advantage.</p>
<p>The most popular nonlinear dimensionality techniques dates back to 1991 and the work of M. Kramer. Simply put, an autoencoder is 
the combination of an encoder function <span class="arithmatex">\(e_\theta\)</span>, which converts the input data into a latent representation, 
and a decoder function <span class="arithmatex">\(d_\theta\)</span>, which converts the new representation back into the original format. Here, both 
<span class="arithmatex">\(e_\theta\)</span> and <span class="arithmatex">\(d_\theta\)</span> and nonlinear and fully learned and stack one after the other as shown below</p>
<p><img alt="AE" src="../figs/ae.png" /></p>
<p>An autoencoder can therefore be simply defined as:</p>
<div class="arithmatex">\[
\hat{\mathbf{x}} = d_\phi(e_\theta(\mathbf{x}))
\]</div>
<p>where similar to PCA, the training process is setup such the parameters of the two networks are optimized to minimize the 
following loss function:</p>
<div class="arithmatex">\[
\hat{e}_\theta,\hat{d}_\phi = \underset{e_\theta,d_\phi} {\mathrm{argmin}} \; \frac{1}{N_s}\sum_i \mathscr{L}(\mathbf{x}^{(i)}, d_\phi(e_\theta(\mathbf{x}^{(i)}))))
\]</div>
<p>where the network architecture for both the encoder and decoder can be chosen accordingly to the type of data we are interested in.</p>
<p>Once again, our code (or latent vector <span class="arithmatex">\(\mathbf{z}\)</span>) must be chosen to be of lower dimensionality compared to the input in order to be able to 
learn useful representations. On the other hand if we choose <span class="arithmatex">\(N_l \ge N_f\)</span>, we will likely not learn something useful: very likely what
we are going to learn is to reproduce the identity mapping. In other words, whilst the loss function is set to reproduce the input itself,
what we are really interested is not the mere reconstruction, rather the creation of some meaningful transformation of the input vector
that first projects it into a latent space and then expands it back to the original space. If we are able to accomplish this task,
we will likely see that if we feed the trained network with a new sample <span class="arithmatex">\(\mathbf{x}_{in}\)</span> that lies inside the distribution of the training data,
the reconstruction will be of similar quality as to what we observed in training. On the other hand, when a out-of-distribution sample
<span class="arithmatex">\(\mathbf{x}_{out}\)</span> is fed to the network, its prediction will be much less accurate.</p>
<h3 id="applications">Applications</h3>
<p>Now that we know how an AutoEncoder works, the next obvious question is why do we care and what can we use if for. Let's recap here a
couple of applications that we have already mentioned here and there in the lecture:</p>
<ul>
<li>Data compression: the use of NNs (and AEs in this specific case) may soon lead to a completely new, nonlinear paradigm in data compression 
  where we could simply store the latent vectors and network architecture and weights and reconstruct the original vector on-demand similar
  to what conventionally done with linear compressors (e.g., JPEG2000).</li>
<li>Learn robust features on large unlabelled data prior to supervised learning: assuming that we have access to a large dataset composed for the majority of
  unlabelled data and for a small portion of labelled data, we could imagine training and AE on the first part of the dataset and use the learned latent
  features as input to a subsequent task of supervised learning. More specifically, the inputs of the labelled data are fed to the trained encoder and the resulting
  features are used in conjunction with the labels in a supervised manner.</li>
<li>Inverse problems in the latent space: this is similar to the previous case, with the main difference that we may have an inverse problem we wish to solve where
  the parameter to estimate lives in the manifold of the <span class="arithmatex">\(\mathbf{x}\)</span> samples. We can once again train and AE to learn a good representation for such
  the manifold of possible solutions and then solve the inverse problem for <span class="arithmatex">\(\mathbf{z}\)</span> instead of <span class="arithmatex">\(\mathbf{x}\)</span> directly.</li>
<li>Perform vector math in the latent space: Imagine we want to compare two multi-dimensional vectors <span class="arithmatex">\(\mathbf{x}\)</span> (e.g., images). Classical
  distance measures may focus too much on small discrepancies and not really on the overall similarity between these samples, that is
  what we usually want to compare. Alternatively, we could convert both vectors into their latent representations and compare them in this reduced space.
  In this case, even simple distance measures like MSE may become more robust as they really compare high-level features of the inputs that
  are encapsulated in the latent vectors.</li>
</ul>
<h3 id="undercomplete-vs-overcomplete-aes">Undercomplete vs. Overcomplete AEs</h3>
<p>Up until now, we are talked about <em>undercomplete</em> representations (i.e., <span class="arithmatex">\(N_l &lt;&lt; N_f\)</span>). We have justified this with the fact that if we give
too many degrees of freedom to the network, we will likely allow it to learn the identity mapping (a form of overfitting for AEs). In short,
a good design for a AE should follow these two rules:</p>
<ul>
<li>choose a small enough code (<span class="arithmatex">\(N_l\)</span>): not too small as it won't be able to reproduce the input accurately, not too large as it will make the AE overfit;</li>
<li>choose a small enough network capacity for both the encoder and decoder: similarly, a too large network will easily overfit even if the size of bottleneck has been appropriately chosen. </li>
</ul>
<p>However, a different choice may be taken as we will see shortly. This is heavily inspired by traditional compression algorithms, where a
(linear) transformation that can produce a compact code (i.e., a code that can be stored in far fewer bits than the corresponding input) is
usually overcomplete. Let's take the Wavelet transform as an example:</p>
<p><img alt="WAVELETTRANSFORM" src="../figs/wt.png" /></p>
<p>Here the input image is initially decomposed into 3 high-pass and one-low pass filtered versions of it, and the low-pass one is further processed
recursively. The overall size of the input and output is however <em>identical</em>. What makes this transform a great compressor is that in the transformed domain,
natural images (and other N-dimensional signals) can be represented by very few non-zero coefficients. In other words, we say that the Wavelet transform provides
a <em>sparse</em> representation of a variety of N-dimensional signals in nature.</p>
<p>A similar approach can be taken for nonlinear transformations, like those applied by AEs. In this case, however, extra care must be taken to
avoid overfitting, which can be done by adding some constraints to the learning process. As already discussed many times, these constraints
can simply come in the form of regularization in the learning process:</p>
<div class="arithmatex">\[
\mathscr{L}_r = \frac{1}{N_s}\sum_i \left( \mathscr{L}(\mathbf{x}^{(i)}, d_\phi(e_\theta(\mathbf{x}^{(i)}))) +\lambda R(\mathbf{x}^{(i)} ; \theta,\phi) \right)
\]</div>
<p>where <span class="arithmatex">\(R(\mathbf{x}^{(i)} ;\theta,\phi)\)</span> can take several forms:</p>
<ul>
<li><em>L1 norm</em>: this encourages the network to produce sparse latent representations;</li>
<li><em>Derivative of the latent vector over the input</em>: this encourages robust latent vectors that a small sensitivity to small perturbations of the input;</li>
<li><em>Noise or missing parts in the inputs</em>: this is not really a regularization in formal sense, as nothing is added to the cost function, 
  rather the input is perturbed to make once again the latent representation robust to small variations in the input.</li>
</ul>
<h4 id="sparse-autoencoders">Sparse AutoEncoders</h4>
<p>Enforcing a sparse latent vector can act as a strong regularization. This can be simply achieved by choosing:</p>
<div class="arithmatex">\[
R(\mathbf{x}^{(i)} ;\theta,\phi) = ||e_\theta(\mathbf{x}^{(i)})||_1
\]</div>
<p>which allows the learning process to optimize for the pair of encoder-decoder that can reproduce the training samples, whilst also forcing the encoder
to produce sparse latent representation.</p>
<p>A step further can be taken by imposing that not only the activations of the latent code are sparse, rather all the activations in the network. Let's
take for simplicity a small network as depicted below:</p>
<p><img alt="AESPARSE" src="../figs/ae_sparse.png" /></p>
<p>and changing the regularizer to:</p>
<div class="arithmatex">\[
R(\mathbf{x}^{(i)} ;\theta,\phi) = \sum_j ||a_e^{[j](i)}||_1 + \sum_j ||a_d^{[j](i)}||_1
\]</div>
<p>An autoencoder that is trained using this strategy is called <em>Sparse Autoencoder</em>.</p>
<p>Finally, a slightly different strategy has been proposed under the name of <em>K-sparse AutoEncoder</em>, where instead of having a soft-constraint
in the form of the regularization term above, the elements of the latent code are modified by a nonlinear 
transformation that brings all elements to zero apart from the K largest elements in absolute value. More formally, even though in practice no regularization term is
therefore explicitly added to the loss function, this approach solves the following constrained problem:</p>
<div class="arithmatex">\[
\underset{\mathbf{e}_\theta, \mathbf{d}_\phi} {\mathrm{argmin}} \; \frac{1}{N_s}\sum_i \mathscr{L}(\mathbf{x}^{(i)}, d_\phi(e_\theta(\mathbf{x}^{(i)}))) 
\quad s.t. \quad ||\mathbf{z}||_0&lt;K
\]</div>
<p>Note that, once again, this procedure can be extended such that all the activations in the network are forced to have only K non-zero values.</p>
<h4 id="contractive-autoencoders">Contractive AutoEncoders</h4>
<p>An alternative regularization term that can make AEs robust to small changes in the input vectors is:</p>
<div class="arithmatex">\[
R(\mathbf{x} ;\theta,\phi) = ||\nabla_\mathbf{x} \mathbf{z}||_F
\]</div>
<p>where the derivative of the latent vector is taken over the input vector and forced to be small. Note that this derivative produces the 
Jacobian of the encoder as both the input and output are multi-dimensional (and therefore the use of the Frobenious norm). Whilst the authors
of this method claim additional robustness, the computational cost of computing a Jacobian makes this approach quite costly.</p>
<h4 id="denoising-autoencoders">Denoising AutoEncoders</h4>
<p>Finally, denoising AEs are another family of regularized autoencoders. In this case, however, the regularization is implemented directly on
the input vectors prior to feeding them to the network, by either replacing some values with zeros (or random values) or adding noise. Considering 
this last case, each step of the training process becomes:</p>
<ul>
<li><span class="arithmatex">\(\tilde{\mathbf{x}}^{(i)} = \mathbf{x}^{(i)} + \mathbf{n}^{(i)} \quad \forall i\)</span>;</li>
<li><span class="arithmatex">\(\mathscr{L} = \frac{1}{N_s}\sum_i \mathscr{L}(\mathbf{x}^{(i)}, d_\phi(e_\theta(\tilde{\mathbf{x}}^{(i)})))\)</span>.</li>
</ul>
<h2 id="additional-readings">Additional readings</h2>
<ul>
<li>the following <a href="https://towardsdatascience.com/independent-component-analysis-ica-a3eba0ccec35">resource</a> provides a detailed 
  explanation of the theory of ICA (and a simple Python implementation!)</li>
<li>the following <a href="https://lilianweng.github.io/posts/2018-08-12-vae/">blog post</a> provides an extensive list (and description) of
  different AutoEncoder networks (and Variational AutoEncoders, which we will discuss in the next lecture).</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../12_seqmod/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Sequence modelling" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Sequence modelling
            </div>
          </div>
        </a>
      
      
        
        <a href="../14_vae/" class="md-footer__link md-footer__link--next" aria-label="Next: Generative Modelling and Variational AutoEncoders" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Generative Modelling and Variational AutoEncoders
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>