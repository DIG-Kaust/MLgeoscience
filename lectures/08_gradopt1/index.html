
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>More on gradient-based optimization - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#more-on-gradient-based-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              More on gradient-based optimization
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          More on gradient-based optimization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        More on gradient-based optimization
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#limitations-of-sgd" class="md-nav__link">
    Limitations of SGD
  </a>
  
    <nav class="md-nav" aria-label="Limitations of SGD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ill-conditioning" class="md-nav__link">
    Ill-conditioning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-minima" class="md-nav__link">
    Local minima
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#saddle-points-and-other-flat-regions" class="md-nav__link">
    Saddle points and other flat regions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cliffs" class="md-nav__link">
    Cliffs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploding-and-vanishing-gradients" class="md-nav__link">
    Exploding and vanishing gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stategies-to-improve-sgd" class="md-nav__link">
    Stategies to improve SGD
  </a>
  
    <nav class="md-nav" aria-label="Stategies to improve SGD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cooling-strategy" class="md-nav__link">
    Cooling strategy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" class="md-nav__link">
    Momentum
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-learning-rates" class="md-nav__link">
    Adaptive learning rates
  </a>
  
    <nav class="md-nav" aria-label="Adaptive learning rates">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adagrad" class="md-nav__link">
    AdaGrad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsprop" class="md-nav__link">
    RMSProp
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    ADAM
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-tricks" class="md-nav__link">
    Other tricks
  </a>
  
    <nav class="md-nav" aria-label="Other tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#polyak-averaging" class="md-nav__link">
    Polyak Averaging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-normalization" class="md-nav__link">
    Batch Normalization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-pre-training" class="md-nav__link">
    Supervised pre-training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#limitations-of-sgd" class="md-nav__link">
    Limitations of SGD
  </a>
  
    <nav class="md-nav" aria-label="Limitations of SGD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#ill-conditioning" class="md-nav__link">
    Ill-conditioning
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#local-minima" class="md-nav__link">
    Local minima
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#saddle-points-and-other-flat-regions" class="md-nav__link">
    Saddle points and other flat regions
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cliffs" class="md-nav__link">
    Cliffs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#exploding-and-vanishing-gradients" class="md-nav__link">
    Exploding and vanishing gradients
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stategies-to-improve-sgd" class="md-nav__link">
    Stategies to improve SGD
  </a>
  
    <nav class="md-nav" aria-label="Stategies to improve SGD">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cooling-strategy" class="md-nav__link">
    Cooling strategy
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#momentum" class="md-nav__link">
    Momentum
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adaptive-learning-rates" class="md-nav__link">
    Adaptive learning rates
  </a>
  
    <nav class="md-nav" aria-label="Adaptive learning rates">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#adagrad" class="md-nav__link">
    AdaGrad
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rmsprop" class="md-nav__link">
    RMSProp
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#adam" class="md-nav__link">
    ADAM
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#other-tricks" class="md-nav__link">
    Other tricks
  </a>
  
    <nav class="md-nav" aria-label="Other tricks">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#polyak-averaging" class="md-nav__link">
    Polyak Averaging
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#batch-normalization" class="md-nav__link">
    Batch Normalization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#supervised-pre-training" class="md-nav__link">
    Supervised pre-training
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="more-on-gradient-based-optimization">More on gradient-based optimization</h1>
<p>Whilst stochastic gradient descent is easy to understand, and simple to implement algorithm (as discussed in this
<a href="lectures/03_gradopt.md">lecture</a>), it presents a number of shortcomings that prevent learning to be as fast and effective
as we would like it to be. In this lecture, we will discuss some of the limitations of SGD and look at alternative optimization
algorithms that have been developed in the last decade and are nowadays preferred to SGD in the process of training NNs.</p>
<h2 id="limitations-of-sgd">Limitations of SGD</h2>
<h3 id="ill-conditioning">Ill-conditioning</h3>
<p>The shape, and more specifically the curvature, of the functional that we wish to minimize affects
our ability to quickly and efficiently converge to one of its minima (ideally the global, likely one of the local). For nonlinear optimization problems, like those encountered in deep learning, this is mathematically represented by the <em>Hessian</em> matrix 
(<span class="arithmatex">\(\mathbf{H}=\frac{\partial^2 f}{\partial \boldsymbol \theta^2}\)</span>). An Hessian matrix with large conditioning number (i.e.,
ratio of the largest and smallest eigenvalues) tends to affect convergence speed of first-order (gradient-based) methods.</p>
<p>In classical optimization theory, second order methods such as the Gauss-Newton method are commonly employed to counteract
this problem. However, as already mentioned in one of our previous lectures, such methods are not yet suitable for deep learning
in that no mathematical foundations have been developed in conjunction with approximate gradients (i.e., mini-batch learning
strategy). </p>
<p>Another factor that is worth knowing about is related to the norm of the gradient <span class="arithmatex">\(\mathbf{g}^T\mathbf{g}\)</span> through iterations.
In theory, this norm should shrink through iterations to guarantee convergence. Nevertheless, successful training may still be 
obtained even if the norm does not shrink as long as the learning rate is kept small. Let's write the second-order Taylor
expansion of the functional around the current parameter estimate <span class="arithmatex">\(\boldsymbol \theta_0\)</span>:</p>
<div class="arithmatex">\[
J(\boldsymbol \theta) \approx J(\boldsymbol \theta_0) + (\boldsymbol \theta - \boldsymbol \theta_0)^T \mathbf{g} + 
\frac{1}{2} (\boldsymbol \theta - \boldsymbol \theta_0)^T \mathbf{H} (\boldsymbol \theta - \boldsymbol \theta_0)
\]</div>
<p>and evaluate it at the next gradient step <span class="arithmatex">\(\boldsymbol \theta = \boldsymbol \theta_0 - \alpha \mathbf{g}\)</span>:</p>
<div class="arithmatex">\[
J(\boldsymbol \theta_0 - \alpha \mathbf{g}) \approx J(\boldsymbol \theta_0) - \mathbf{g}^T \mathbf{g} + 
\frac{1}{2} \alpha^2 \mathbf{g}^T \mathbf{H} \mathbf{g}
\]</div>
<p>We can interpret this expression as follows: a gradient step of <span class="arithmatex">\(- \alpha \mathbf{g}\)</span> adds the following contribution
to the cost function, <span class="arithmatex">\(-\mathbf{g}^T \mathbf{g} + 
\frac{1}{2} \alpha^2 \mathbf{g}^T \mathbf{H} \mathbf{g}\)</span>. When this contribution is positive (i.e., 
<span class="arithmatex">\(\frac{1}{2} \alpha^2 \mathbf{g}^T \mathbf{H} \mathbf{g} &gt; \mathbf{g}^T \mathbf{g}\)</span>), the cost function grows instead of
being reduced. Under the assumption that <span class="arithmatex">\(\mathbf{H}\)</span> is known, we could easily choose a step-size <span class="arithmatex">\(\alpha\)</span> that prevents this from happening. However, when the Hessian cannot be estimated, a conservative selection of the step-size is the only remedy to prevent the cost function from growing. A downside of such an approach is that the smaller the learning rate the slower the training process.</p>
<h3 id="local-minima">Local minima</h3>
<p>Whilst the focus of the previous section has been in the neighbour of <span class="arithmatex">\(\boldsymbol \theta_0\)</span> where the functional 
<span class="arithmatex">\(J_{\boldsymbol \theta}\)</span> can be approximated by a convex function, the landscape of NN functionals is generally non-convex
and populated with a multitude of local minima. The problem of converging to the global minimum without getting stuck 
in one of the local minima is a well-known problem for any non-convex optimization. An example in geophysics is represented
by waveform inversion and a large body of work has been carried out by the geophysical research community to identify
objective functions that are more well-behaved (i.e., show a large basin of attraction around the global minimum).</p>
<p>Nevertheless, getting stuck into local minima is much less of a problem when training neural networks. 
This can be justified by the fact that multiple models may perform equally well on both the training and testing data. 
To be more precise this relates to the concept of <em>model identifiability</em>, where a model is defined identifiable if there exist a 
single set of parameters (<span class="arithmatex">\(\boldsymbol \theta_{gm}\)</span>) that lead to optimal model performance. On the other hand, when multiple models <span class="arithmatex">\(\{ \boldsymbol \theta_{gm}, 
\boldsymbol \theta_{lm,1}, ..., \boldsymbol \theta_{lm1,N}\)</span> perform similarly those models are said to be non-identifiable. Moreover, even when a 
single model performs best, a distinction must be made between training and testing performance. As far as training performance is concerned,
this model must be that of the global minimum of the functional <span class="arithmatex">\(\boldsymbol \theta_{gm}\)</span>. Nevertheless, the model that performs best on the testing
data may be the one obtained from any of the local minima <span class="arithmatex">\(\boldsymbol \theta_{lm,i}\)</span> as such a model be have better generalization capabilities
than the one from the global minimum. </p>
<h3 id="saddle-points-and-other-flat-regions">Saddle points and other flat regions</h3>
<p>Recent research in the field of deep learning has however revealed that multi-dimensional landscapes associated to the training of 
deep neural networks may actually have much fewer local minima than we tend to believe, and the main hinder to slow training is actually 
represented by saddle points (and flat regions in general). More specifically, empirically it can be shown that the ratio between saddle points and local 
minima is in the order of <span class="arithmatex">\(e^n\)</span> where <span class="arithmatex">\(n\)</span> is the number of dimensions of the model vector <span class="arithmatex">\(\boldsymbol \theta\)</span>.</p>
<p>The main problem associated with saddle points is similar to that of local minima: the associated gradient is <span class="arithmatex">\(J(\boldsymbol \theta) \rightarrow 0\)</span>; 
as a consequence, during training, when the trajectory of the model parameter vector
approaches a saddle point, the learning process may experience a slow down. </p>
<h3 id="cliffs">Cliffs</h3>
<p>Another potentially dangerous feature of NN landscapes is represented by steep regions where <span class="arithmatex">\(J(\boldsymbol \theta) \rightarrow \infty\)</span>.
This may in fact lead to unstable behaviours during training as large jumps will arise in the trajectory of the model parameter vector.</p>
<p>Heuristic approaches to mitigate this problem exist, one of them is the so-called <em>gradient clipping</em> strategy where:</p>
<p>$$
\nabla J(\theta_i) = min(\nabla J(\theta_i), th)
$$
where <span class="arithmatex">\(th\)</span> is a user-defined threshold. This approach allows element-wise gradient clipping for those directions with an extremely large gradient
whilst not forcing us to lower the overall learning rate.</p>
<h3 id="exploding-and-vanishing-gradients">Exploding and vanishing gradients</h3>
<p>Two problems that we commonly encounter whilst training Neural Networks are the so-called exploding and vanishing gradient phenomena. Whilst we already mentioned two scenarios where either of these situations can occur, i.e., cliffs and saddle points, the shape of the functional that we wish to optimize is not the only reason for gradients to grow uncontrolled or stagnate. It is in fact the NN architecture itself that sometimes may give rise to such phenomena.</p>
<p>To provide some intuition, let's consider a matrix of weights <span class="arithmatex">\(\mathbf{W}\)</span> and apply it N times recursively to a certain input
(where for simplicity we ignore the nonlinear activation functions):</p>
<div class="arithmatex">\[
\mathbf{y}=\mathbf{W}^N\mathbf{x}
\]</div>
<p>If we assume <span class="arithmatex">\(\mathbf{W}\)</span> to be symmetric for simplicity and express it using
its eigendecomposition</p>
<div class="arithmatex">\[
\mathbf{W}=\mathbf{V} \boldsymbol \Sigma \mathbf{V}^{-1}
\]</div>
<p>the resulting output vector <span class="arithmatex">\(\mathbf{y}\)</span> can be equivalently written as:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbf{y} &amp;= \mathbf{V} \boldsymbol \Sigma \mathbf{V}^{-1} \mathbf{V} \boldsymbol \Sigma \mathbf{V}^{-1} ... \mathbf{V} \boldsymbol \Sigma \mathbf{V}^{-1} \mathbf{x} \\
&amp;= \mathbf{V} \boldsymbol \Sigma^N \mathbf{V}^{-1} \mathbf{x}
\end{aligned}
\]</div>
<p>where we have used here the property of eigendecomposition, <span class="arithmatex">\(\mathbf{V}^{-1} \mathbf{V} = \mathbf{I}\)</span>. Note that since
the matrix of eigenvalues is raised to the power of N, when N is large we will experience the following phenomena:</p>
<ul>
<li><span class="arithmatex">\(\lambda_i &gt; 1 \rightarrow\)</span> exploding gradient; </li>
<li><span class="arithmatex">\(\lambda_i &lt; 1 \rightarrow\)</span> vanishing gradient; </li>
</ul>
<p>Note that the scenario discussed here does not manifest itself when training feed forward networks, whilst it is much more relevant in the context of recurrent neural networks as the same weights are repeatedly applied to the input
as it flows through the computational graph. We defer a more extensive discussion of this phenomenon to this <a href="lectures/XX.md">lecture</a>.</p>
<h2 id="stategies-to-improve-sgd">Stategies to improve SGD</h2>
<p>After looking at some of the problems that we should be aware of when training NNs (note that some of them can be
easily overcome as we will see in the following, whilst others are outstanding and do not have a simple solution), let's look back
at the SGD algorithm and consider a number of improvements that can lead to both faster and more stable training.</p>
<p>We remember from our previous <a href="lectures/03_gradopt.md">lecture</a>, that the optimization step of SGD is simply composed of two steps:</p>
<ul>
<li>compute the gradient of the cost function with respect to the free-parameters, obtained via back-propagation</li>
<li>apply a scaled step, dictated by the learning rate <span class="arithmatex">\(\alpha\)</span>.</li>
</ul>
<h3 id="cooling-strategy">Cooling strategy</h3>
<p>The most basic version of SGD uses a constant learning rate. However, a learning rate that may be optimal at the start of training and lead to fast convergence towards one of the minima of the cost function, may lead to unstable behaviour at later iterations.</p>
<p>A question arises: given a gradient telling us where to move in the NN functional landscape, can we do something smart with the 
learning rate to reach the minimum faster. A common approach usually referred to as <em>cooling strategy</em> or <em>learning rate scheduling</em>, where the learning rate is not kept fixed through epochs. Instead, the learning rate is slowly reduced as epochs progress allowing the trajectory of the free-parameters to not fluctuate too much as it progresses towards a valley. </p>
<p><img alt="COOLING" src="../figs/cooling.png" /></p>
<p>Many alternative approaches to LR scheduling exist. However, to be effective, they must respect the following conditions:</p>
<div class="arithmatex">\[
\sum_i \alpha_i = \infty, \; \sum_i \alpha_i^2 &lt; \infty'
\]</div>
<p>or, in words, the learning rate should reduce slowly as iterations progress. </p>
<p>One common approach uses a linearly decaying LR for the first <span class="arithmatex">\(\tau\)</span> iterations, followed by a constant LR:</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\alpha_i = (1-\beta) \alpha_0 + \beta \alpha_\tau \qquad i&lt;\tau\\
&amp;\alpha_i = \alpha_\tau \qquad i\ge\tau
\end{aligned}
\]</div>
<p>where <span class="arithmatex">\(\beta=i/\tau\)</span>. As a rule of thumb, <span class="arithmatex">\(\tau \approx 100 N_{epochs}, \alpha_\tau = \alpha_0/100\)</span>, whilst the choice
of <span class="arithmatex">\(\alpha_0\)</span> is problem dependent and chosen by monitoring the first few iterations.</p>
<p><img alt="PWLR" src="../figs/piecewiselr.png" /></p>
<p>Alternative approaches can either apply a fixed decay (i.e., exponential) or choose to reduce the LR when the training (or validation) metric has not decreased for a number of epochs.</p>
<h3 id="momentum">Momentum</h3>
<p>Another commonly used strategy aimed at improving the convergence of SGD is called <em>Momentum</em> and dates back to the 60s and the
seminal works of Polyak and Nesterov in the area of mathematical optimization. The idea of momentum is rather simple, yet very effective. It is based on the idea of using information not only from the current gradient but also from past gradients when making a step. More specifically, the step is based on an exponentially decaying moving average of the past gradients created during iterations.</p>
<p><img alt="MOMENTUM" src="../figs/momentum.png" /></p>
<p>The motivation behind using multiple gradients is to use the knowledge about the landscape shape accumulated through time in
the proximity of the current parameters to make a more informed decision on where to move. This can generally help dealing with 
poorly conditioned modelling matrices in linear optimization and poorly conditioned Hessian matrices in nonlinear optimization.
Intuitively, momentum can be understood as some sort of medium resistance or inertia when moving down a valley which slows down the trajectory and keeps it close to the axes of the ellipses of the functional (or its linearization around the current position). This physical interpretation is actually used when defining SGD with momentum as a vector <span class="arithmatex">\(\mathbf{v}\)</span> (where v stands for velocity) is introduced:</p>
<div class="arithmatex">\[
\mathbf{v}_{i+1} = \gamma \mathbf{v}_i - \mathbf{g}_{i+1} = \gamma \mathbf{v}_i - \frac{\alpha}{N_b} \sum_{j=1}^{N_b} \nabla \mathscr{L}_j
\]</div>
<p>and the update becomes:</p>
<div class="arithmatex">\[
\boldsymbol\theta_{i+1} = \boldsymbol\theta_{i} - \mathbf{v}_{i+1}
\]</div>
<p>where <span class="arithmatex">\(\gamma \in [0, 1)\)</span> is the momentum term. If we write explicitly the first three iterates of the velocity vector:</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\mathbf{v}_0 = - \alpha \mathbf{g}_0\\
&amp;\mathbf{v}_1 = \gamma \mathbf{v}_0 - \alpha \mathbf{g}_1 =  - \gamma \alpha \mathbf{g}_0 - \alpha \mathbf{g}_1 \\
&amp;\mathbf{v}_2 = \gamma \mathbf{v}_1 - \alpha \mathbf{g}_2 = 
- \gamma^2 \alpha \mathbf{g}_0 - \gamma \alpha \mathbf{g}_1 - \alpha \mathbf{g}_2
\end{aligned}
\]</div>
<p>we notice that the momentum tells us how quickly the contribution of the previous gradients should decay. With <span class="arithmatex">\(\alpha=0\)</span> we are back to the standard SGD algorithm, whilst with <span class="arithmatex">\(\alpha \rightarrow 1\)</span> we take into account the entire history of gradients. More commonly used values of momentum are <span class="arithmatex">\(\alpha=0.5/0.9/0.99\)</span> which can also be combined with a warming strategy (i.e., start from 0.5 and increase through iterations all the way to 0.99). This is a similar strategy (even though in opposite direction) to the one we previously discussed for the learning rate, even though it is known to impact the learning process to a lesser extent.</p>
<p>Based on what we wrote above for the first three iterates, we can easily conclude that:</p>
<ul>
<li>if <span class="arithmatex">\(\mathbf{g}_i \approx \mathbf{g}_{i-1} \approx \mathbf{g}_{i-2}\)</span> (where the sign <span class="arithmatex">\(\approx\)</span> is used here to
  indicate a vector with approximately the same direction), the gradients' sum constructively leading to higher momentum and therefore a faster trajectory</li>
<li>if <span class="arithmatex">\(\mathbf{g}_i \ne \mathbf{g}_{i-1} \ne \mathbf{g}_{i-2}\)</span> (where the sign $\ne is used here to
  indicate a vector with different directions), the gradients' sum destructively leading to lower momentum and therefore a slower trajectory</li>
</ul>
<p>Finally, an even smarter approach would require us not only to accumulate past gradients but also to look ahead of time
so that we could slow down the trajectory if the landscape is about to change curvature (i.e., slow up). This
requires a slight modification of the momentum term, referred to as <em>Nesterov momentum</em>:</p>
<div class="arithmatex">\[
\mathbf{v}_{i+1} = \gamma \mathbf{v}_i - \frac{\alpha}{N_b} \sum_{j=1}^{N_b} \nabla \mathscr{L}_j(f_{\theta+\gamma \mathbf{v}_i}(\mathbf{x}_i), y_i)
\]</div>
<p>where the main change here is represented by the fact that the loss function (<span class="arithmatex">\(\mathscr{L}\)</span>), and therefore, the gradient is
evaluated at location <span class="arithmatex">\(\theta+\gamma \mathbf{v}_i\)</span> rather than at the current one. Here, <span class="arithmatex">\(\gamma \mathbf{v}_i\)</span> represents
a correction factor to the standard method of momentum. In classical optimization (i.e., for batched gradient descent), this small change provides an improvement in the rate of convergence from <span class="arithmatex">\(\mathcal{O}(1/i)\)</span> to <span class="arithmatex">\(\mathcal{O}(1/i^2)\)</span>. Note that this is however not always the case when using stochastic gradient descent.</p>
<h3 id="adaptive-learning-rates">Adaptive learning rates</h3>
<p>Up until now, we have introduced some modifications to the standard SGD algorithm that globally change the scaling of the
gradient (also referred to as learning rate). However, if we believe that directions of sensitivity of the functional should be axis aligned, different learning rates should be used for the different parameters we wish to optimize for. More specifically a small LR should be preferred for those directions associated with large eigenvalues of the local Hessian whilst a large LR should be used for the other directions that associated with small eigenvalues.</p>
<p>The <em>delta-bar-delta</em> algorithm of Jacobs (1988) represents an early heuristic approach to automatically adapting
learning rates of individual parameters. It is based on this simple rule:</p>
<ul>
<li>if <span class="arithmatex">\(sign\{g_{i+1}^j\} = sign\{g_{i}^j\}\)</span>, increase LR</li>
<li>if <span class="arithmatex">\(sign\{g_{i+1}^j\} \ne sign\{g_{i}^j\}\)</span>, decrease LR</li>
</ul>
<p>where <span class="arithmatex">\(j\)</span> refers here to the j-th component of the gradient vector.</p>
<p>However, in the last decade a large variety of optimizers have appeared in the literature mostly focusing on this 
particular aspect of training, i.e. parameter-dependent learning rate. We will go through some of the most popular ones
that have revolutionized the way we train NNs nowadays.</p>
<h4 id="adagrad">AdaGrad</h4>
<p>This optimizer scales the gradient vector by the inverse of the square root of the sum of all historical squared 
values of the gradient.</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\mathbf{g}_{i+1} = \frac{1}{N_b} \sum_{j=1}^{N_b} \nabla \mathscr{L}_j\\
&amp;\mathbf{r}_{i+1} = \mathbf{r}_i + \mathbf{g}_{i+1} \cdot \mathbf{g}_{i+1} \\
&amp;\Delta \boldsymbol\theta_{i+1} = -\frac{\alpha}{\delta + \sqrt{\mathbf{r}_{i+1}}} \cdot \mathbf{g}_{i+1} \\
&amp;\boldsymbol\theta_{i+1} = \boldsymbol\theta_{i} - \Delta \boldsymbol\theta_{i+1}
\end{aligned}
\]</div>
<p>where the vector <span class="arithmatex">\(\mathbf{r}\)</span> contains a running sum of the element-wise square gradients 
(with <span class="arithmatex">\(\mathbf{r}_0=0\)</span>), <span class="arithmatex">\(\cdot\)</span> and <span class="arithmatex">\(\sqrt{\;}\)</span> represent the element-wise multiplication of two vectors and
square root, respectively. Finally, <span class="arithmatex">\(\delta=10^{-6}\)</span> is used as stabilizer to avoid division by zero.</p>
<p>If we look at the learning rate of AdaGrad, it is clear that this is parameter dependent and more importantly, it is 
a function of the norm of the past gradients. Therefore, parameters associated with large gradients will experience
a rapid decrease in their associated LR, whilst parameters with small gradients will have an increase of the LR
through iterations. </p>
<p>The effect of such adaptive LR, is that the trajectory of the parameters will show greater
progress over gently sloped directions of the landscape. Nevertheless, it has been reported in the literature that a
main drawback of AdaGrad is that this effect is too strong, leading to a premature decrease of the LR in those
directions with large gradients and therefore an overall slow learning process.</p>
<h4 id="rmsprop">RMSProp</h4>
<p>A modified version of AdaGrad particularly suited for nonconvex optimization where the gradient accumulation 
(i.e., <span class="arithmatex">\(\mathbf{r}\)</span> vector) is exponentially weighted on a moving window. The idea behind is that for NN training it may take a large number of gradient steps to converge to a satisfactory solution, and therefore it is important for the LR not to decrease too fast in the first few hundred steps. In mathematical terms, a single change is needed to the AdaGrad equations, namely:</p>
<div class="arithmatex">\[
\mathbf{r}_{i+1} = \rho \mathbf{r}_i + (1-\rho)\mathbf{g}_{i+1} \cdot \mathbf{g}_{i+1} \\
\]</div>
<p>where <span class="arithmatex">\(\rho\)</span> represents the decay rate in the accumulation of past gradients. RMSProp, which was proposed by
Geoffrey Hinton during a Coursera class, is shown to be one of the best-in-class optimizers for NN training and it
is widely adopted by the DL community.</p>
<h4 id="adam">ADAM</h4>
<p>ADAM stands for Adaptive Moments and it is a variant of RMSProp that further includes Momentum. Nowadays, ADAM 
is by far the most popular optimizer in the training of deep NNs. </p>
<p>Two key changes have been introduced in the ADAM algorithm when compared to RMSProp:</p>
<ul>
<li>Momentum is applied via an estimate of the first-order momentum plus an exponential decay and used in spite of 
  pure gradients in the parameter update step;</li>
<li>A bias correction is included to take into account initialization.</li>
</ul>
<p>The algorithm can be written as follows:</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\mathbf{g}_{i+1} = \frac{1}{N_b} \sum_{j=1}^{N_b} \nabla \mathscr{L}_j\\
&amp;\mathbf{m}_{i+1} = \rho_1 \mathbf{m}_i + (1-\rho_1)\mathbf{g}_{i+1} \leftarrow velocity \; term \\
&amp;\mathbf{v}_{i+1} = \rho_2 \mathbf{v}_i + (1-\rho_2)\mathbf{g}_{i+1} \cdot \mathbf{g}_{i+1}  \leftarrow scaling \; term \\
&amp;\hat{\mathbf{m}}_{i+1} = \frac{\mathbf{m}_{i+1}}{1-\rho_1^{i+1}} \leftarrow bias \; correction \\
&amp;\hat{\mathbf{v}}_{i+1} = \frac{\mathbf{v}_{i+1}}{1-\rho_2^{i+1}} \leftarrow bias \; correction \\
&amp;\Delta \boldsymbol\theta_{i+1} = -\frac{\alpha}{\delta + \sqrt{\hat{\mathbf{v}}_{i+1}}} \cdot \hat{\mathbf{m}}_{i+1}\\
&amp;\boldsymbol\theta_{i+1} = \boldsymbol\theta_{i} - \Delta \boldsymbol\theta_{i+1}
\end{aligned}
\]</div>
<p>where, once again, a number of hyperparameters are introduced. These are the stabilizer, <span class="arithmatex">\(\delta=10^{-6}\)</span>, and two 
decay rates (<span class="arithmatex">\(\rho_1\)</span> and <span class="arithmatex">\(\rho_2\)</span>).</p>
<p>To conclude, we have first introduced simpler optimizers and subsequently built complexity in terms of both momentum and 
parameter-dependent learning, there is no universal winner. Although both momentum and adaptive LR do clearly seem
to be beneficial to the training on NNs, it is not always the case that ADAM provides the best results both in
terms of robustness and convergence speed. It is therefore important to be aware of the different optimizers that
are available in the DL arsenal and identify the best based on the task at end. In other words, the choice of the
optimizer can usually represent one of those hyperparameters that ML practitioners need to evaluate and select
when developing a new ML pipeline.</p>
<h3 id="other-tricks">Other tricks</h3>
<p>In the following, we report a few other practical tricks that can be used when training NNs to further
improve the learning capabilities of our optimizer (no matter what optimizer has been selected).</p>
<h4 id="polyak-averaging">Polyak Averaging</h4>
<p>When training a NN, the most common approach is to select the last iterate (<span class="arithmatex">\(\boldsymbol\theta_{N_{it}}\)</span>) where 
<span class="arithmatex">\(N_{it}\)</span> is the overall number of iterations and use it at inference stage. Nevertheless, given the highly nonconvex 
optimization problem that we are required to solver, it is logical to expect that perhaps the last estimate of
model parameters is not the best. Let's for example imagine that towards the end of the training process we are approaching a (local or global) minimum. However, our trajectory is bouncing all around the valley: </p>
<p><img alt="POLYAK" src="../figs/Polyak.png" /></p>
<p>A simple approach to mitigate this effect is to average over the last <span class="arithmatex">\(N\)</span> iterations:</p>
<div class="arithmatex">\[
\boldsymbol\theta = \frac{1}{N} \sum_{i=0}^{N-1} \boldsymbol\theta_{N_{it}-i}
\]</div>
<p>This averaging acts as a denoising process that takes away some of the fluctuations and makes the optimization 
process less sensitive to the last step.</p>
<h4 id="batch-normalization">Batch Normalization</h4>
<p>This is a very recent advancement in the field of DL, from the seminal work of Ioffe and Szegedy (2015). It has been shown
to be particularly beneficial to the training of very deep neural networks.</p>
<p>Let's first take a look at what happens during the training process if we do not include batch normalization. As 
previously discussed, given the gradient <span class="arithmatex">\(\partial J / \partial \boldsymbol \theta\)</span>, at every step of the optimization
process all the parameters (weights and biases) in the different layers of a NN are simultaneously updated. This
goes against the "theoretical assumption" that the optimization process should update one parameter at the time (which
is however too expensive and therefore unfeasible). As a consequence of the fact that all free-parameters are updated
together is that second order updates are introduced or, in other words, the statistical distribution of various parameters
across the layers of the NN are modified. This is commonly referred to as <em>internal covariate shift</em>.</p>
<p><em>Batch normalization</em> use a general way to reparametrize every NN, which reduces the need for coordination across many 
layers during an update (making the process of updating all parameters at the same time more stable). It is simply
implemented by modifying the output of a layer (or all the layers) at training time as follows:</p>
<p><img alt="BATCHNORM" src="../figs/batchnorm.png" /></p>
<p>where a re-normalization process is applied to every row of the output matrix <span class="arithmatex">\(\mathbf{A}\)</span> and it is directly based on the local statistics (mean and standard deviation) of the output of the layer. The overall forward and backward passes remain unchanged with the simple difference that the network is now operating on the re-normalized output <span class="arithmatex">\(\mathbf{A}'\)</span> instead of the original one <span class="arithmatex">\(\mathbf{A}\)</span>.</p>
<p>The implications of such an additional step of re-normalization are that now the activations are distributed as <span class="arithmatex">\(\mathcal{N}(0, 1)\)</span> throughout the entire training process. By doing so, the optimization algorithm is discouraged to propose an update that simply acts constantly over the mean or the standard deviation of <span class="arithmatex">\(\mathbf{A}\)</span>.</p>
<p>At testing time, the mean and standard deviation (<span class="arithmatex">\(\boldsymbol \mu\)</span> and <span class="arithmatex">\(\boldsymbol \sigma\)</span>) are usually fixed and taken from a
running mean computed during training time.</p>
<p>In practice, however, batch normalization includes an extra step where instead of forcing the mean and standard deviation
of each layer to be fixed, these parameters are learned to make the units of the network more expressive. This is simply
accomplished by defining the output <span class="arithmatex">\(\mathbf{A}''\)</span> as:</p>
<div class="arithmatex">\[
\mathbf{A}'' = \gamma \mathbf{A}' + \beta
\]</div>
<p>where <span class="arithmatex">\(\gamma\)</span> and <span class="arithmatex">\(\beta\)</span> are also learned alongside the weights and biases of the network. Finally, since the bias is now induced by <span class="arithmatex">\(\beta\)</span> a common recommendation when using batch normalization is to avoid adding a learnable bias to the layer of the network.</p>
<h4 id="supervised-pre-training">Supervised pre-training</h4>
<p>So far, we have talked about optimizing the free-parameters of a neural network starting from a random initialization of such
parameters and using all the available data to get the best estimate of such parameters. We have also briefly mentioned that
transfer learning, a technique that uses a pre-trained network on a different set of data and possible different task and fine-tunes it on the task and data at hand, as a way to speed-up the training process as well as get around to the fact that sometimes we have access to a small amount of labelled data.</p>
<p>Another interesting technique that can be used to ease the learning capabilities of a NN is called <em>pre-training</em> or <em>greedy training</em>. Two alternative approaches are generally taken:</p>
<ul>
<li><span class="arithmatex">\(\boldsymbol \theta_0\)</span> (selected at random) <span class="arithmatex">\(\rightarrow\)</span> Simple task: <span class="arithmatex">\(\tilde{\boldsymbol \theta}\)</span> <span class="arithmatex">\(\rightarrow\)</span> Hard task: <span class="arithmatex">\(\tilde{\boldsymbol \theta'}\)</span></li>
<li><span class="arithmatex">\(\boldsymbol \theta^1_0\)</span> (selected at random) <span class="arithmatex">\(\rightarrow\)</span> Simple network: <span class="arithmatex">\(\tilde{\boldsymbol \theta^1}, \boldsymbol \theta^2_0\)</span>
  <span class="arithmatex">\(\rightarrow\)</span> Complex network: <span class="arithmatex">\(\tilde{\boldsymbol \theta^1}, \tilde{\boldsymbol \theta^2}\)</span></li>
</ul>
<p>where in the latter case a common approach is to fix the hidden layers and discard the output layer after the first training process, add a number of extra layers to make the network deeper and continue training those layers alone. However, since N independent optimizations generally do not provide the overall optimal solution, a final fine-tuning step may be required.</p>
<h2 id="additional-readings">Additional readings</h2>
<ul>
<li>A <a href="https://github.com/jettify/pytorch-optimizer">great resource</a> containing references (and Pytorch implementations) of more than 20 optimizers. This may be a good starting point if interest to experiment with different optimizers in both classical optimization and training of NNs.</li>
<li>Another great <a href="https://github.com/labmlai/annotated_deep_learning_paper_implementations">resource</a> with step-by-step implementations of 
  some popular optimizers and networks.</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../07_bestpractice/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Best practices in the training of Machine Learning models" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Best practices in the training of Machine Learning models
            </div>
          </div>
        </a>
      
      
        
        <a href="../09_mdn/" class="md-footer__link md-footer__link--next" aria-label="Next: Uncertainty Quantification in Neural Networks and Mixture Density Networks" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Uncertainty Quantification in Neural Networks and Mixture Density Networks
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>