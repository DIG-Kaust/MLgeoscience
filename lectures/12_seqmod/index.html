
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Sequence modelling - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#sequence-modelling" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Sequence modelling
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Sequence modelling
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Sequence modelling
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    Motivation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-rnn" class="md-nav__link">
    Basic RNN
  </a>
  
    <nav class="md-nav" aria-label="Basic RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss" class="md-nav__link">
    Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backprop" class="md-nav__link">
    Backprop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bidirectional-rnn" class="md-nav__link">
    Bidirectional RNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deep-rnns" class="md-nav__link">
    Deep RNNs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#long-term-dependencies-implications-for-gradients" class="md-nav__link">
    Long-term dependencies: implications for gradients
  </a>
  
    <nav class="md-nav" aria-label="Long-term dependencies: implications for gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    Gradient clipping
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gated-recurrent-networks-or-gru-unit" class="md-nav__link">
    Gated recurrent networks or GRU unit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-short-term-memory-lstm-unit" class="md-nav__link">
    Long-short term memory (LSTM) unit
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#present-and-future-of-sequence-modelling" class="md-nav__link">
    Present and future of sequence modelling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_INN/" class="md-nav__link">
        Invertible Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19_implicit/" class="md-nav__link">
        Implicit neural networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#motivation" class="md-nav__link">
    Motivation
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#basic-rnn" class="md-nav__link">
    Basic RNN
  </a>
  
    <nav class="md-nav" aria-label="Basic RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#architecture" class="md-nav__link">
    Architecture
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#loss" class="md-nav__link">
    Loss
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#backprop" class="md-nav__link">
    Backprop
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#inference" class="md-nav__link">
    Inference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#bidirectional-rnn" class="md-nav__link">
    Bidirectional RNN
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#deep-rnns" class="md-nav__link">
    Deep RNNs
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#long-term-dependencies-implications-for-gradients" class="md-nav__link">
    Long-term dependencies: implications for gradients
  </a>
  
    <nav class="md-nav" aria-label="Long-term dependencies: implications for gradients">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#gradient-clipping" class="md-nav__link">
    Gradient clipping
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#gated-recurrent-networks-or-gru-unit" class="md-nav__link">
    Gated recurrent networks or GRU unit
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#long-short-term-memory-lstm-unit" class="md-nav__link">
    Long-short term memory (LSTM) unit
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#present-and-future-of-sequence-modelling" class="md-nav__link">
    Present and future of sequence modelling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="sequence-modelling">Sequence modelling</h1>
<p>In this lecture we will start investigating a family of Neural Network that are particularly suitable for learning tasks that involve sequences
as input data.</p>
<p>To understand what a sequence is in the context of Deep learning, let's consider a recording over time (e.g., an audio recording):</p>
<p><img alt="SEQUENCE" src="../figs/sequence.png" /></p>
<p>Compared to other dataset types (e.g., tabular or gridded data), the different samples of a sequence present an obvious degree of correlation
that tends to diminuish the further away to samples are from each other. Moreover, in the case of multi-feature sequences (e.g., multi-component
seismological recordings), the overall sequence contains a number of features at each time step that can be more or less correlated to each other.</p>
<p>Sequences appear in every aspect of life. For example, outside of geoscience, the two most commonly used data in sequence modelling are:</p>
<ul>
<li>text</li>
<li>audio</li>
</ul>
<p>More specifically, as we will see, the field of Natural Language Processing (NPL) has experienced a revolutionary growth in the last decade thanks
to sequence modelling and deep learning. In geoscience, many of the commonly used datasets can also be interpreted as sequences, for example:</p>
<ul>
<li>seismograms</li>
<li>well logs</li>
<li>production data</li>
</ul>
<p>are all datatypes that present a certain degree of correlation along either the time or depth axis. </p>
<p>Finally, similar to FFNs or CNNs, sequence modelling can be used for various applications:</p>
<ul>
<li>Single output classification: given an input sequence of a certain length <span class="arithmatex">\(\mathbf{x}\)</span>, a model is trained to decide whether than sequence contains
  a feature of interest or not. For example, given a seismogram we may be interest to detect the presence of a seismic event, or we may want to find out
  if a well log is clean or corrupted by some recording error or what is the facies in the middle of the sequence;</li>
<li>Multi output classification (i.e., semantic segmentation): given an input sequence of a certain length <span class="arithmatex">\(\mathbf{x}\)</span>, a model is trained to classify each element
  of the input sequence into a predefined set of classes. Taking once again the example of facies labelling, here the task is extended to predicting labels
  at each depth level (and not only in the middle of the sequence);</li>
<li>Regression: given an input sequence of a certain length <span class="arithmatex">\(\mathbf{x}\)</span>, a model is trained to predict a continuous output, which could be
  a single value <span class="arithmatex">\(y\)</span> or a sequence of values <span class="arithmatex">\(\mathbf{y}\)</span> that has the same (or different length) of the input. For example, given a set of 
  well logs we may want to predict another one that was not acquired. Similarly, given a seismic trace recorded by the vertical component of a geophone
  we may be interested to predict the horizontal components. Both of these example fall under the area of <em>domain translation</em>;</li>
</ul>
<h2 id="motivation">Motivation</h2>
<p>Let's start by considering what we have learned so far and discuss how we could use those tools to handle sequential data. First of all,
we consider a sequence of <span class="arithmatex">\(N_\tau\)</span> samples and <span class="arithmatex">\(N_f\)</span> features:</p>
<div class="arithmatex">\[
\mathbf{X} = \begin{bmatrix} 
                x_1^{&lt;1&gt;} &amp; x_1^{&lt;2&gt;} &amp; x_1^{&lt;N_\tau&gt;} \\
                ...     &amp; ...     &amp; ... \\
                x_{N_f}^{&lt;1&gt;} &amp; x_1^{&lt;2&gt;} &amp; x_{N_f}^{&lt;N_\tau&gt;}
  \end{bmatrix} =
  \begin{bmatrix} 
                \mathbf{x}^{&lt;1&gt;} &amp; \mathbf{x}^{&lt;2&gt;} &amp; \mathbf{x}^{&lt;N_\tau&gt;}
  \end{bmatrix}_{[N_f \times N_\tau]}
\]</div>
<p>we could easily deal with this as if it was a 2D-array (i.e., an image) and use CNNs. However, the locality argument used for the convolutional filters
that constitute a convolutional layer would not make much sense here, especially if we know that elements in the sequence away from each other may still have
a certain degree of correlation. Alternatively, the matrix <span class="arithmatex">\(\mathbf{X}\)</span> could be simply vectorized and used as input to a FFN. This approach does 
however present two main limitations:</p>
<ul>
<li>since the vector <span class="arithmatex">\(vec(\mathbf{X})\)</span> is likely to be very long, weight matrices will be very large leading to a very expensive training process;</li>
<li>FFNs cannot easily handle inputs of variable lengths, so all sequences will need to have fixed length. We will see that being able to handle 
  variable-length sequences is very useful in some situations. </li>
</ul>
<p>Both problems can be overcome by taking advantage of <em>parameter sharing</em>. We have already introduced this concept in the context of CNNs,
where the same filters are used in different parts of the input. Similarly in sequence modelling, the idea of parameter sharing allows using the same
parameters at different stages of the sequence and therefore allows the network to easily handle sequences of variable length. By doing so,
a new type of neural network is created under the name of Recurrent Neural Network (RNN):</p>
<p><img alt="RNN" src="../figs/rnnfold.png" /></p>
<p>where <span class="arithmatex">\(\mathbf{x}\)</span> is the input vector (or matrix when multiple features are present), <span class="arithmatex">\(\mathbf{y}\)</span> is the output vector, and <span class="arithmatex">\(\mathbf{h}\)</span>
is the so called hidden state vector.</p>
<p>As clearly shown in the unrolled version of the network into a standard computational graph, various inputs and hidden states are passed through
the same function <span class="arithmatex">\(f_\theta\)</span> with a given number of training parameters. This is very different from a feed-forward network where different functions 
is are used over consecutive layers. The choice of the function <span class="arithmatex">\(f_\theta\)</span> leads to the definition of different RNN architectures. </p>
<p>Before we begin introducing a number of popular architectures for sequence modelling, let's introduce some useful notation. Inputs and outputs of a RNNs
will be always defined as follows:</p>
<div class="arithmatex">\[
\mathbf{X} = \begin{bmatrix} 
                \mathbf{x}^{&lt;1&gt;} &amp; \mathbf{x}^{&lt;2&gt;} &amp; \mathbf{x}^{&lt;T_x&gt;}
  \end{bmatrix}_{[N_f \times T_x]}
\]</div>
<p>and</p>
<div class="arithmatex">\[
\mathbf{Y} = \begin{bmatrix} 
                \mathbf{y}^{&lt;1&gt;} &amp; \mathbf{y}^{&lt;2&gt;} &amp; \mathbf{y}^{&lt;T_y&gt;}
  \end{bmatrix}_{[N_t \times T_y]}
\]</div>
<p>where <span class="arithmatex">\(T_x\)</span> and <span class="arithmatex">\(T_y\)</span> are the length of the input and output sequences. First, note that this notations differs from before in that a
single training sample is now represented as a matrix; therefore, the entire training data becomes a 3-D tensor of size <span class="arithmatex">\([N_s \times N_f \times T_x]\)</span> 
(and <span class="arithmatex">\([N_s \times N_t \times T_y]\)</span>). Finally, note that in the most general case these parameters may be sample dependant (i.e., when we allow sequences of variable size): the following notation will be used in that case, <span class="arithmatex">\(T_x^{(i)}\)</span> and <span class="arithmatex">\(T_y^{(i)}\)</span>
where <span class="arithmatex">\(i\)</span> refers to the i-th training sample. Moreover, given that we recurrently apply the same function <span class="arithmatex">\(f_\theta\)</span>, we can very compactly write an
RNN as:</p>
<div class="arithmatex">\[
\mathbf{h}^{&lt;t&gt;}, \mathbf{y}^{&lt;t&gt;}=f_\theta(\mathbf{h}^{&lt;t-1&gt;}, \mathbf{x}^{&lt;t&gt;}) \qquad t=1,2,T_x
\]</div>
<p>that we can unroll into:</p>
<div class="arithmatex">\[
\mathbf{h}^{&lt;t&gt;}, \mathbf{y}^{&lt;t&gt;}=f_\theta(f_\theta(f_\theta(\mathbf{h}^{&lt;0&gt;}, \mathbf{x}^{&lt;1&gt;}), ...), \mathbf{x}^{&lt;t-2&gt;}), \mathbf{x}^{&lt;t-1&gt;}), \mathbf{x}^{&lt;t&gt;}) 
\]</div>
<p>As we have already briefly mentioned, RNNs allows some flexibility on the choice of <span class="arithmatex">\(T_y\)</span> (i.e., the length of the output sequence).
This leads to the creation of different network architectures that are suitable to different tasks:</p>
<p><img alt="RNNARCHS" src="../figs/rnnarchs.png" /></p>
<p>Note that in the cases 3 and 4, the predicted output is fed back to the network as input to the next step at inference stage as shown in the figure above. 
At training stage, however, the true output is used as input.</p>
<p>In summary, what we wish to achieve here is to create a network that can learn but short and long term relationships in the data such that
both samples closes to each other as well as far away samples can help in the prediction of the current step. By using parameter sharing in a smart way,
we can avoid overparametrizing the network and therefore limit the risk of overfitting on short and long term trends in the data. In other words,
by assuming stationariety in the data, we let the network understand if step <span class="arithmatex">\(t\)</span> and <span class="arithmatex">\(t+N_t\)</span> are correlated to each other across the entire time sequence,
instead of giving the network with the freedom to find relationships between any two samples in the sequence.</p>
<h2 id="basic-rnn">Basic RNN</h2>
<h3 id="architecture">Architecture</h3>
<p>It is now time to discuss in more details what is an effective function, <span class="arithmatex">\(f_\theta\)</span>.</p>
<p>The most basic Recurrent Neural Network can be written as follows:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbf{a}^{&lt;t&gt;} &amp;= \mathbf{W}_h \mathbf{h}^{&lt;t-1&gt;} + \mathbf{W}_x \mathbf{x}^{&lt;t&gt;} + \mathbf{b}_a = \mathbf{W} [\mathbf{h}^{&lt;t-1&gt;}, \mathbf{x}^{&lt;t&gt;}]^T + \mathbf{b}_a  \\
\mathbf{h}^{&lt;t&gt;} &amp;= \sigma(\mathbf{a}^{&lt;t&gt;} )  \\
\mathbf{o}^{&lt;t&gt;} &amp;= \mathbf{W}_y \mathbf{h}^{&lt;t&gt;} + \mathbf{b}_y \\
\hat{\mathbf{y}}^{&lt;t&gt;} &amp;= \sigma' (\mathbf{o}^{&lt;t&gt;}) 
\end{aligned}
\]</div>
<p>where:</p>
<ul>
<li><span class="arithmatex">\(\sigma\)</span> and <span class="arithmatex">\(\sigma'\)</span> are the activation functions for the hidden and output paths (the choice of the activation
for the latter depends on the problem we wish to solve, e.g., softmax for binary classification)</li>
<li><span class="arithmatex">\(\mathbf{h}^{&lt;0&gt;}\)</span> is the initial hidden state vector which is usually initalialized as a zero vector.</li>
<li><span class="arithmatex">\(\mathbf{W} = [\mathbf{W}_h, \mathbf{W}_x]_{[N_h \times N_h + N_x]}\)</span> is the matrix of weights for the hidden path</li>
<li><span class="arithmatex">\(\mathbf{W}_{y \; [N_y \times N_h]}\)</span> is the matrix of weights for the output path</li>
</ul>
<p>In conclusion, the learnable parameters for this kind of RNN block are: <span class="arithmatex">\(\mathbf{W}_h, \mathbf{W}_x, \mathbf{W}_y, 
\mathbf{b}_a, \mathbf{b}_y\)</span> whose overall size is <span class="arithmatex">\(N_h(N_h+N_x) + N_y N_h + N_h + N_y\)</span>. To give some perspective, this 
is much smaller than the number of learnable parameters of an 'equivalent' Feed-Forward network where the entire input matrix <span class="arithmatex">\(\mathbf{X}\)</span>
is flattened into a 1-d array of size <span class="arithmatex">\(N_f T_x\)</span> and the entire output matrix <span class="arithmatex">\(\mathbf{Y}\)</span>
is flattened into a 1-d array of size <span class="arithmatex">\(N_t T_y\)</span>. The equivalent weight matrix and bias vectors have size <span class="arithmatex">\(N_x N_y T_x T_y\)</span> and <span class="arithmatex">\(N_yT_y\)</span>.
For example, given a problem of size <span class="arithmatex">\(N_x=2\)</span>, <span class="arithmatex">\(N_y=3\)</span>, <span class="arithmatex">\(N_h=5\)</span>, and <span class="arithmatex">\(T_x=T_y=4\)</span>, we obtain <span class="arithmatex">\(N_{FFN}=108\)</span> and <span class="arithmatex">\(N_{RNN}=58\)</span>.</p>
<p><img alt="BASICRNN" src="../figs/basicrnn.png" /></p>
<h3 id="loss">Loss</h3>
<p>Once the architecture is defined, the next step is to understand how the loss function should be defined for this kind of networks.
As shown in the figure below, this can be simply accomplished by considering a loss function per time step and summing them together:</p>
<div class="arithmatex">\[
\mathscr{L} = \sum_{t=1}^{T_x} \mathscr{L}^{&lt;t&gt;}, \qquad \mathscr{L}^{&lt;t&gt;}= f(\hat{\mathbf{y}}^{&lt;t&gt;}, \mathbf{y}^{&lt;t&gt;})
\]</div>
<p>where <span class="arithmatex">\(f\)</span> can be the MSE, MAE, BCE, etc. This loss function can be easily interpreted in probabilistic terms as:</p>
<div class="arithmatex">\[
f \rightarrow -log P (\mathbf{y}^{&lt;t&gt;} | \mathbf{x}^{&lt;1&gt;}, \mathbf{x}^{&lt;2&gt;}, ..., \mathbf{x}^{&lt;t&gt;})
\]</div>
<p>To conclude, we note that the process of evaluating the various terms of the loss function is sequential as a previous hidden state is 
required to evaluate the current output. This can be very expensive and does not allow for parallelization (beyond across training samples), similar
to the case of very deep feedforward neural networks.</p>
<p><img alt="BASICRNLOSS" src="../figs/basicrnn_loss.png" /></p>
<h3 id="backprop">Backprop</h3>
<p>Given the loss function defined above, the computation of its gradient easily follows the principles that we have already extensively
discussed in previous lectures; in simple terms, the backpropagation algorithm is applied on the unrolled computational graph in order
to obtain the gradients of the weights and biases of the network block. Backpropagation over an RNN block is usually referred to as
back-propagation through time (BPTT).</p>
<p>Looking at this in more details, we can observe how the overall gradient of each of the weights or biases can be written as</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \cdot} = \sum_{t=1}^{T_x} \frac{\partial \mathscr{L}^{&lt;t&gt;}}{\partial \cdot}
\]</div>
<p>or, in other words, the gradient accumulates over the unrolled graph. Note also that,</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathscr{L}^{&lt;t&gt;}} = 1, \qquad \frac{\partial \mathscr{L}}{\partial \cdot} = \sum_{t=1}^{T_x} \frac{\partial \mathscr{L}}{\partial \mathscr{L}^{&lt;t&gt;}} \frac{\partial \mathscr{L}^{&lt;t&gt;}}{\partial \cdot} = 
\sum_{t=1}^{T_x} \frac{\partial \mathscr{L}^{&lt;t&gt;}}{\partial \cdot}
\]</div>
<p><img alt="BASICRNBACK" src="../figs/basicrnn_backprop.png" /></p>
<p>Let's now look more in details at the equations of backpropagation through time for a specific case of multi-label classification. 
More specifically we assume that the output of each step of the recurrent network (<span class="arithmatex">\(\mathbf{o}^{&lt;t&gt;}\)</span>) is passed through a softmax
to get <span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t&gt;}= \sigma' (\mathbf{o}^{&lt;t&gt;})\)</span>, and the loss in the negative log-likelihood of a Multinoulli distribution. 
Moreover, we will use tanh for the internal activation function <span class="arithmatex">\(\sigma\)</span>. Starting from the gradients of the internal nodes:</p>
<div class="arithmatex">\[
\left(\frac{\partial \mathscr{L}}{\partial \mathbf{o}^{&lt;t&gt;}}\right)_i = \hat{y}_i^{&lt;t&gt;} - \mathbf{1}_{i=y^{&lt;t&gt;}}
\]</div>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;T_x&gt;}} = \frac{\partial \mathscr{L}^{&lt;T_x&gt;}}{\partial \mathbf{o}^{&lt;T_x&gt;}} 
\frac{\partial \mathbf{o}^{&lt;T_x&gt;}}{\partial \mathbf{h}^{&lt;T_x&gt;}} = \mathbf{W}_y^T (\hat{\mathbf{y}}^{&lt;T_x&gt;} - \mathbf{1}_{i=y^{&lt;T_x&gt;}})
\]</div>
<div class="arithmatex">\[
\begin{aligned}
\frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}} &amp;= \frac{\partial \mathscr{L}}{\partial \mathbf{o}^{&lt;t&gt;}} 
\frac{\partial \mathbf{o}^{&lt;t&gt;}}{\partial \mathbf{h}^{&lt;t&gt;}} + \frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t+1&gt;}} 
\frac{\partial \mathbf{h}^{&lt;t+1&gt;}}{\partial \mathbf{h}^{&lt;t&gt;}} \\
&amp;= \mathbf{W}_y^T (\hat{\mathbf{y}}^{&lt;t&gt;} - \mathbf{1}_{i=y^{&lt;t&gt;}}) + \mathbf{W}_h^T diag(1 - (\mathbf{h}^{&lt;t+1&gt;})^2) 
\frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t+1&gt;}}
\end{aligned}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{1}_{i=y^{&lt;t&gt;}}\)</span> is a vector of zeros with 1 at location of the true label, i.e. <span class="arithmatex">\(i=y^{&lt;t&gt;}\)</span>, <span class="arithmatex">\(diag(1 - (\mathbf{h}^{&lt;t+1&gt;})^2)\)</span>
is the Jacobian of the tanh activation function, and <span class="arithmatex">\(\partial \mathscr{L} / \partial \mathbf{h}^{&lt;t+1&gt;}\)</span> is computed recursively from <span class="arithmatex">\(t+1=T_x\)</span>
as we know <span class="arithmatex">\(\partial \mathscr{L} / \partial \mathbf{h}^{&lt;T_x&gt;}\)</span>. Moreover, it is worth noting how the gradient of the loss function
over any hidden state <span class="arithmatex">\(\mathbf{h}^{&lt;t&gt;}\)</span> is composed of two terms, one coming directly from the corresponding output 
<span class="arithmatex">\(\mathbf{o}^{&lt;t&gt;}\)</span> and one from the next hidden state <span class="arithmatex">\(\mathbf{h}^{&lt;t+1&gt;}\)</span>.</p>
<p>It follows that the gradients of the parameters to update are:</p>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathbf{b}_y} = \sum_{t=1}^{T_x} \left( \frac{\partial \mathbf{o}^{&lt;t&gt;}}{\partial \mathbf{b}_y} \right)^T 
\frac{\partial \mathscr{L}}{\partial \mathbf{o}^{&lt;t&gt;}} =  \sum_{t=1}^{T_x} \frac{\partial \mathscr{L}}{\partial \mathbf{o}^{&lt;t&gt;}}
\]</div>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathbf{b}_a} = \sum_{t=1}^{T_x} \left( \frac{\partial \mathbf{h}^{&lt;t&gt;}}{\partial \mathbf{b}_a} \right)^T 
\frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}} = \sum_{t=1}^{T_x} diag(1 - (\mathbf{h}^{&lt;t&gt;})^2) \frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}}
\]</div>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathbf{W}_y} = \sum_{t=1}^{T_x} \frac{\partial \mathscr{L}}{\partial \mathbf{o}^{&lt;t&gt;}} 
\frac{\partial \mathbf{o}^{&lt;t&gt;}}{\partial \mathbf{W}_y} =
\sum_{t=1}^{T_x} \frac{\partial \mathscr{L}}{\partial \mathbf{o}^{&lt;t&gt;}} \mathbf{h}^{&lt;t&gt;T}
\]</div>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathbf{W}_h} = \sum_{t=1}^{T_x} \frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}} 
\frac{\partial \mathbf{h}^{&lt;t&gt;}}{\partial \mathbf{W}_h} = \sum_{t=1}^{T_x} diag(1 - (\mathbf{h}^{&lt;t&gt;})^2) \frac{\partial 
\mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}} \mathbf{h}^{&lt;t-1&gt;T}
\]</div>
<div class="arithmatex">\[
\frac{\partial \mathscr{L}}{\partial \mathbf{W}_x} = \sum_{t=1}^{T_x} \frac{\partial \mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}} 
\frac{\partial \mathbf{h}^{&lt;t&gt;}}{\partial \mathbf{W}_x} = \sum_{t=1}^{T_x} diag(1 - (\mathbf{h}^{&lt;t&gt;})^2) \frac{\partial 
\mathscr{L}}{\partial \mathbf{h}^{&lt;t&gt;}} \mathbf{x}^{&lt;t&gt;T}
\]</div>
<h3 id="inference">Inference</h3>
<p>At test time, the evaluation of a RNN is straightforward. We usually simply need to pass through the forward pass and get 
the output <span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t&gt;}\)</span>. However, this is not always true, especially in the following two cases:</p>
<ul>
<li><span class="arithmatex">\(T_x=1, T_y&gt;1\)</span> (generative network)</li>
<li><span class="arithmatex">\(T_x, T_y\)</span> (encoder-decoder network)</li>
</ul>
<p>as in both cases we will be required to use the output at a given step (<span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t-1&gt;}\)</span>) as part of the input to
produce the output of the next step (<span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t&gt;}\)</span>). These two scenarios are dominant in so-called <em>Language Modelling</em>
for tasks where we want to generate sentences given some initial guess (e.g., first word) or perform language-to-language translation.
However, similar concepts could also be used to for example generate well logs or seismograms. Let's briefly take a look at some of
the required changes in the inference process of these 2 network types. </p>
<p>First of all, in conventional cases our loss function can be written as:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathscr{L} &amp;= \prod_{t=1}^{T_x} P (\mathbf{y}^{&lt;t&gt;} | \mathbf{x}^{&lt;1&gt;}, \mathbf{x}^{&lt;2&gt;}, ..., \mathbf{x}^{&lt;t&gt;}) \\
&amp;= - \sum_{t=1}^{T_x} log P (\mathbf{y}^{&lt;t&gt;} | \mathbf{x}^{&lt;1&gt;}, \mathbf{x}^{&lt;2&gt;}, ..., \mathbf{x}^{&lt;t&gt;})
\end{aligned}
\]</div>
<p>where each output is here totally independent from the others. On the other hand, we are now faced with a joint distribution to
sample from:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathscr{L} &amp;= P (\mathbf{y}^{&lt;1&gt;}, \mathbf{y}^{&lt;2&gt;},..., \mathbf{y}^{&lt;t&gt;})\\
&amp;= - \prod_{t=1}^{T_x} log P (\mathbf{y}^{&lt;t&gt;} | \mathbf{y}^{&lt;1&gt;}, \mathbf{y}^{&lt;2&gt;},..., \mathbf{y}^{&lt;t-1&gt;})
\end{aligned}
\]</div>
<p>Evaluating such a probability is not a big deal during training as we can simply use the true labels as inputs (similarly to the 
more conventional network architectures where we use <span class="arithmatex">\(\mathbf{x}^{&lt;t&gt;}\)</span>) instead. However, at inference stage we do not have access to the
exact previous outputs when evaluating the current one. In order to simplify the evaluation of such a probability, we are therefore required
to make an assumption: more specifically, we assume that the outputs can be modelled as a Markov Chain. In other words, we assume that the 
current output depends only on the previous one and not all of the other previous outputs. We can therefore write:</p>
<div class="arithmatex">\[
\mathscr{L} \approx - \prod_{t=1}^{T_x} log P (\mathbf{y}^{&lt;t&gt;} | \hat{\mathbf{y}}^{&lt;t-1&gt;})
\]</div>
<p>which can be easily evaluated by placing the prediction at step <span class="arithmatex">\(t-1\)</span> as input to step <span class="arithmatex">\(t\)</span>. </p>
<p>However, when we are interested in using our trained RNN for generative tasks, this approach comes with a limitation. 
It is in fact deterministic, and therefore we can only create a single output sequence. A more sophisticated procedure can be
designed such that we can take advantage of our predictions in terms of their probabilities (and not the most probable outcome).
Given <span class="arithmatex">\(P (\mathbf{y}^{&lt;t-1&gt;} | ...)\)</span> (from, e.g., before a softmax later), what we can instead do is to sample one value of 
<span class="arithmatex">\(\mathbf{y}^{&lt;t-1&gt;}\)</span> and feed it to the next step of our recurrent network. If we now repeat the same procedure multiple times,
we will produce a bunch of different sequences. Finally, we could go even one step beyond and sample multiple values at step <span class="arithmatex">\(t-1\)</span>, 
feed them concurrently to the next step (or the next N steps) and evaluate which one(s) has the highest joint probability, then go back 
to step <span class="arithmatex">\(t-1\)</span> and choose that value(s). This procedure, usually referred as <em>Beam Search</em>, is however beyond the scope of this lecture.</p>
<h2 id="bidirectional-rnn">Bidirectional RNN</h2>
<p>Up until now, we have tried to construct NNs that can learn from short and long term patterns in the data in a <em>causal</em> fashion: in other
words, by feeding our time series from left to right to the network we allow it at every time step <span class="arithmatex">\(t\)</span> to learn dependencies from 
the past <span class="arithmatex">\((t-1,t-2,t-i)\)</span>. This is very useful for streaming data where we record the data sequentially from <span class="arithmatex">\(t=0\)</span> to <span class="arithmatex">\(t=T_x\)</span>, and we do not
want to wait until the entire data has been collected before we can make some predictions. This is usually referred to as <em>online</em> processing. 
An example of such a scenario is represented by real-time drilling, when we drill a hole into the subsurface and record some measurements whilst doing so. We would like a machine to process
such recordings as they come in and provide us with useful insights on how to best continue drilling:</p>
<p><img alt="DRILLBIT" src="../figs/drillbit.png" /></p>
<p>Of course, not every problem lends naturally to the above depicted scenario. In most cases we are able to record data over an entire time window
and only after that we are concerned with analyzing such data. This is usually referred to as <em>offline</em> processing. In this case it may be useful
to also look at correlations between samples at time <span class="arithmatex">\(t\)</span> and future samples <span class="arithmatex">\((t+1,t+2,t+i)\)</span>. Bidirectional RNNs represent a solution to this as they 
allow learning short and long term dependencies not only from the past but also from the future. Let's start with a schematic diagram:</p>
<p><img alt="BRNN" src="../figs/brnn.png" /></p>
<p>where the network architecture presents a simple modification. Instead of having a single flow of information from left to right as it is the 
case for basic RNNs, we have now added a second flow of information from right to left. The hidden states of the first have been labelled with
the suffix F (for forward), and those of the second with the suffix B (for backward). The inputs remain unchanged, apart from the fact that they 
are now fed twice to the network, once for the forward flow and once for the backward flow, whilst the output is now the concatenation of the 
outputs of the two flows, i.e., <span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t&gt;} = [\hat{\mathbf{y}}_F^{&lt;t&gt;T} \; \hat{\mathbf{y}}_B^{&lt;t&gt;T}]^T\)</span>.</p>
<h2 id="deep-rnns">Deep RNNs</h2>
<p>Similarly to any other network architecture that we have investigated so far, the concept of shallow and deep network also applies to RNNs. Shallow
RNNs are recurrent networks that have a single hidden layer connecting the inputs to the outputs. On the other than, deep RNNs are composed of more hidden
layers. This is simply achieved as follows:</p>
<ul>
<li><strong>First layer</strong> input: <span class="arithmatex">\(\mathbf{x}^{&lt;t&gt;}\)</span>, hidden and output: <span class="arithmatex">\(\mathbf{h}_0^{&lt;t&gt;}\)</span>,</li>
<li><strong>Second layer</strong> input: <span class="arithmatex">\(\mathbf{h}_0^{&lt;t&gt;}\)</span>, hidden and output: <span class="arithmatex">\(\mathbf{h}_1^{&lt;t&gt;}\)</span>,</li>
<li><strong>Last layer</strong> input: <span class="arithmatex">\(\mathbf{h}_{N-1}^{&lt;t&gt;}\)</span>, hidden:<span class="arithmatex">\(\mathbf{h}_N^{&lt;t&gt;}\)</span>, output: <span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t&gt;}\)</span>.</li>
</ul>
<p>that we can visually represent as:</p>
<p><img alt="DEEPRNN" src="../figs/deeprnn.png" /></p>
<p>Mathematically, a deep RNN can be simply expressed as follows.</p>
<ul>
<li>
<p>For <span class="arithmatex">\(i=0,1,N-1\)</span> (with <span class="arithmatex">\(\mathbf{h}_{-1}=\mathbf{x}\)</span>)</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbf{a}_i^{&lt;t&gt;} &amp;= \mathbf{W}_{h_i} \mathbf{h}_i^{&lt;t-1&gt;} + \mathbf{W}_{x_i} \mathbf{h}_{i-1}^{&lt;t&gt;} + \mathbf{b}_{a_i} \\
\mathbf{h}_i^{&lt;t&gt;} &amp;= \sigma(\mathbf{a}_i^{&lt;t&gt;} )  \\
\end{aligned}
\]</div>
</li>
<li>
<p>For <span class="arithmatex">\(i=N\)</span></p>
<div class="arithmatex">\[
\begin{aligned}
\mathbf{a}_N^{&lt;t&gt;} &amp;= \mathbf{W}_{h_N} \mathbf{h}_N^{&lt;t-1&gt;} + \mathbf{W}_{x_N} \mathbf{h}_{N-1}^{&lt;t&gt;} + \mathbf{b}_{a_N} \\
\mathbf{h}_N^{&lt;t&gt;} &amp;= \sigma(\mathbf{a}_N^{&lt;t&gt;} )  \\ 
\mathbf{o}^{&lt;t&gt;} &amp;= \mathbf{W}_y \mathbf{h}_N^{&lt;t&gt;} + \mathbf{b}_y \\
\hat{\mathbf{y}}^{&lt;t&gt;} &amp;= \sigma' (\mathbf{o}^{&lt;t&gt;})  \\
\end{aligned}
\]</div>
</li>
</ul>
<h2 id="long-term-dependencies-implications-for-gradients">Long-term dependencies: implications for gradients</h2>
<p>In this section, we will discuss a long-standing challenge arising when implementing backpropagation through a RNN. A number
of solution to circumvent this problem will be presented in following sections.</p>
<p>Let's start by considering the forward pass of a recurrent network. For the information to flow from left to right, a 
recurrent network repeatedly applies the matrix <span class="arithmatex">\(\mathbf{W}_h\)</span> to the hidden state vectors (interleaved by nonlinear
transformations): as already discussed in the <a href="../08_gradopt1/">Optimization</a> lecture, this leads to raising the eigenvalues
of this matrix to the power of <span class="arithmatex">\(T_x\)</span>. Eigenvalues smaller than one decay very fast to zero, whilst those bigger than one grow
exponentially fast to infinity. As a consequence, only the part of the initial vector <span class="arithmatex">\(\mathbf{h}^{&lt;0&gt;}\)</span> aligned with the largest
eigenvectors successfully propagates through the network whilst the other components become insignificant after a few steps. So,
no matter how we choose the initial weights of the network and hidden state, long term dependencies tend to become irrelevant when
compared to short terms ones in terms of their contribution to the gradient. In other words, the network will take a long time to
train and learn long-term dependencies. </p>
<p>In order to avoid that, a number of strategies have been proposed in the literature. In the following, we will look at three of them:
the first tries to circumvent this problem as part of the learning process, whilst the latter two tackle the issue from the perspective 
of the network architecture design. By no means, these are the preferred choices nowadays when using RNNs.</p>
<h3 id="gradient-clipping">Gradient clipping</h3>
<p>We have previously mentioned that one simple strategy to prevent exploding gradient is represented by so-called gradient clipping. As the name
suggests, this is applied only during the backward pass to gradients that overcome a given threshold. A forward-backward pass with gradient
clipping can be therefore written as:</p>
<ul>
<li>Forward pass: <span class="arithmatex">\(\hat{\mathbf{y}}^{&lt;t&gt;} = f_\theta(\mathbf{x}^{&lt;t&gt;} , \mathbf{h}^{&lt;0&gt;}) \; \forall t=0,1,...T_x\)</span></li>
<li>Backward pass: <span class="arithmatex">\(\partial \mathscr{L} / \partial \theta\)</span></li>
<li>Gradient clipping: if <span class="arithmatex">\(|\partial \mathscr{L} / \partial \theta| &gt; th\)</span>, then 
  <span class="arithmatex">\(\partial \mathscr{L} / \partial \theta = sign(\partial \mathscr{L} / \partial \theta) \cdot th\)</span></li>
</ul>
<p>Unfortunately, a similar simply trick does not exist for the other problem, vanishing gradients. So, whislt adopting this strategy will avoid
instabilities in the training of basic RNNs, the training process will still be painfully slow.</p>
<h3 id="gated-recurrent-networks-or-gru-unit">Gated recurrent networks or GRU unit</h3>
<p>The most effective family of networks that can tackle both the exploding and vanishing gradient problem is called <em>Gated networks</em>. As the name
implies, a gate is introduced in each block of the network to help information flow and be used by later units without vanishing and exploding 
gradient issues. By doing so, the gate helps the network <em>remembering</em> some information from early steps, use it much later down the flow, and eventually
<em>forget</em> about it.</p>
<p>A GRU unit can be simply seen as a classical RNN unit with a number of small modifications. Let's start by drawing them side-by-side
(note that for the moment we are considering a simplified GRU block):</p>
<p><img alt="SIMPGRU" src="../figs/simpgru.png" /></p>
<p>Apart from a slight change in name (<span class="arithmatex">\(\mathbf{h}^{&lt;t&gt;}\)</span> has been replaced by <span class="arithmatex">\(\mathbf{c}^{&lt;t&gt;}\)</span>, which stands for <em>memory</em> cell), compared to the basic RNN
the GRU block contains a number of additional internal states. More specifically:</p>
<ul>
<li><span class="arithmatex">\(\tilde{\mathbf{c}}^{&lt;t&gt;}\)</span>: the candidate replacement for the memory cell. It is a candidate as in some cases it will not be used, rather the current 
  memory cell will be fast-tracked to allow learning long-term dependencies.</li>
<li><span class="arithmatex">\(\Gamma_u\)</span>: update gate, which is responsible to choose whether to pass the candidate memory cell <span class="arithmatex">\(\tilde{\mathbf{c}}^{&lt;t&gt;}\)</span> or the previous memory
  cell <span class="arithmatex">\(\mathbf{c}^{&lt;t-1&gt;}\)</span> to the next layer.</li>
</ul>
<p>The associated update equations for this simplified GRU block are:</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\boldsymbol \Gamma_{u}=\sigma\left(\mathbf{W}_{u}\left[\begin{array}{c}
\mathbf{c}^{&lt;t-1&gt;} \\
\mathbf{x}^{&lt;t&gt;}
\end{array}\right]+\mathbf{b}_{u}\right) \\
&amp;\tilde{\mathbf{c}}^{&lt;t&gt;}=\tanh \left(\mathbf{W}_{c}\left[\begin{array}{c} 
\mathbf{c}^{&lt;t-1&gt;} \\
\mathbf{x}^{&lt;t&gt;}
\end{array}\right]+\mathbf{b}_{c}\right) \\
&amp;\mathbf{c}^{&lt;t&gt;}=\boldsymbol \Gamma_{u} \cdot \tilde{\mathbf{c}}^{&lt;t&gt;}+\left(1-\boldsymbol \Gamma_{u}\right) \cdot  \mathbf{c}^{&lt;t-1&gt;}\\
&amp;\hat{\mathbf{y}}^{&lt;t&gt;}=\sigma' (\mathbf{W}_y \mathbf{c}^{&lt;t&gt;} + \mathbf{b}_{y})
\end{aligned}
\]</div>
<p>In the last equation, the new memory cell is computed as the linear interpolation between the old memory cell and the candidate one. 
However, since a sigmoid is usually chosen for the update gate, <span class="arithmatex">\(\boldsymbol \Gamma_{u}\)</span> roughly acts as a binary gate (0-stop, 1-pass). 
This way, the gate can stop the flowing of new information for a number of steps allowing the old information to be moved further up the flow
without being multiplicated by the weight matrix and therefore creating long-term dependencies that do not suffer from the vanishing gradient
problem. </p>
<p>To conclude, let's look at the real GRU and its equations, which introduces an additional gate called the relevance or reset gate <span class="arithmatex">\(\boldsymbol \Gamma_{r}\)</span>:</p>
<p><img alt="GRU" src="../figs/gru.png" /></p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\boldsymbol \Gamma_{u}=\sigma\left(\mathbf{W}_{u}\left[\begin{array}{c}
\mathbf{c}^{&lt;t-1&gt;} \\
\mathbf{x}^{&lt;t&gt;}
\end{array}\right]+\mathbf{b}_{u}\right) \\
&amp;\boldsymbol \Gamma_{r}=\sigma\left(\mathbf{W}_{r}\left[\begin{array}{c}
\mathbf{c}^{&lt;t-1&gt;} \\
\mathbf{x}^{&lt;t&gt;}
\end{array}\right]+\mathbf{b}_{r}\right) \\
&amp;\tilde{\mathbf{c}}^{&lt;t&gt;}=\tanh \left(\mathbf{W}_{c}\left[\begin{array}{c}
\boldsymbol \Gamma_{r} \cdot \mathbf{c}^{&lt;t-1&gt;} \\
\mathbf{x}^{&lt;t&gt;}
\end{array}\right]+\mathbf{b}_{c}\right) \\
&amp;\mathbf{c}^{&lt;t&gt;}=\boldsymbol \Gamma_{u} \cdot \tilde{\mathbf{c}}^{&lt;t&gt;}+\left(1-\boldsymbol \Gamma_{u}\right) \cdot \mathbf{c}^{&lt;t-1&gt;}\\
&amp;\hat{\mathbf{y}}^{&lt;t&gt;}=\sigma' (\mathbf{W}_y \mathbf{c}^{&lt;t&gt;} + \mathbf{b}_{y})
\end{aligned}
\]</div>
<h3 id="long-short-term-memory-lstm-unit">Long-short term memory (LSTM) unit</h3>
<p>Another popular, probably the most popular, RNN block that mitigates the vanishing gradient problem is called LSTM block. It uses similar concepts
to those introduced for the GRU block, but at the same time introduces a number of additional hidden states, namely:</p>
<ul>
<li><span class="arithmatex">\(\Gamma_f\)</span>: forget gate, which provides more flexibility when updating the memory cell with the old and candidate memory cells. 
  More specifically, whilst in the GRU block, the new memory cell was a linear combination of those two terms, now we have two independent weights (both of them learned) that 
  can allow passing more or less information from the two inputs instead of having to weight their total contribution to 1.</li>
<li><span class="arithmatex">\(\Gamma_o\)</span>: output gate;</li>
</ul>
<p><img alt="LSTM" src="../figs/lstm.png" /></p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\boldsymbol{\Gamma}_{u}=\sigma\left(\boldsymbol{W}_{u}\left[\begin{array}{c}
\boldsymbol{h}^{&lt;t-1&gt;} \\
\boldsymbol{x}^{&lt;t&gt;}
\end{array}\right]+\boldsymbol{b}_{u}\right) \\
&amp;\boldsymbol{\Gamma}_{\boldsymbol{f}}=\sigma\left(\boldsymbol{W}_{f}\left[\begin{array}{c}
h^{&lt;t-1&gt;} \\
x^{&lt;t&gt;}
\end{array}\right]+\boldsymbol{b}_{f}\right) \\
&amp;\boldsymbol{\Gamma}_{o}=\sigma\left(\boldsymbol{W}_{o}\left[\begin{array}{c}
\boldsymbol{h}^{&lt;t-1&gt;} \\
\boldsymbol{x}^{&lt;t&gt;}
\end{array}\right]+\boldsymbol{b}_{o}\right) \\
&amp;\tilde{\boldsymbol{c}}^{&lt;t&gt;}=\tanh \left(\boldsymbol{W}_{c}\left[\begin{array}{c}
\boldsymbol{h}^{&lt;t-1&gt;} \\
\boldsymbol{x}^{&lt;t&gt;}
\end{array}\right]+\boldsymbol{b}_{c}\right) \\
&amp;\boldsymbol{c}^{&lt;t&gt;}=\boldsymbol{\Gamma}_{u} \tilde{\boldsymbol{c}}^{&lt;t&gt;}+\boldsymbol{\Gamma}_{f} \boldsymbol{c}^{&lt;t-1&gt;} \\
&amp;\boldsymbol{h}^{&lt;t&gt;}=\boldsymbol{\Gamma}_{o} \tanh \left(\boldsymbol{c}^{&lt;t&gt;}\right) \\
&amp;\boldsymbol{y}^{&lt;t&gt;}=\sigma^{\prime}\left(\boldsymbol{W}_{y} \boldsymbol{h}^{&lt;t&gt;}+\boldsymbol{b}_{y}\right)
\end{aligned}
\]</div>
<h2 id="present-and-future-of-sequence-modelling">Present and future of sequence modelling</h2>
<p>Finally, it is worth noting that the field of sequence modelling with deep neural networks has been taken by a storm a couple of years
ago with novel architectures that have led to great improvements in the field of Natural Language Processing. The first innovation, which
goes under the name of <em>Attention</em> layer has been initially introduced to mitigate one of the main limitations of the encoder-decoder RNN
architecture that we have extensively discussed in this lecture. More specifically, the attention layer can find global correlations between
the input(s) of the decoder layer and any of the hidden states of the encoder, avoiding the problem of having a bottleneck at the end of the 
encoder and a single hidden state that is required to encode the information of the various inputs of the encoder. </p>
<p>The attention layer has later led to the design of a completely new type of neural network architecture, the so-called <em>Transformer</em> layer. In this
case, instead of processing the input sequentially as in RNNs, the transformer layer takes all the inputs at once and find both local and global correlations
by means of so-called self-attention blocks.</p>
<h2 id="additional-readings">Additional readings</h2>
<ul>
<li>If you are interested to learn more about attention and transformer layers, I recommend watching this
  <a href="![img.png](img.png)">lecture</a> from the KAUST Summer School on Unstructured Data in Geoscience</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../11_cnnarch/" class="md-footer__link md-footer__link--prev" aria-label="Previous: CNNs Popular Architectures" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              CNNs Popular Architectures
            </div>
          </div>
        </a>
      
      
        
        <a href="../13_dimred/" class="md-footer__link md-footer__link--next" aria-label="Next: Dimensionality reduction" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Dimensionality reduction
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>