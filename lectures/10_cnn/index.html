
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Convolutional Neural Networks - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#convolutional-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Convolutional Neural Networks
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Convolutional Neural Networks
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#convolution" class="md-nav__link">
    Convolution
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-convolution" class="md-nav__link">
    Why Convolution?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#padding-and-strides" class="md-nav__link">
    Padding and strides
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#channels" class="md-nav__link">
    Channels
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-layer" class="md-nav__link">
    Convolutional layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-network" class="md-nav__link">
    Convolutional network
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pooling" class="md-nav__link">
    Pooling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1x1-convolutions" class="md-nav__link">
    1x1 convolutions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#skip-connections" class="md-nav__link">
    Skip connections
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#convolution" class="md-nav__link">
    Convolution
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-convolution" class="md-nav__link">
    Why Convolution?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#padding-and-strides" class="md-nav__link">
    Padding and strides
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#channels" class="md-nav__link">
    Channels
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-layer" class="md-nav__link">
    Convolutional layer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#convolutional-network" class="md-nav__link">
    Convolutional network
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#pooling" class="md-nav__link">
    Pooling
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#1x1-convolutions" class="md-nav__link">
    1x1 convolutions
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#skip-connections" class="md-nav__link">
    Skip connections
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>
<p>Convolutional Neural Networks are one of the most powerful types of neural network, very popular and successful in image processing (and more broadly computer vision). They are based on a simple mathematical operation that we, geoscientists, know very well and user in a variety of tasks: the <em>convolution</em> operator. This is motivated in most scenarios where local dependencies in the input data are known to be predominant. </p>
<p>Imagine for example a geological model, or a core section. If we decide to apply Deep Learning to such data to either classify
rock types, estimate rock parameters, or even for generative modelling tasks, the first thing that we would like our NN to know is that nearby geological features are likely to be correlated, whilst the further apart we move the more the features become
independent from each other. By looking at the schematic diagrams below, a FCN would not take this prior information into account
as each input value is linearly combined to give rise to the output. On the other hand, a convolutional block which represents
the key component of a CNN will only use values of the input vector in a certain neighbour to obtain the output:</p>
<p><img alt="CNN" src="../figs/cnn.png" /></p>
<p>The example mentioned above is just one of many in geoscience where convolution-based networks have been lately shown to be very 
successfull. Other examples are:</p>
<ul>
<li><em>Seismic interpretation</em> (faults, horizons, bodies)</li>
<li><em>Seismic processing</em> (denoising, interpolation, deblurring)</li>
<li><em>Satellite imaginery</em> (denoising, segmentation)</li>
<li><em>Microseismicity</em> (detection, source mechanism)</li>
<li><em>Laboratory studies</em> (CT, SEM, Microscopy for various processing and interpretation tasks)</li>
</ul>
<p>In general, any data type that is represented regularly on a 1D, 2D, or ND gridded topology is fit for CNNs.</p>
<h2 id="convolution">Convolution</h2>
<p>First of all, let's briefly recall what a convolution is. This represents in fact the core operation performed by a convolutional layer.</p>
<p>A convolution between two signals can be mathematically written as</p>
<div class="arithmatex">\[
y(t) = \int x(\tau) h(t-\tau) d\tau \leftrightarrow y = x * h
\]</div>
<p>where <span class="arithmatex">\(x(t)\)</span> and <span class="arithmatex">\(y(t)\)</span> are the input and output, respectively, and <span class="arithmatex">\(h(t)\)</span> is the filter (also called <em>kernel</em> in the DL jargon).
This equation can be interpreted as follows: take the filter and flip it across the origin, then slide it along the time axis and multiply-and-sum it to the input signal.</p>
<p>In practice, when working with digital data in a computer, all signals are discrete and the continuous formula above can be rewritten as follows:</p>
<div class="arithmatex">\[
y_i = \sum_{j=-\infty}^{\infty} x_j h_{i-j}
\]</div>
<p>where, to be general, we have here extended the integral from <span class="arithmatex">\(-\infty\)</span> to <span class="arithmatex">\(\infty\)</span>. In most applications, the filter <span class="arithmatex">\(h\)</span> is however compact (it has a small size of N samples, also called <em>kernel size</em>) and therefore we can limit the summation within the window of samples where the filter is non-zero.</p>
<p>A similar (but still different!) concept in signal processing is <em>correlation</em></p>
<div class="arithmatex">\[
y(t) = \int x(\tau) h(t+\tau) d\tau \leftrightarrow y_i = \sum_{j=-\infty}^{\infty} x_j h_{i+j}
\]</div>
<p>where the filter is simply slid across the <span class="arithmatex">\(t\)</span> axis (without being initially flipped). The main difference between convolution and correlation is therefore that one delays the input signal whilst the other anticipates it when the filter is non-symmetric to zero. As we will see later, it is important to immediately empathize also a slight difference in the jargon used in classical signal processing and deep learning: what usually we refer to as convolution in DL is what signal processing refers to as correlation. However, since in DL we do not choose the filter <span class="arithmatex">\(h\)</span>, rather this is learned from data, if signal processing convolution was used instead of correlation, the learning algorithm would just learned the flipped version of the filter.</p>
<p>In both cases, when we convolve two signals of size <span class="arithmatex">\(N_x\)</span> with a filter of size <span class="arithmatex">\(N_h\)</span>, the output signal has size:</p>
<div class="arithmatex">\[
N_y = N_x + N_h - 1
\]</div>
<p>However, in the context of CNNs, we generally only want to consider the so-called <em>valid</em> part of the convolution, i.e., where the entire filter contributes to the computation. For this reason the output signal size becomes:</p>
<div class="arithmatex">\[
N_y = N_x - N_h + 1
\]</div>
<p>In the next section, we will see how we can actually make the choice of <span class="arithmatex">\(N_y\)</span> more flexible with the help of additional tools like padding and striding.</p>
<p>Extending the concept of convolution to two- and multi-dimensional data is straightforward. This can be done by simply
sliding the filter in all dimensions and can be mathematically written (in the discrete form) as follows:</p>
<div class="arithmatex">\[
y_{i,j} = \sum_m \sum_l x_{ij} h_{i+m,j+l}
\]</div>
<p>Finally, another interesting thing to notice is that convolution is a linear process. Therefore we can express it as a matrix-vector multiplication where the vector identifies the input data and the filter is re-organized into a Toeplitz matrix as show in the figure below</p>
<p><img alt="CONVMTX" src="../figs/convmtx.png" /></p>
<p>which means that the gradient of a convolutional operator that we need for backpropagation is just the adjoint of the matrix <span class="arithmatex">\(\mathbf{H}^T\)</span>. This is a convolution with the flipped kernel (so truly a convolution!).</p>
<h2 id="why-convolution">Why Convolution?</h2>
<p>A first intuitive motivation about locality of interactions, also referred to as <em>space interactions</em> 
(or <em>sparse connectivity</em> or <em>sparse weights</em>), has been already provided onto why convolution blocks may represent an appealing
alternative to fully connected blocks in the context of neural networks. However, this is not the only reason why convolution blocks are so powerful and widely used nowadays when training NNs for image processing tasks.</p>
<p>Let's start with an example. Imagine we are given a large image and a small 3x3 kernel. By sliding the kernel across the image we can still be able to detect useful local features (e.g., edges). Note that, the Machine Learning community has been aware of this for decades, and in fact many early approaches to image detection relied on hand-crafted filters that could highlight one feature of the input data over another. 
The modern DL approach simply takes this paradigm one step further where the filters are learned instead of being defined upfront. Experience has further shown that deep CNNs learn initially low level features (e.g., edges), then middle level features (e.g., shapes) and finally high level features (e.g., objects).</p>
<p><img alt="HANDLEARNED" src="../figs/hand_learn_filters.png" /></p>
<p>Compared to flattening the input data and applying a matrix that transforms it into the dimension of the output data (that is what a FCC would do as shown above), using convolutions with small filters can save both memory and computations. Given for example an image of size <span class="arithmatex">\(N_{w,x} \times N_{h,x}\)</span>, a fully connected layer that produces an output of the same size requires a matrix with <span class="arithmatex">\((N_{w,x} N_{h,x})^2\)</span> parameters and <span class="arithmatex">\((N_{w,x} N_{h,x})^2\)</span> computations are required to obtain
the output. On the other hand, if we now consider a simple filter of size <span class="arithmatex">\(N_{w,h} \times N_{h,h}\)</span>, the number of computations is reduced to <span class="arithmatex">\(N_{w,x} N_{h,x} N_{w,h} N_{h,h}\)</span>.</p>
<p>The second main advantage of convolutional blocks is so-called <em>parameter sharing</em>. The same learned kernels are applied all over the input data, instead of having one filter operating on all (or part of) the input data to produce a single output component.</p>
<p>Finally, a third benefit is the <em>equivariance of convolution to translation</em>. This means that if we shift the input by <span class="arithmatex">\(k\)</span> samples, the output will also be shifted by the same number of samples; however, the shape of the output will not change.</p>
<h2 id="padding-and-strides">Padding and strides</h2>
<p>We have previously seen how applying convolution to a signal with a kernel of a given size produces an output signal of different size, either with the total or valid output size is chosen. It may be however much easier when designing a convolutional neural network to have inputs and outputs of the same size, or more in general to be free to design the size of the output independent on that of the input and filter. Two simple approaches exist:</p>
<ul>
<li><em>padding</em>: the input signal is padded with zeros on both sides (for 1D signals) or all sides (for ND signals) prior to convolution. This allows producing outputs that can have the same size or even larger size than the input. Let's first look at this with an example when the output size is computed using the equation above for the valid case. We can devise a padding such that the size of the output stays the same as that of the input. This is actually easy to do once we choose the size of the filter and more specifically <span class="arithmatex">\(N_{x,pad} = N_x + 2*pad\)</span> with <span class="arithmatex">\(pad = (N_h-1)/2\)</span> when <span class="arithmatex">\(N_h\)</span> is a odd number and <span class="arithmatex">\(N_h/2\)</span> when <span class="arithmatex">\(N_h\)</span> is a even number.</li>
</ul>
<p>Moreover, apart from the obvious benefit of not having to handle outputs that keep reducing in size, padding ensures that edge values in the inputs are also used the same number of times that central values in the convolution process.</p>
<p><img alt="PADDING" src="../figs/padding.png" /></p>
<ul>
<li><em>strides</em>: a common approach when building convolutional neural network, as we will see when discussing popular CNN architecture, is however to gradually reduce the size of the signal (or image in 2D) whilst going deeper and deeper into the network. Two alternative ways to achieve this exist: the simplest is to couple convolutional layers that do not change the size of the input and downsampling (or pooling layers). Alternatively, one can choose to apply a special type of convolution called <em>strided convolution</em> that simply moves the filter around the input jumping (or striding) by more than a single sample at the time. Again, if we look at an example, we can observe how by doing so the size of the output is reduced by the striding factor. If we stride by a factor of two the output size will be half of the input size. As a result the output size can be written as <span class="arithmatex">\(N_y = \lfloor (N_x - N_h) / stride + 1 \rfloor\)</span>.</li>
</ul>
<p><img alt="STRIDING" src="../figs/striding.png" /></p>
<p>Eventually striding and padding can be used together to get for example an output that is exactly half of the size of the input in all directions. An important formula to remember when designing convolutional layers is:</p>
<div class="arithmatex">\[
N_y = \Bigl\lfloor \frac{N_x + 2pad - N_h}{stride} + 1 \Bigr\rfloor
\]</div>
<h2 id="channels">Channels</h2>
<p>We need to introduce one last key ingredient before we can define a convolutional layer. Let's imagine we have a 3D tensor and a 3D filter; the extension of 2D convolution to 3D (or any extra dimension) is as easy as sliding the filter along the third dimension as well as the first two. However, in deep learning we generally do something different when we are dealing with convolutional networks. We define a special dimension called <em>channel</em>.</p>
<p>Imagine having a 1D signal like a seismic trace but recording both the horizontal and vertical components of the particle displacement field. One way to arrange such data is as a 2D array where one of the dimensions is the size of the trace and the other is the number of components (or channels), here two. A similar scenario may arise for 2D signals if we record for example different spectral components or for pre-stack seismic data where we record data at different angles. Here once again we will have two "classical" dimensions, say latitude and longitude or geographical location and depth and one channel dimension. For the first example this will contain the different spectral components, for the second example it will be represented by the different angles (or offsets). This is the geoscientific equivalent to natural images that are commonly used in deep learning tasks where the channel contains different colors (e.g., RGB or CMYK). In order to make ourselves already familiar with the ordering used in computational frameworks like PyTorch, a batch of training samples is usually organized as follows:</p>
<div class="arithmatex">\[
N_x = (N_s \times N_{ch,x} \times N_{w,x} \times N_{h,x})
\]</div>
<p>where <span class="arithmatex">\(N_{ch,x}\)</span> is the number of input channels, whilst <span class="arithmatex">\(N_{w,x}\)</span> and <span class="arithmatex">\(N_{w,h}\)</span> are the width and the height of the image, respectively.</p>
<p>By defining a special dimension, we can now decide to still work with filters that slide only across the width and height axes. Such kernels will have size <span class="arithmatex">\(N_{ch,x} \times N_{w,h} \times N_{h,h}\)</span>. 
By doing so, for every step of convolution, the input and filter and multiplied and then all the values across all channels are summed together.</p>
<p><img alt="CHANNEL" src="../figs/channel.png" /></p>
<h2 id="convolutional-layer">Convolutional layer</h2>
<p>A convolutional layer is simply a stack of <span class="arithmatex">\(N_{ch,y}\)</span> filters. The resulting output has therefore a shape equal to:</p>
<div class="arithmatex">\[
N_y = (N_s \times N_{ch,y} \times N_{w,y} \times N_{h,y})
\]</div>
<p>where <span class="arithmatex">\(N_{w,y}\)</span> and <span class="arithmatex">\(N_{w,y}\)</span> can be computed upfront using the formulas derived above.</p>
<p><img alt="CONVLAYER" src="../figs/convlayer.png" /></p>
<p>Note that a convolutional layer contains trainable parameters both in the form of the coefficients of the various filters and
a vector of biases <span class="arithmatex">\(\mathbf{b}=[b_1, b_2,...,b_{N_{ch,y}}]\)</span> where every bias is applied to a different output channel. The output can be therefore written in a compact mathematical
form as follows:</p>
<div class="arithmatex">\[y = \sigma \Big( \begin{bmatrix} 
                h_1 * x + b_1 \\
                ...     \\
                h_{N_{ch,y}} * x + b_{N_{ch,y}} 
  \end{bmatrix} \Big)
\]</div>
<p>In summary, a convolutional layer has the following number of trainable parameters:</p>
<div class="arithmatex">\[
N_{clay}=N_{w,h}N_{h,h}N_{ch,x}N_{ch,y} + N_{ch,y}
\]</div>
<p>For example, if <span class="arithmatex">\(N_{ch,x}=3\)</span>, <span class="arithmatex">\(N_{ch,y}=10\)</span>, and the filters have size <span class="arithmatex">\(3 \times 3\)</span>, the overall number of parameters is <span class="arithmatex">\(3\cdot3\cdot3\cdot10 + 10 =280\)</span>. </p>
<p>Moreover, as convolutional layers can be stacked similarly to what we have done with MLP layers, the following nomenclature will be used in the following when
referring to a generic layer <span class="arithmatex">\(l\)</span>:</p>
<div class="arithmatex">\[
\begin{aligned}
x:&amp;\quad N_{ch}^{[l-1]} \times N_w^{[l-1]} \times N_h^{[l-1]},\\
h:&amp;\quad N_{ch}^{[l]} \times N_{ch}^{[l-1]} \times N_w^{[l-1]} \times N_h^{[l-1]},\\
b:&amp;\quad N_{ch}^{[l]},\\
y:&amp;\quad N_{ch}^{[l]} \times N_w^{[l]} \times N_h^{[l]}
\end{aligned}
\]</div>
<h2 id="convolutional-network">Convolutional network</h2>
<p>Similar to a fully connected network, a convolutional network can be easily created by putting together a certain number of convolutional layers.
Although we will see that different tasks call for different design choices, most convolutional neural networks share the following design features:</p>
<ul>
<li>the height and width (<span class="arithmatex">\(N_h\)</span> and <span class="arithmatex">\(N_w\)</span>) tends to reduce the deeper we travel into the network;</li>
<li>the number of channels (<span class="arithmatex">\(N_{ch}\)</span>) does instead increase as function of network depth;</li>
<li>after a certain number of convolutional layers, the output of size <span class="arithmatex">\(N_{ch}^{[l]} \times N_w^{[l]} \times N_h^{[l]}\)</span> is flattened and fed into one or more
  fully connected layers and then sent into a classifier (or regressor) loss function.</li>
</ul>
<p><img alt="CNN" src="../figs/cnn1.png" /></p>
<h2 id="pooling">Pooling</h2>
<p>As we have previously mentioned in the previous section, convolutional neural networks require reducing the size of the height and width of an input image 
We have already discussed that by choosing the filter size, stride and padding, the output can be either kept of the same size of the input or reduced (or increased) in size.</p>
<p>At times, it may however be better to avoid changing the size of the output directly as part of the convolution process, rather perform this in a separate step. In this section we introduce the
so-called <em>pooling</em> process, which is designed specifically to reduce the size of an input N-dimensional array by an arbitrary factor <span class="arithmatex">\(N_p\)</span>.</p>
<p>Let's start with an example. We are interested to take a matrix of size $N_{h,x} \times <span class="arithmatex">\(N_{w,x}\)</span> as input and produce an output of half the size (i.e., $N_{h,x}/2 \times <span class="arithmatex">\(N_{w,x}/2\)</span>.
A possible way to achieve this without purely discarding some of the values of the matrix is to select the maximum value within a sliding window of size <span class="arithmatex">\(2 \times 2\)</span> (stride=2):</p>
<p><img alt="MAXP" src="../figs/maxpooling.png" /></p>
<p>This approach is commonly referred to in the literature as <em>Max Pooling</em>. This approach can be easily extended to any other subsampling by simply extending the size of the window and stride
accordingly (i.e., using to the equations defined above used for the output sizes of a convolutional layer based on the filter size and stride). Moreover, even though less commonly used, <em>Mean Pooling</em> represent an alternative approach where the mean value inside each patch is taken 
instead of the maximum.</p>
<p>Finally, it is important to observe that Pooling is done for each channel independently and that it does not contain any learnable parameter.</p>
<h2 id="1x1-convolutions">1x1 convolutions</h2>
<p>At this point we know how to take an input tensor with an arbitrary number of dimensions (two or more) and a given number of channels, feed it 
through a convolutional layer, and obtain an output sensor with the same (or slightly different size) and a new chosen number of channels.</p>
<p>It is common practice when building convolutional neural networks to start with a small number of channels and increase it gradually as we go deeper
into the network. However, when you start stacking many of these layers the number of channels will quickly grow to a point where
<span class="arithmatex">\(N_{ch} \rightarrow \infty\)</span>. As a consequence of this fact, also the size of the filters start to grow indefinitely. But since having deeper networks
has been shown an effective way to learn very complex mappings, we need something to be able to reduce the size of these filters at any time
we are in need for it. A simple, yet very effective approach was proposed in 2013 by <a href="https://arxiv.org/abs/1312.4400">Lin and coauthors</a>
where filters of size <span class="arithmatex">\(1\times1\)</span> are used to reduce the number of channels whilst keeping the number of learnable parameter to a minimum 
(any other filter with bigger depth or width will introduce more learnable parameters). The authors actually refer to this <span class="arithmatex">\(1\times1\)</span> 
convolutional layer as a specific implementation of cross-channel parametric pooling, as similar to pooling reduces the size of the input tensor over
one dimensions (channel in this case).</p>
<h2 id="skip-connections">Skip connections</h2>
<p>As already extensively discussed in one of our previous lectures, one of the problem associated with making neural networks very deep is that of
so-called vanishing gradients. However, since deep neural networks are key to high performing models, the DL community has for long time tried to
come up with strategies that can speed up the training process (or at least avoid a slow down) in the presence of long stacks of convolutional blocks.</p>
<p>One successful idea that was proposed in 2015 by <a href="https://arxiv.org/abs/1512.03385v1">He and coauthors</a> under the name of <em>Residual Block</em>, where
so-called <em>skip connection</em> is introduced in a NN to take the activation of a certain layer and feed it directly to another layer further down in the computational graph.
In the figure below, we consider an example where a skip connection of 2 layers is introduced to connect the activations of layer <span class="arithmatex">\(l\)</span> and
<span class="arithmatex">\(l+2\)</span> (just before applying a nonlinear activation). Here the <em>connection</em> is achieved by summing the two activations.</p>
<p><img alt="SKIPCON" src="../figs/skipcon.png" /></p>
<p>Mathematically we can write:</p>
<div class="arithmatex">\[
\textbf{a}^{[l+2]}= \sigma(\textbf{a}^{[l]}+\textbf{z}^{[l+2]})
\]</div>
<p>and we can clearly see how the information contained in <span class="arithmatex">\(\textbf{a}^{[l]}\)</span> flows through the graph along both a longer path (i.e., main path)
and a shorter one (i.e., shortcut). Finally note that in the last 5 years or so many variations of the residual block have been introduced. For example,
one could have more or less than 2 convolutional layers (or MPLs) inside the main path. Moreover, since the size of <span class="arithmatex">\(\textbf{a}^{[l]}\)</span> and 
<span class="arithmatex">\(\textbf{z}^{[l+2]}\)</span> may be different, an additional layer with learnable parameter may be introduced as part of the shortcut to adjust for the size of 
<span class="arithmatex">\(\textbf{a}^{[l]}\)</span>:</p>
<div class="arithmatex">\[
\textbf{a}^{[l+2]}= \sigma(f_\theta(\textbf{a}^{[l]})+\textbf{z}^{[l+2]})
\]</div>
<p>where <span class="arithmatex">\(f_\theta\)</span> here could simply be a convolutional layer.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../09_mdn/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Uncertainty Quantification in Neural Networks and Mixture Density Networks" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Uncertainty Quantification in Neural Networks and Mixture Density Networks
            </div>
          </div>
        </a>
      
      
        
        <a href="../11_cnnarch/" class="md-footer__link md-footer__link--next" aria-label="Next: CNNs Popular Architectures" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              CNNs Popular Architectures
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>