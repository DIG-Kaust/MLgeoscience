
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.2.8">
    
    
      
        <title>Convolutional Neural Networks - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.cb6bc1d0.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.39b8e14a.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#convolutional-neural-networks" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      <div class="md-header-nav__ellipsis">
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            
              Convolutional Neural Networks
            
          </span>
        </div>
      </div>
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
      
      <label class="md-nav__link" for="nav-4">
        Lectures
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="nav-4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Convolutional Neural Networks
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Convolutional Neural Networks
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#convolution" class="md-nav__link">
    Convolution
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-convolution" class="md-nav__link">
    Why Convolution?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#padding-and-strides" class="md-nav__link">
    Padding and strides
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#channels" class="md-nav__link">
    Channels
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../11_vae/" class="md-nav__link">
        VAE
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../12_gan/" class="md-nav__link">
        GANs
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_autoencoder/" class="md-nav__link">
        Autoencoders
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_pca/" class="md-nav__link">
        PCA
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#convolution" class="md-nav__link">
    Convolution
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#why-convolution" class="md-nav__link">
    Why Convolution?
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#padding-and-strides" class="md-nav__link">
    Padding and strides
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#channels" class="md-nav__link">
    Channels
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="convolutional-neural-networks">Convolutional Neural Networks</h1>
<p>Convolutional Neural Networks are one of the most powerful types of neural network, very popular and successfull in image processing (and more broadly 
computer vision). They are based on a simple mathematical operationrthat we, geoscientists, know very well and user in a
variety of tasks: the <em>convolution</em> operator. This is motivated in most scenarios where local dependancies in the input data
are known to be predominant. </p>
<p>Imagine for example a geological model, or a core section. If we decide to apply Deep Learning to such data to either classify
rock types, estimate rock parameters, or even for generative modelling tasks, the first thing that we would like our NN to know is that nearby geological features are likely to be correlated, whilst the further apart we move the more the features become
indipendent from each other. By looking at the schematic diagrams below, a FCN would not take this prior information into account
as each input value is linearly combined to give rise to the output. On the other hand, a convolutional block which represents
the key component of a CNN will only use values of the input vector in a certain neighbour to obtain the output:</p>
<p><img alt="CNN" src="../figs/cnn.png" /></p>
<p>The example mentioned above is just one of many in geoscience where convolution-based networks have been lately shown to be very 
successfull. Other examples are:</p>
<ul>
<li><em>Seismic interpretation</em> (faults, horizons, bodies)</li>
<li><em>Seismic processing</em> (denoising, interpolation, deblurring)</li>
<li><em>Satellite imaginery</em> (denoising, segmentation)</li>
<li><em>Microseismicity</em> (detection, source mechanism)</li>
<li><em>Laboratory studies</em> (CT, SEM, Microscopy for various processing and interpretation tasks)</li>
</ul>
<p>In general, any data type that is represented regularly on a 1D, 2D, or ND gridded topology is fit for CNNs.</p>
<h2 id="convolution">Convolution</h2>
<p>First of all, let's briefly recall what a convolution is. This represents in fact the core operation performed by a convolutional layer.</p>
<p>A convolution between two signals can be mathematically written as</p>
<div class="arithmatex">\[
y(t) = \int x(\tau) h(t-\tau) d\tau \leftrightarrow y = x * h
\]</div>
<p>where <span class="arithmatex">\(x(t)\)</span> and <span class="arithmatex">\(y(t)\)</span> are the input and output, respectively, and <span class="arithmatex">\(h(t)\)</span> is the filter (also called <em>kernel</em> in the DL jargon).
This equation can be interpreted as follows: take the filter and flip it across the origin, then slide it along the time axis and multiply-and-sum it
to the input signal.</p>
<p>In practice, when working with digital data in a computer, all signals are discrete and the continous formula above can be rewritten
as follows:</p>
<div class="arithmatex">\[
y_i = \sum_{j=-\infty}^{\infty} x_j h_{i-j}
\]</div>
<p>where, to be general, we have here extended the integral from <span class="arithmatex">\(-\infty\)</span> to <span class="arithmatex">\(\infty\)</span>. In most applications, the filter <span class="arithmatex">\(h\)</span> is however
compact (it has a small size of N samples, also called <em>kernel size</em>) and therefore we can limit the summation within the window of samples
where the filter is non-zero.</p>
<p>A similar (but still different!) concept in signal processing is <em>correlation</em></p>
<div class="arithmatex">\[
y(t) = \int x(\tau) h(t+\tau) d\tau \leftrightarrow y_i = \sum_{j=-\infty}^{\infty} x_j h_{i+j}
\]</div>
<p>where the filter is simply slided across the <span class="arithmatex">\(t\)</span> axis (without being initially flipped). The main difference between convolution and correlation
is therefore that one delays the input signal whilst the other anticipates it when the filter is non-symmetric to zero. As we will see later, 
it is important to immediately empathise also a slight difference in the jargon used in classical signal processing
and deep learning: what usually we refer to as convolution in DL is what signal processing refers to as correlation. However, since in DL we do not choose
the filter <span class="arithmatex">\(h\)</span>, rather this is learned from data, if signal processing convolution was used instead of correlation, the learning algorithm would
just learned the flipped version of the filter.</p>
<p>In both cases, when we convolve two signals of size <span class="arithmatex">\(N_x\)</span> with a filter of size <span class="arithmatex">\(N_h\)</span>, the output signal has size:</p>
<div class="arithmatex">\[
N_y = N_x + N_h - 1
\]</div>
<p>However, in the context of CNNs, we generally only want to consider the so-called <em>valid</em> part of the convolution, i.e., where the entire filter contributes
to the computation. For this reason the output signal size becomes:</p>
<div class="arithmatex">\[
N_y = N_x - N_h + 1
\]</div>
<p>In the next section, we will see how we can actually make the choice of <span class="arithmatex">\(N_y\)</span> more flexible with the help of additional tools like padding and striding.</p>
<p>Extending the concept of convolution to two- and multi-dimensional data is straighforward. This can be done by simply
sliding the filter in all dimensions and can be mathematically written (in the discrete form) as follows:</p>
<div class="arithmatex">\[
y_{i,j} = \sum_m \sum_l x_{ij} h_{i+m,j+l}
\]</div>
<p>Finally, another interesting thing to notice is that convolution is a linear process. Therefore we can express it as a matrix-vector
multiplication where the vector identifies the input data and the filter is re-organized into a Toeplitz matrix as show in the figure below</p>
<p><img alt="CONVMTX" src="../figs/convmtx.png" /></p>
<p>which means that the gradient of a convolutional operator that we need for backpropagation is just the adjoint of the matrix <span class="arithmatex">\(\mathbf{H}^T\)</span>.
This is a convolution with the flipped kernel (so truly a convolution!).</p>
<h2 id="why-convolution">Why Convolution?</h2>
<p>A first intuitive motivation about locality of interactions, also referred to as <em>space interactions</em> 
(or <em>sparse connectivity</em> or <em>sparse weights</em>), has been already provided onto why convolution blocks may represent an appealing
alternative to fully connected blocks in the context of neural networks. However, this is not the only reason why convolution blocks
are so powerful and widely used nowadays when training NNs for image processing tasks.</p>
<p>Let's start with an example. Imagine we are given a large image and a small 3x3 kernel. By sliding the kernel across the image we can still
be able to detect useful local features (e.g., edges). Note that the Machine Learning community has been aware of this for decades, and 
in fact many early approaches to image detection relied on hand-crafted filters that could highlight one feature of the input data over another. 
The modern DL approach simply takes this paradigm one step further where the filters are learned instead of being defined upfront. Experience has
further shown that deep CNNs learn initially low level features (e.g., edges), then middle level features (e.g., shapes) and finally
high level features (e.g., objects).</p>
<p><img alt="HANDLEARNED" src="../figs/hand_learn_filters.png" /></p>
<p>Compared to flattening the input data and applying a matrix that transforms it into the dimension of the output data (that is what a FCC would do as shown above),
using convolutions with small filters can save both memory and computations. Given for example an image of size <span class="arithmatex">\(W_x \times H_x\)</span>, a fully connected
layer that produces an output of the same size requires a matrix with <span class="arithmatex">\((W_x H_x)^2\)</span> parameters and <span class="arithmatex">\((W_xH_x)^2\)</span> computations are required to obtain
the output. On the other hand, if we now consider a simple filter of size <span class="arithmatex">\(W_h \times H_h\)</span>, the number of computations is reduced to <span class="arithmatex">\(W_x H_x W_h H_h\)</span>.</p>
<p>The second main advantage of convolutional blocks is so-called <em>parameter sharing</em>. The same learned kernels are applied all over the input data, instead of having
one filter operating on all (or part of) the input data to produce a single output component.</p>
<p>Finally, a third benefit is the <em>equivariance of convolution to translation</em>. This means that if we shift the input by <span class="arithmatex">\(k\)</span> samples, the output
will also be shifted by the same number of samples; however, the shape of the output will not change.</p>
<h2 id="padding-and-strides">Padding and strides</h2>
<p>We have previously seen how applying convolution to a signal with a kernel of a given size produces an output signal of different size, either with the total
or valid output size is chosen. It may be however much easier when designing a convolutional neural network to have inputs and outputs of the same size, or more in
general to be free to design the size of the output indipendent on that of the input and filter. Two simple approaches exist:</p>
<ul>
<li><em>padding</em>: the input signal is padded with zeros on both sides (for 1D signals) or all sides (for ND signals) prior to convolution. This allows producing outputs that
  can have the same size or even larger size than the input. Let's first look at this with an example when the output size is computed using the equation above for the
  valid case. We can devise a padding such that the size of the output stays the same as that of the input. This is actually easy to do once we choose the size of
  the filter and more specifically <span class="arithmatex">\(N_{x,pad} = N_x + 2*pad\)</span> with <span class="arithmatex">\(pad = (N_h-1)/2\)</span> when <span class="arithmatex">\(N_h\)</span> is a odd number and <span class="arithmatex">\(N_h/2\)</span> when <span class="arithmatex">\(N_h\)</span> is a even number.</li>
</ul>
<p>Moreover, apart from the obvious benefit of not having to handle outputs that keep reducing in size, padding ensures that edge values in the inputs are also used
  the same number of times that central values in the convolution process.</p>
<p><img alt="PADDING" src="../figs/padding.png" /></p>
<ul>
<li><em>strides</em>: a common approach when building convolutional neural network, as we will see when discussing popular CNN architecture, is however to gradually reduce the size of
  the signal (or image in 2D) whilst going deeper and deeper into the network. Two alternative ways to achieve this exist: the simplest is to couple convolutional layers that do
  not change the size of the input and downsampling (or pooling layers). Alternatively, one can choose to apply a special type of convolution called <em>strided convolution</em>
  that simply moves the filter around the input jumping (or striding) by more than a single sample at the time. Again, if we look at an example, we can observe
  how by doing so the size of the output is reduced by the striding factor. If we stride by a factor of two the output size will be half of the input size. As a result
  the output size can be written as <span class="arithmatex">\(N_y = \lfloor (N_x - N_h) / stride + 1 \rfloor\)</span>.</li>
</ul>
<p><img alt="STRIDING" src="../figs/striding.png" /></p>
<p>Eventually striding and padding can be used together to get for example an output that is exactly half of the size of the input in all directions. An important formula
to remember when designing convolutional layers is:</p>
<div class="arithmatex">\[
N_y = \Bigl\lfloor \frac{N_x + 2pad - N_h}{stride} + 1 \Bigr\rfloor
\]</div>
<h2 id="channels">Channels</h2>
<p>We need to introduce one last key ingredient before we can define a convolutional layer. Let's imagine we have a 3D tensor and a 3D filter; the extension of 2D
convolution to 3D (or any extra dimension) is as easy as sliding the filter along the third dimension as well as the first two. However, in deep learning we generally do something 
different when we are dealing with convolutional networks. We define a special dimension called <em>channel</em>.</p>
<p>Imagine having a 1D signal like a seismic trace but recording both the horizontal and vertical components of the particle displacement field. One way to arrange
such data is as a 2D array where one of the dimensions is the size of the trace and one is the number of components (or channels), here two. A similar scenario
may arise for 2D signals if we record for example different spectral components or for pre-stack seismic data where we record data at different angles. Here once again
we will have two <code>classical</code> dimensions, say latitude and longitude or geographical location and depth and one channel dimension. For the first example this will contain
the different spectral components, for the second example it will be represented by the different angles (or offsets). This is the geoscientific equivalent to natural images that
are commonly used in deep learning tasks where the channel contains different colors (e.g., RGB or CMYK). In order to make ourselves already familiar with the
ordering used in computational frameworks like PyTorch, a batch of training samples is usually organized as follows:</p>
<div class="arithmatex">\[
N_x = (N_s \times N_{ch} \times N_w \times N_h)
\]</div>
<p>where <span class="arithmatex">\(N_{ch}\)</span> is the number of channels, whilst <span class="arithmatex">\(N_w\)</span> and <span class="arithmatex">\(N_h\)</span> are the width and the height of the image, respectively.</p>
<p>By defining a special dimension, we can now decide to still work with filters that slide only across the width and height axes. Such kernels will have size
<span class="arithmatex">\(N_{ch} \times H_w \times H_h\)</span>. By doing so, for every step of convolution, the input and filter and multiplied and then all the values across all channels summed together.</p>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../09_mdn/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Uncertainty Quantification in Neural Networks and Mixture Density Networks
              </div>
            </div>
          </a>
        
        
          <a href="../11_vae/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                VAE
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.18f0862e.min.js"></script>
      <script src="../../assets/javascripts/bundle.994580cf.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.9c0e82ba.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>