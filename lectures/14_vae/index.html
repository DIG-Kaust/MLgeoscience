
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Generative Modelling and Variational AutoEncoders - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generative-modelling-and-variational-autoencoders" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generative Modelling and Variational AutoEncoders
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_4" type="checkbox" id="__nav_4" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_4">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Generative Modelling and Variational AutoEncoders
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Generative Modelling and Variational AutoEncoders
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#variational-autoencoders-vaes" class="md-nav__link">
    Variational AutoEncoders (VAEs)
  </a>
  
    <nav class="md-nav" aria-label="Variational AutoEncoders (VAEs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reparametrization-trick" class="md-nav__link">
    Reparametrization trick
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-vaes" class="md-nav__link">
    Why VAEs?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization-in-vaes" class="md-nav__link">
    Regularization in VAEs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematics-of-vaes" class="md-nav__link">
    Mathematics of VAEs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_INN/" class="md-nav__link">
        Invertible Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../19_implicit/" class="md-nav__link">
        Implicit neural networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#variational-autoencoders-vaes" class="md-nav__link">
    Variational AutoEncoders (VAEs)
  </a>
  
    <nav class="md-nav" aria-label="Variational AutoEncoders (VAEs)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#reparametrization-trick" class="md-nav__link">
    Reparametrization trick
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#why-vaes" class="md-nav__link">
    Why VAEs?
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#regularization-in-vaes" class="md-nav__link">
    Regularization in VAEs
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mathematics-of-vaes" class="md-nav__link">
    Mathematics of VAEs
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="generative-modelling-and-variational-autoencoders">Generative Modelling and Variational AutoEncoders</h1>
<p>Up until now, our attention has been mostly focused on supervised learning tasks where we have access to a certain number
of training samples, in the form of input-target pairs, and we train a model (e.g., a NN) to learn the best possible mapping
between the two. These kind of models are also usually referred to as <em>discriminative models</em> as they learn from training samples
their underlying conditional probability distribution <span class="arithmatex">\(p(\mathbf{y}|\mathbf{x})\)</span>.</p>
<p>In the last lecture, we have also seen how the general principles of supervised learning can be adapted to accomplish a
number of different tasks where input-target pairs are not available. Dimensionality reduction is one of such tasks, which are 
usually categorized under the umbrella of unsupervised learning.</p>
<p>Another very exciting area of statistics that has been recently heavily influenced by the deep learning revolution is the
so-called field of <em>generative modelling</em>. Here, instead of having access to input-target pairs, we are able to only gather
a (large) number of samples <span class="arithmatex">\(\mathbf{X} = \{ \mathbf{x}^{(1)}, \mathbf{x}^{(2)}, ..., \mathbf{x}^{(N_s)} \}\)</span>
that we believe come from a given hidden distribution. The task that we wish to accomplished is  therefore:</p>
<ul>
<li>Learn the underlying distribution <span class="arithmatex">\(p(\mathbf{x})\)</span>, or </li>
<li>Learn to sample from the underlying distribution <span class="arithmatex">\(\tilde{\mathbf{x}} \sim p(\mathbf{x})\)</span></li>
</ul>
<p>Obviously, the first task is more general and usually more ambitious. Once you know a distribution, sampling from it is rather an
easy task. In the next two lectures, we will however mostly focused on the second task and discuss two popular algorithms that
have shown impressive capabilities to sample from high-dimensional, complex distributions.</p>
<p>To set the scene, let's take the simplest approach to generative modelling that has nothing to do with neural networks. Let's imagine 
we are provided with <span class="arithmatex">\(N_s\)</span> multi-dimensional arrays and we are told that they come from a multi-variate gaussian distribution. We can 
set up a generative modelling task as follows:</p>
<ul>
<li>
<p>Training</p>
<ul>
<li>Compute the sample mean and covariance from the training samples: <span class="arithmatex">\(\boldsymbol \mu, \boldsymbol \Sigma\)</span></li>
<li>Apply the Cholesky decomposition to the covariance matrix: <span class="arithmatex">\(\boldsymbol \Sigma = \mathbf{L} \mathbf{L}^T\)</span></li>
</ul>
</li>
<li>
<p>Inference / Generation</p>
<ul>
<li>Sample a vector from a unitary, zero-mean normal distribution <span class="arithmatex">\(\mathbf{z} \sim \mathcal{N}(\mathbf{0}, \mathbf{I})\)</span></li>
<li>Create a new sample from the true distribution: <span class="arithmatex">\(\tilde{\mathbf{x}} = \mathbf{L} \mathbf{z} + \boldsymbol \mu\)</span></li>
</ul>
</li>
</ul>
<p>Unfortunately, multi-dimensional distributions that we usually find in nature are hardly gaussian and this kind of simple
generative modelling procedure falls short. Nevertheless, the approach that we take with some of the more advanced generative modelling
methods that we are going to discuss later on in this lecture does not differ from what we have done so far. A training phase, where the 
free-parameters of the chosen parametric model (e.g., a NN) are learned from the available data, followed by a generation phase that uses
the trained model and some stochastic input (like the <span class="arithmatex">\(\mathbf{z}\)</span> vector in the example above).</p>
<h2 id="variational-autoencoders-vaes">Variational AutoEncoders (VAEs)</h2>
<p>Variational AutoEncoders have been proposed by <a href="https://arxiv.org/abs/1312.6114">Kingma and Welling</a> in 2013. in As the name implies, these networks take inspiration from the AutoEncoder networks that we have presented in the previous lecture. However, some 
small, yet fundamental changes are implemented to the network architecture as well as the learning process (i.e., loss function) to turn such family
of networks from being able to perform dimensionality reduction to being generative models. </p>
<p>Let's start by looking at a schematic representation of a VAEs:</p>
<p><img alt="VAE" src="../figs/vae.png" /></p>
<p>Even before we delve into the mathematical details, we can clearly see that one main change has been implemented to the network architecture:
instead of directly producing a vector <span class="arithmatex">\(\mathbf{z} \in \mathbb{R}^{N_l}\)</span>, the encoder's output is composed of two vectors 
<span class="arithmatex">\(\boldsymbol \mu \in \mathbb{R}^{N_l}\)</span> and <span class="arithmatex">\(\boldsymbol \sigma \in \mathbb{R}^{N_l}\)</span> that represent the mean and standard deviation of a <span class="arithmatex">\(N_l\)</span> dimensional
gaussian distribution (with uncorrelated variables, i.e., diagonal covariance matrix). Mathematically, the encoder can be written as 
<span class="arithmatex">\(\boldsymbol \mu = e_{\theta,\mu}(\mathbf{x}), \; \boldsymbol \sigma = e_{\theta,\sigma}(\mathbf{x})\)</span>, where the two networks share all weights apart from
the last layer. The two vectors produced by the encoder are then fed together to a sampler,
who similar to what we did before, produces a sample from the following gaussian distribution: <span class="arithmatex">\(\mathcal{N}(\boldsymbol \mu, diag\{ \boldsymbol \sigma \})\)</span>.
In practice this is however achieved by sampling a vector and then transforming it into the desired distribution, 
<span class="arithmatex">\(\mathbf{z} = \boldsymbol \sigma \cdot \mathbf{z} + \boldsymbol \mu\)</span> where <span class="arithmatex">\(\cdot\)</span> refers to an element-wise product. </p>
<h3 id="reparametrization-trick">Reparametrization trick</h3>
<p>This rather simple trick is referred
to as <em>Reparametrization trick</em> and it is strictly needed in neural networks every time we want to introduce a stochastic process within the computational graph.
In fact, by simply having a stochastic process parametrized by a certain mean and standard deviation that may come from a previous part of the computational graph
(as in VAEs) we lose the possibility to perform backpropagation. Instead if we decouple the stochastic component (which we are not interested to update, and 
therefore to backpropagate onto) and the deterministic component(s), we do not lose access to backpropagation:</p>
<p><img alt="REPARAMETRIZATIONTRICK" src="../figs/reptrick.png" /></p>
<h3 id="why-vaes">Why VAEs?</h3>
<p>Before we progress in discussing the loss function and training procedure of VAEs, a rather simple question may arise: 'Why can we not use AEs for
generative modelling?'</p>
<p>In fact, this could be achieved by simply modifying the inference step:</p>
<p><img alt="GENAE" src="../figs/generativeae.png" /></p>
<p>where instead of taking a precomputed <span class="arithmatex">\(\mathbf{z}\)</span> vector (from a previous stage of compression), we could sample a new <span class="arithmatex">\(\mathbf{z}\)</span> 
value from a properly crafted distribution (perhaps chosen from statistical analysis of the training latent vectors) at any time we want 
to create a new sample.</p>
<p>Unfortunately, whilst this idea may sound reasonable, we will be soon faced with a problem. In fact, the latent manifold learned by a AE may
not be regular, or in other words it may be hard to ensure that areas of such manifold that have not been properly sampled by the training data will
produce meaningful samples <span class="arithmatex">\(\tilde{\mathbf{z}}\)</span>. Just to give an idea, let's look at the following schematic representation:</p>
<p><img alt="LATENTAE" src="../figs/latentspaceae.png" /></p>
<p>as we can see, if a part of the latent 1-d manifold is not rich in training data, the resulting generated sample may be non-representative at all.
Whilst we discussed techniques that can mitigate this form of overfitting (e.g., sparse AEs), VAEs bring the learning process to a whole new level
by choosing a more appropriate regularization term <span class="arithmatex">\(R(\mathbf{x}^{(i)} ;\theta,\phi)\)</span> to add to the reconstruction loss.</p>
<h3 id="regularization-in-vaes">Regularization in VAEs</h3>
<p>In order to better understand the regularization choice in VAEs, let's look once again at a schematic representation of VAEs but this time in a
probabilistic mindset:</p>
<p><img alt="VAEPROB" src="../figs/vaeprob.png" /></p>
<p>where we highlight here the fact that the encoder and decoder can be seen as probability approximators. More specifically:</p>
<ul>
<li><span class="arithmatex">\(e_\theta(\mathbf{x}) \approx p(\mathbf{z}|\mathbf{x})\)</span>: the encoder learns to sample from the latent space distribution conditioned on a specific input</li>
<li><span class="arithmatex">\(d_\phi(\mathbf{z}) \approx p(\mathbf{x}|\mathbf{z})\)</span>: the decoder learns to sample from the true distribution conditioned on a specific latent sample</li>
</ul>
<p>By doing so, we can reinterpret the reconstruction loss as the negative log-likelihood of the decoder. And, provided that we have defined a 
prior for the latent space <span class="arithmatex">\(\mathbf{z} \sim P(\mathbf{z})\)</span>, we can learn the parameters of the decoder by ensuring that the posterior does not deviate
too much from the prior. This can be achieved by choosing:</p>
<div class="arithmatex">\[
R(\mathbf{x} ;\theta,\phi) = KL(p(\mathbf{z}|\mathbf{x})||p(\mathbf{z}))
\]</div>
<p>As in any statistical learning process, the overall loss of our VAEs shows a trade-off between the likelihood (i.e., learning from data) and 
prior (i.e., keeping close to the initial guess). </p>
<p>Before we provide a mathematical derivation supporting these claims, let's briefly try to provide some intuition onto why adding this regularization
makes VAEs more well behaved than AEs in terms of generating representation samples of the input distribution (<span class="arithmatex">\(p(\mathbf{x})\)</span>) whilst sampling directly
in the latent space. Back to the example with geometrical shapes, if we now assume a 2-dimensional latent space for both an AE and a VAEs:</p>
<p><img alt="VAELATENT" src="../figs/vaelatent.png" /></p>
<p>the effect of the regularization term in VAEs is such that the probability density function of the latent space ((<span class="arithmatex">\(p(\mathbf{z}|\mathbf{x})\)</span>) is forced
to stay close to the prior, and therefore the "clouds" of different classes do not really separate from each other abruptly. As a consequence, the 
geometrical shapes associated with the transition zones in the latent space are still meaningful. The same cannot be said for the AE as the
"clouds" of different classes tend to move apart leaving unexplored regions between them. Sampling from such region will result in non-representative
geometrical shapes.</p>
<p>More precisely, the regularization term in VAEs ensures the following two properties for the latent space:</p>
<ul>
<li>continuity: two closely points in the latent space are similar in the original space;</li>
<li>completness: any point sampled from the latent distribution is meaningful in the original space;</li>
</ul>
<h3 id="mathematics-of-vaes">Mathematics of VAEs</h3>
<p>To conclude our lecture on VAEs, we would like to gain a stronger mathematical understanding about the inner working of this model. In order to do
so, we are required to introduce a technique commonly used in statistics to estimate complex distributions. 
This technique goes under the name of <em>Variational Inference (VI)</em>. </p>
<p>Let's begin from the classical setup of Bayesian inference. We are interested in a certain probability distribution that we want to sample from 
or characterize (e.g., in terms of its mean and standard deviation), for example the following posterior distribution in a general inverse problem setting:</p>
<div class="arithmatex">\[
p(\mathbf{x} | \mathbf{y}) = \frac{p(\mathbf{y}|\mathbf{x}) p(\mathbf{x})}{p(\mathbf{y})}
\]</div>
<p>where <span class="arithmatex">\(\mathbf{x}\)</span> is the model we wish to estimate and <span class="arithmatex">\(\mathbf{y}\)</span> are the available observations. We assume knowledge of the prior distribution <span class="arithmatex">\(p(\mathbf{x})\)</span> and the underlying
physical process that links the model to the data, <span class="arithmatex">\(\mathbf{y}=f(\mathbf{x})\)</span> from which we can compute the likelihood <span class="arithmatex">\(p(\mathbf{y}|\mathbf{x})\)</span> (assuming a certain statistics for the noise).
The denominator of the Bayes rule (<span class="arithmatex">\(p(\mathbf{y}) = \int p(\mathbf{y}|\mathbf{x}) p(\mathbf{x}) d\mathbf{x}\)</span>) is what prevents us from computing the posterior directly.</p>
<p>Variational Inference approaches the above problem in a special way. A parametric distribution <span class="arithmatex">\(q_\theta(\mathbf{x})\)</span> is defined, also sometimes referred to as
guide or proposal distribution, and an optimization problem is set up to find the best free-parameters <span class="arithmatex">\(\theta\)</span> such that this easy-to-evaluate distribution
closely resembles to posterior distribution of interest. As usual when setting up an optimization problem, a measure of distance between such distributions is required to be able to optimize 
for such set of parameters. In this case, since we are dealing with distributions, it comes natural to choose the Kullback-Leibler divergence as metric:</p>
<div class="arithmatex">\[
\underset{\theta} {\mathrm{argmin}} \; KL(q_\theta(\mathbf{x})||p(\mathbf{x}|\mathbf{y}))
\]</div>
<p>Let's now expand the expression of the KL divergence and show an equivalent formula for this optimization problem:</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\underset{\theta} {\mathrm{argmin}} \; KL(q_\theta(\mathbf{x})||p(\mathbf{x}|\mathbf{y})) \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; E_{\mathbf{x} \sim q_\theta} \left[ log \left( \frac{q_\theta(\mathbf{x})}{p(\mathbf{x}|\mathbf{y})} \right) \right] \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; E_{\mathbf{x} \sim q_\theta} [ log q_\theta(\mathbf{x}) ] - E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{x}|\mathbf{y}) ] \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; E_{\mathbf{x} \sim q_\theta} [ log q_\theta(\mathbf{x}) ] - E_{\mathbf{x} \sim q_\theta} \left[ log \left( \frac{p(y|\mathbf{x})p(\mathbf{x})}{p(\mathbf{y})} \right) \right] \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; E_{\mathbf{x} \sim q_\theta} [ log q_\theta(\mathbf{x}) ] - E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{y}|\mathbf{x}) ] - E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{x}) ] + \cancel{E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{y}) ]} \\
&amp;= \underset{\theta} {\mathrm{argmin}} \; KL(q_\theta(\mathbf{x})||p(\mathbf{x})) - E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{y}|\mathbf{x}) ]
\end{aligned}
\]</div>
<p>where we have eliminated <span class="arithmatex">\(E_{x \sim q_\theta} [ p(\mathbf{y}) ]\)</span> the in the 5th row since it does not depend on <span class="arithmatex">\(\theta\)</span>. In the last row, we can see the two 
terms that we have previously described:</p>
<ul>
<li><span class="arithmatex">\(-E_{x \sim q_\theta} [ p(\mathbf{y}|\mathbf{x}) ]\)</span> is the negative log-likelihood of a traditional Maximum likelihood estimation (i.e., data misfit term). In the
  special case of gaussian noise (<span class="arithmatex">\(\mathbf{y} \sim \mathcal{N}(f(\mathbf{x}), \sigma^2 \mathbf{I})\)</span>), this becomes the MSE loss as discussed in one 
  of our previous lectures;</li>
<li><span class="arithmatex">\(KL(q_\theta(\mathbf{x})||p(\mathbf{x}))\)</span> is the regularization term encouraging the proposal distribution to stay close to the prior.</li>
</ul>
<p>Finally, let's slightly rearrange the expression in the 5th row:</p>
<div class="arithmatex">\[
E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{y}) ] - KL(q_\theta(\mathbf{x})||p(\mathbf{x}|\mathbf{y})) = 
E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{y}|\mathbf{x}) ] - KL(q_\theta(\mathbf{x})||p(\mathbf{x})) 
\]</div>
<p>The left hand side of this equation is called <em>Evidence Lower Bound (ELBO)</em>. The names comes from the fact that the sum of these two terms is
always <span class="arithmatex">\(\le E_{\mathbf{x} \sim q_\theta} [ log p(\mathbf{y}) ]\)</span> since KL divergence is always positive. Therefore, by maximizing the right hand side (or
equivalently by minimizing the negative of the right hand side), we effectively maximize the lower bound of the probability of the evidence <span class="arithmatex">\(p(\mathbf{y})\)</span>.
Variational inference can be therefore seen also as a maximization problem over the ELBO.</p>
<p>Whilst we now understand the theoretical foundations of VI, to make it practical we need to specify:</p>
<ul>
<li>
<p>A suitable proposal <span class="arithmatex">\(q_\theta(\mathbf{x})\)</span>, where <em>suitable</em> means that we can easily evaluate such probability, its KL divergence with a prior of choice,
  as well as sample from it. The simplest choice that is sometimes made in VI is named <em>mean-field approximation</em> where:
    $$
    q_\theta(\mathbf{x}) = \prod_i q_\theta(x_i) \sim \mathcal{N}(\boldsymbol \mu , diag(\boldsymbol \sigma))
    $$
  where <span class="arithmatex">\(\theta={\boldsymbol \mu, \boldsymbol \sigma}\)</span>. This implies that there is no correlation over the different variables of the N-dimensional 
  proposal distribution. Whilst this choice may be too simple in many practical scenarios, it is important to notice that this is not the same as 
  assuming that the variables of the posterior itself are uncorrelated!</p>
</li>
<li>
<p>A suitable optimizer. In the case where multiple \mathbf{x} samples are available, <span class="arithmatex">\(p(\mathbf{y}|\mathbf{x}\)</span>, <span class="arithmatex">\(p(\mathbf{x}\)</span>, and 
  <span class="arithmatex">\(q_\theta(\mathbf{x})\)</span> are differentiable we can simply use a stochastic gradient method. This special case of VI is named ADVI.</p>
</li>
</ul>
<p>Moving back to where we started, the VAE model. Let's now rewrite the problem as a VI estimation (where <span class="arithmatex">\(\mathbf{z}\)</span> plays here the role of the
model or unseen variable and <span class="arithmatex">\(\mathbf{x}\)</span> represents the available observations):</p>
<div class="arithmatex">\[
\begin{aligned}
&amp;\underset{\theta, \phi} {\mathrm{argmin}} \; KL(q_\theta(\mathbf{z})||p(\mathbf{z}|\mathbf{x})) \\
&amp;= \underset{\theta, \phi} {\mathrm{argmin}} \; KL(q_\theta(\mathbf{z})||p(\mathbf{z})) - E_{\mathbf{z} \sim q_\theta} [ log p_\phi(\mathbf{x}|\mathbf{z}) ]
\end{aligned}
\]</div>
<p>where the first term is responsible for updating the encoder whilst the second term contributes to the update of both the encoder and decoder. 
The proposal distribution is here parametrized as <span class="arithmatex">\(q_\theta(\mathbf{z}) \sim \mathcal{N}(e_{\theta,\mu}(\mathbf{x}), diag(e_{\theta,\sigma}(\mathbf{x})))\)</span>.
and the expectation is taken over the training samples (or a batch of them).</p>
<h2 id="additional-readings">Additional readings</h2>
<ul>
<li>A great resource to learn more about the basics of probabilistic (or generative) modelling can be found <a href="https://jmtomczak.github.io/blog/19/19_mog_pcs.html">here</a>.</li>
<li>The flow of this lecture is heavily inspired by this <a href="https://towardsdatascience.com/understanding-variational-autoencoders-vaes-f70510919f73">blog post</a></li>
<li>A Python library that can help you step up your game with Variational Inference is <a href="https://pyro.ai">Pyro</a> from Uber.</li>
</ul>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../13_dimred/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Dimensionality reduction" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Dimensionality reduction
            </div>
          </div>
        </a>
      
      
        
        <a href="../15_gans/" class="md-footer__link md-footer__link--next" aria-label="Next: Generative Adversarial Networks (GANs)" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Generative Adversarial Networks (GANs)
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4Z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>