
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-6.2.8">
    
    
      
        <title>Gradient-based optimization - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.cb6bc1d0.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.39b8e14a.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>body,input{font-family:"Roboto",-apple-system,BlinkMacSystemFont,Helvetica,Arial,sans-serif}code,kbd,pre{font-family:"Roboto Mono",SFMono-Regular,Consolas,Menlo,monospace}</style>
      
    
    
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#gradient-based-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header-nav md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header-nav__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header-nav__title" data-md-component="header-title">
      <div class="md-header-nav__ellipsis">
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header-nav__topic">
          <span class="md-ellipsis">
            
              Gradient-based optimization
            
          </span>
        </div>
      </div>
    </div>
    
      <label class="md-header-nav__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0116 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 019.5 16 6.5 6.5 0 013 9.5 6.5 6.5 0 019.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" data-md-component="search-reset" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 003-3 3 3 0 00-3-3 3 3 0 00-3 3 3 3 0 003 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="nav-4" type="checkbox" id="nav-4" checked>
      
      <label class="md-nav__link" for="nav-4">
        Lectures
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="nav-4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Gradient-based optimization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Gradient-based optimization
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-algorithms" class="md-nav__link">
    Gradient-descent algorithms
  </a>
  
    <nav class="md-nav" aria-label="Gradient-descent algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-length-selection" class="md-nav__link">
    Step length selection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#second-order-optimization" class="md-nav__link">
    Second-order optimization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stochastic-gradient-descent-sgd" class="md-nav__link">
    Stochastic-gradient descent (SGD)
  </a>
  
    <nav class="md-nav" aria-label="Stochastic-gradient descent (SGD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batched-gradient-descent" class="md-nav__link">
    Batched gradient descent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-gradient-descent" class="md-nav__link">
    Stochastic gradient descent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mini-batched-gradient-descent" class="md-nav__link">
    Mini-batched gradient descent
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        Convolutional Neural Network Architectures
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../11_vae/" class="md-nav__link">
        VAE
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../12_gan/" class="md-nav__link">
        GANs
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_autoencoder/" class="md-nav__link">
        Autoencoders
      </a>
    </li>
  

          
            
  
  
  
    <li class="md-nav__item">
      <a href="../4_pca/" class="md-nav__link">
        PCA
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#gradient-descent-algorithms" class="md-nav__link">
    Gradient-descent algorithms
  </a>
  
    <nav class="md-nav" aria-label="Gradient-descent algorithms">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#step-length-selection" class="md-nav__link">
    Step length selection
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#second-order-optimization" class="md-nav__link">
    Second-order optimization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#stochastic-gradient-descent-sgd" class="md-nav__link">
    Stochastic-gradient descent (SGD)
  </a>
  
    <nav class="md-nav" aria-label="Stochastic-gradient descent (SGD)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#batched-gradient-descent" class="md-nav__link">
    Batched gradient descent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#stochastic-gradient-descent" class="md-nav__link">
    Stochastic gradient descent
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#mini-batched-gradient-descent" class="md-nav__link">
    Mini-batched gradient descent
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#additional-readings" class="md-nav__link">
    Additional readings
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="gradient-based-optimization">Gradient-based optimization</h1>
<p>After reviewing some of the basic concepts of linear algebra and probability that we will be using during this course, 
we are now in a position to start our journey in the field of <em>learning algorithms</em>. </p>
<p>Any learning algorithm, no matter its level of complexity, is composed of 4 key elements:</p>
<p><strong>Dataset</strong>: a collection of many examples (sometimes referred to as samples of data points) that represents the experience
we wish our machine learning algorithm to learn from. More speficically, the dataset is defined as:
$$
\mathbf{x} = [x_1, x_2, ..., x_{N_f}]^T \quad \mathbf{X} = [\mathbf{x}^{(1)}, \mathbf{x}^{(2)}, ..., \mathbf{x}^{(N_s)}] 
$$
and
$$
\mathbf{y} = [y_1, y_2, ..., y_{N_t}]^T \quad \mathbf{Y} = [\mathbf{y}^{(1)}, \mathbf{y}^{(2)}, ..., \mathbf{y}^{(N_s)}] 
$$
where <span class="arithmatex">\(N_f\)</span> and <span class="arithmatex">\(N_t\)</span> are the number of features and targets for each sample in the dataset, respectively, and 
<span class="arithmatex">\(N_s\)</span> is the number of samples.</p>
<p><strong>Model</strong>: a mathematical relation between the input (or features) and output (or target) 
of our dataset. It is generally parametrized as function <span class="arithmatex">\(f\)</span> of a number of free parameters <span class="arithmatex">\(\theta\)</span> which we want the 
learning algorithm to estimate given a task and a measure of performance, and we write it as 
$$
\mathbf{y} = f_\theta(\mathbf{x})
$$</p>
<p><strong>Loss (and cost) function</strong>: quantitative measure of the performance of the learning algorithm, which we wish to minimize 
(or maximize) in order to make accurate predictions on the unseen data. It is written as
$$
J_\theta = \frac{1}{N_s} \sum_{j=1}^{N_s} \mathscr{L} (\mathbf{y}^{(j)}, f_\theta(\mathbf{x}^{(j)}))
$$</p>
<p>where <span class="arithmatex">\(\mathscr{L}\)</span> is the loss function for each input-output pair and <span class="arithmatex">\(J\)</span> is the overall cost function.</p>
<p><strong>Optimization algorithm</strong>: mathematical method that aims to drive down (up) the cost function by modifying
its free-parameters <span class="arithmatex">\(\theta\)</span>:
$$
\hat{\theta} = \underset{\theta} {\mathrm{argmin}} \; J_\theta
$$</p>
<p>Optimization algorithms are generally divided into two main families: gradient-based 
(or local) and gradient-free (or global). Gradient-based optimization is by far the most popular way to train NNs and 
will be discussed in more details below.</p>
<h2 id="gradient-descent-algorithms">Gradient-descent algorithms</h2>
<p>The simplest of gradient-based methods is the so-called <strong>Gradient-descent</strong> algorithms (e.g., steepest descent algorithm). As the name implies, this algorithm uses local gradient information of the functional to minimize/maximize to move towards its global 
mimimum/maximum as depicted in the figure below.</p>
<p><img alt="GRADIENT OPTIMIZATION" src="../figs/opt_gradient.png" /></p>
<p>More formally, given a functional <span class="arithmatex">\(J_\theta\)</span> and its gradient 
<span class="arithmatex">\(\nabla J = \frac{\delta J}{\delta \theta}\)</span>, the (minimization) algorithm can be written as:</p>
<p>Initialization: choose <span class="arithmatex">\(\theta \in \mathbb{R}\)</span></p>
<p>For <span class="arithmatex">\(i=0,...N-1\)</span>;</p>
<ol>
<li>Compute update direction <span class="arithmatex">\(d_i = -\nabla J |_{\theta_i}\)</span></li>
<li>Estimate step-lenght <span class="arithmatex">\(\alpha_i\)</span></li>
<li>Update <span class="arithmatex">\(\theta_{i+1} = \theta_{i} + \alpha_i d_i\)</span></li>
</ol>
<p>Note that the maximization version of this algorithm simply swaps the sign in the update direction (first equation of the algorithm).
Moreover, the proposed algorithm can be easily extended to N-dimensional model vectors <span class="arithmatex">\(\theta=[\theta_1, \theta_2, ..., \theta_N]\)</span> by
defining the following gradient vector 
<span class="arithmatex">\(\nabla J=[\delta J / \delta\theta_1, \delta J / \delta\theta_2, ..., \delta J/ \delta\theta_N]\)</span>.</p>
<h3 id="step-length-selection">Step length selection</h3>
<p>The choice of the step-length has tremendous impact on the performance of the algorithm and its ability to converge 
fast (i.e., in a small number of iterations) to the optimal solution.</p>
<p>The most used selection rules are:</p>
<ul>
<li>Constant: the step size is fixed to a constant value <span class="arithmatex">\(\alpha_i=\hat{\alpha}\)</span>. This is the most common situation that we
  will encounter when training neural networks. In practice, some adaptive schemes based on the evolution of the train
  (or validation) norm are generally adopted, but we will still refer to this case as costant step size;</li>
<li>Exact line search: at each iteration, <span class="arithmatex">\(\alpha_i\)</span> is chosen such that it minimizes <span class="arithmatex">\(J(\theta_{i} + \alpha_i d_i)\)</span>. This
  is the most commonly used approach when dealing with linear systems of equations.</li>
<li>Backtracking  "Armijo" line search: at each iteration, given a parameter <span class="arithmatex">\(\mu \in (0,1)\)</span>, start with <span class="arithmatex">\(\alpha_i=1\)</span> 
  and reduce it by a factor of 2 until the following condition is satisfied: <span class="arithmatex">\(J(\theta_i) - J(\theta_{i} + \alpha_i d_i) \ge  -\mu \alpha_i \nabla J^T d_i\)</span></li>
</ul>
<h2 id="second-order-optimization">Second-order optimization</h2>
<p>Up until now we have discussed first-order optimization techniques that rely on the ability to evaluate the function <span class="arithmatex">\(J\)</span> and 
its gradient <span class="arithmatex">\(\nabla J\)</span>. Second-order optimization method go one step beyond in that they use information from both the 
local slope and curvature of the function <span class="arithmatex">\(J\)</span>. </p>
<p>When a function has small curvature, the function and its tangent line are very similar: 
the gradient alone is therefore able to provide a good local approximation of the function (i.e., <span class="arithmatex">\(J(\theta+\delta \theta)\approx J(\theta) + \nabla J \delta \theta\)</span>).
On the other hand, if the curvature of the function of large, the function and its tangent line start to differ very quickly away from the linearization point. The gradient alone is not able anymore to provide a good local approximation of the function 
(i.e., <span class="arithmatex">\(J(\theta+\delta \theta)\approx J(\theta) + \nabla J \delta \theta + \nabla^2 J \delta \theta^2\)</span>).</p>
<p>Let's start again from the one-dimensional case and the well-known <strong>Newton's method</strong>. This method is generally employed to find the zeros of a function:
<span class="arithmatex">\(\theta: J(\theta)=0\)</span> and can be written as:</p>
<div class="arithmatex">\[
\theta_{i+1} = \theta_i - \frac{J(\theta)|_{\theta_i}}{J'(\theta)|_{\theta_i}} 
\]</div>
<p>which can be easily derived from the Taylor expansion of <span class="arithmatex">\(f(\theta)\)</span> around <span class="arithmatex">\(\theta_{i+1}\)</span>.</p>
<p>If we remember that finding the minimum (or maximum) of a function is equivalent to find the zeros of its first derivative 
(<span class="arithmatex">\(\theta: min_\theta f(\theta) \leftrightarrow \theta: f'(\theta)=0\)</span>), the Netwon's method can be written as:</p>
<div class="arithmatex">\[
\theta_{i+1} = \theta_i - \frac{J'(\theta)|_{\theta_i}}{J''(\theta)|_{\theta_i}} 
\]</div>
<p>In order to be able to discuss second-order optimization algorithms for the multi-dimensional case, 
let's first introduce the notion of <em>Jacobian</em>:</p>
<div class="arithmatex">\[\mathbf{y} = J(\boldsymbol\theta) \rightarrow  \mathbf{J}  = \begin{bmatrix} 
                \frac{\partial J_1}{\partial \theta_1} &amp; \frac{\partial J_1}{\partial \theta_2} &amp; ... &amp; \frac{\partial J_1}{\partial \theta_M} \\
                ...     &amp; ...  &amp; ...   &amp; ... \\
                \frac{\partial J_N}{\partial \theta_1} &amp; \frac{\partial J_N}{\partial \theta_2} &amp; ... &amp; \frac{\partial J_N}{\partial \theta_M} \\
  \end{bmatrix} \in \mathbb{R}^{[N \times M]}
\]</div>
<p>Through the notion of Jacobian, we can define the <strong>Hessian</strong> as the Jacobian of the gradient vector</p>
<div class="arithmatex">\[\mathbf{H} = \nabla (\nabla J) = \begin{bmatrix} 
                \frac{\partial J^2}{\partial \theta_1^2} &amp; \frac{\partial J^2}{\partial x_1 \partial \theta_2} &amp; ... &amp; \frac{\partial J^2}{\partial \theta_1\partial \theta_M} \\
                ...     &amp; ...  &amp; ...   &amp; ... \\
                \frac{\partial J^2}{\partial \theta_M \partial \theta_1} &amp; \frac{\partial J^2}{\partial \theta_M \partial \theta_2} &amp; ... &amp; \frac{\partial J^2}{\partial \theta_M^2} \\
  \end{bmatrix} \in \mathbb{R}^{[M \times M]}
\]</div>
<p>where we note that when <span class="arithmatex">\(J\)</span> is continuous, <span class="arithmatex">\(\partial / \partial \theta_i \partial \theta_j = \partial / \partial \theta_j \partial \theta_i\)</span>, and <span class="arithmatex">\(\mathbf{H}\)</span>
is symmetric.</p>
<p>The Newton method for the multi-dimensional case becomes:</p>
<div class="arithmatex">\[
\boldsymbol\theta_{i+1} = \boldsymbol\theta_i - \mathbf{H}^{-1}\nabla J
\]</div>
<p>Approximated version of the Netwon method have been developed over the years, mostly based on the idea that inverting <span class="arithmatex">\(\mathbf{H}\)</span> is sometimes a prohibitive task. Such methods, generally referred to as Quasi-Netwon methods attempt to approximate the Hessian (or its inverse) using the collections of gradient information from the previous iterations. <a href="https://en.wikipedia.org/wiki/Broyden–Fletcher–Goldfarb–Shanno_algorithm">BFGS</a> or its limited memory version <a href="https://en.wikipedia.org/wiki/Limited-memory_BFGS">L-BFGS</a> are examples of such a kind. Due to their computational cost
(as well as the lack of solid theories for their use in conjunction with approximate gradients), these methods are not yet commonly used by the machine learning community to optimize the parameters of NNs in deep learning.</p>
<h2 id="stochastic-gradient-descent-sgd">Stochastic-gradient descent (SGD)</h2>
<p>To conclude, we look again at gradient-based iterative solvers and more specifically in the context of finite-sum functionals of the kind that we will encountering when training neural networks:</p>
<div class="arithmatex">\[
J_\theta = \frac{1}{N_s} \sum_{i=1}^{N_s} \mathscr{L} (\mathbf{y}^{(i)}, f_\theta(\mathbf{x}^{(i)}))
\]</div>
<p>where the summation here is performed over training data.</p>
<h3 id="batched-gradient-descent">Batched gradient descent</h3>
<p>The solvers that we have considered so far are generally update the model parameters <span class="arithmatex">\(\boldsymbol\theta\)</span> using the full gradient (i.e., over the entire batch of samples):</p>
<div class="arithmatex">\[
\boldsymbol\theta_{i+1} = \boldsymbol\theta_{i} - \alpha_i \nabla J = \boldsymbol\theta_{i} - \frac{\alpha_i}{N_s} \sum_{j=1}^{N_s} \nabla \mathscr{L}_j
\]</div>
<p>A limitation of such an approach is that, if we have a very large number of training samples, the computational cost of computing the full gradient is very high and when some of the samples are similar, their gradient contribution is somehow redundant.</p>
<h3 id="stochastic-gradient-descent">Stochastic gradient descent</h3>
<p>In this case we take a completely opposite approach to computing the gradient. More specifically, a single training sample is considered at each iteration:</p>
<div class="arithmatex">\[
\boldsymbol\theta_{i+1} = \boldsymbol\theta_{i} - \alpha_i \nabla \mathscr{L}_j
\]</div>
<p>The choice of the training sample <span class="arithmatex">\(j\)</span> at each iteration is generally completely random and this is repeated once all training data have been used at least once (generally referred to as <em>epoch</em>). In this case, the gradient may be noisy because the gradient of a single sample is a very rough approximation of the total cost function <span class="arithmatex">\(J\)</span>: such a high variance of gradients requires lowering the step-size <span class="arithmatex">\(\alpha\)</span> leading to slow convergence.</p>
<h3 id="mini-batched-gradient-descent">Mini-batched gradient descent</h3>
<p>A more commonly used strategy lies in between the batched and stochastic gradient descent algorithms 
uses batches of training samples to compute the gradient at each iteration. More spefically given a batch of 
<span class="arithmatex">\(N_b\)</span> samples, the update formula can be written as:</p>
<div class="arithmatex">\[
\boldsymbol\theta_{i+1} = \boldsymbol\theta_{i} - \frac{\alpha_i}{N_b} \sum_{j=1}^{N_b} \nabla \mathscr{L}_j
\]</div>
<p>and similarly to the stochastic gradient descent, the batches of data are chosen at random and this is repeated as soon as all 
data are used once in the training loop. Whilst the choice of the size of the batch depends on many factors 
(e.g., overall size of the dataset, variety of training samples), common batch sizes in training of NNs are from around 50 to 256 
(unless memory requirements kick in leading to even small batch sizes).</p>
<p><img alt="GD LOSSES" src="../figs/opt_losses.png" /></p>
<h2 id="additional-readings">Additional readings</h2>
<ul>
<li>the following <a href="https://ruder.io/optimizing-gradient-descent/">blog post</a> for a more
detailed overview of the optimization algorithms discussed here. Note that in one of our future <a href="lectures/08_gradopt1.md">lectures</a> 
we will also look again at the optimization algorithms and more specifically discuss strategies that allow overcoming some of the 
limitations of standard SGD in <a href="lectures/10_gradopt1.md">this lecture</a>.</li>
</ul>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <div class="md-footer-nav">
      <nav class="md-footer-nav__inner md-grid" aria-label="Footer">
        
          <a href="../02_prob/" class="md-footer-nav__link md-footer-nav__link--prev" rel="prev">
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
            </div>
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Previous
                </span>
                Probability refresher
              </div>
            </div>
          </a>
        
        
          <a href="../04_linreg/" class="md-footer-nav__link md-footer-nav__link--next" rel="next">
            <div class="md-footer-nav__title">
              <div class="md-ellipsis">
                <span class="md-footer-nav__direction">
                  Next
                </span>
                Linear and Logistic Regression
              </div>
            </div>
            <div class="md-footer-nav__button md-icon">
              <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
            </div>
          </a>
        
      </nav>
    </div>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    
      <script src="../../assets/javascripts/vendor.18f0862e.min.js"></script>
      <script src="../../assets/javascripts/bundle.994580cf.min.js"></script><script id="__lang" type="application/json">{"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}</script>
      
      <script>
        app = initialize({
          base: "../..",
          features: [],
          search: Object.assign({
            worker: "../../assets/javascripts/worker/search.9c0e82ba.min.js"
          }, typeof search !== "undefined" && search)
        })
      </script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>