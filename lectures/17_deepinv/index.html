
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.3.1, mkdocs-material-8.5.1">
    
    
      
        <title>Deep learning for Inverse Problems - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.2e8b5541.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.cbb835fc.min.css">
        
          
          
          <meta name="theme-color" content="#009485">
        
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#deep-learning-for-inverse-problems" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Deep learning for Inverse Problems
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../READMEcurvenotelocal/" class="md-nav__link">
        READMEcurvenotelocal
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_5" type="checkbox" id="__nav_5" checked>
      
      
      
      
        <label class="md-nav__link" for="__nav_5">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" aria-label="Lectures" data-md-level="1">
        <label class="md-nav__title" for="__nav_5">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_prob/" class="md-nav__link">
        Probability refresher
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Deep learning for Inverse Problems
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Deep learning for Inverse Problems
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#data-driven-or-learned-regularization-of-inverse-problems" class="md-nav__link">
    Data-driven or learned regularization of inverse problems
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learned-solvers" class="md-nav__link">
    Learned solvers
  </a>
  
    <nav class="md-nav" aria-label="Learned solvers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variants-of-learned-solvers" class="md-nav__link">
    Variants of learned solvers
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#data-driven-or-learned-regularization-of-inverse-problems" class="md-nav__link">
    Data-driven or learned regularization of inverse problems
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#learned-solvers" class="md-nav__link">
    Learned solvers
  </a>
  
    <nav class="md-nav" aria-label="Learned solvers">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#variants-of-learned-solvers" class="md-nav__link">
    Variants of learned solvers
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="deep-learning-for-inverse-problems">Deep learning for Inverse Problems</h1>
<p>The field of inverse problem has experienced a renaissance in the last decade thanks to the recent advances in Deep Learning.
Whilst solid theories exist for the solution of linear (or nonlinear) inverse problems, in practice one is always faced with
problems that are ill-posed by nature, i.e. many solutions exist that can match data equally well. This is where for 
long time the inverse problem community has spent time and resources to identify mitigating strategies to reduce the so-called
nullspace of an inverse problem by means of prior information. Similarly, for long time the optimization community has developed
iterative solvers that can provide solutions to convex or non-convex functionals by requiring only access to function and gradient
evaluations of the functional of interest. In this lecture we will discuss where and how Deep Learning may be of great help in the 
solution of inverse problems.</p>
<h2 id="data-driven-or-learned-regularization-of-inverse-problems">Data-driven or learned regularization of inverse problems</h2>
<p>To begin, let's consider the solution of an inverse problem of the form:</p>
<p><span class="arithmatex">\(\mathbf{d}^{obs}=g(\mathbf{m})\)</span></p>
<p>or </p>
<p><span class="arithmatex">\(\mathbf{d}^{obs} = \mathbf{Gm}\)</span></p>
<p>where <span class="arithmatex">\(g\)</span> or <span class="arithmatex">\(\mathbf{G}\)</span> is the known modelling operator, <span class="arithmatex">\(\mathbf{m}\)</span> are the unknown model parameters,
and <span class="arithmatex">\(\mathbf{d}^{obs}\)</span> are the observed data. As previously mentioned, in many (geo)scientific applications
the operator may be ill-posed and prior knowledge is required to obtain a plausible solution (not just one 
of the many that matches the data). In classical inverse problem theory this can be achieved as follows:</p>
<ul>
<li>Regularization: <span class="arithmatex">\(J = ||\mathbf{d}^{obs}-g(\mathbf{m})||_p^p + \lambda ||r(\mathbf{m})||_p^p\)</span>
  where <span class="arithmatex">\(r\)</span> is a function that tries to penalize some features of the model that we are not interested in. Classical
  choices of <span class="arithmatex">\(r\)</span> are linear operators such as the identity matrix (this type of regularization is called Tikhonov regularization
  and favours solution with small L2 norm - <span class="arithmatex">\(p=2\)</span>) or the second derivative of laplacian operator (this type of regularization 
  favour smooth solutions). Alternatively, one could choose a linear or nonlinear projection that transforms the model into a
  domain where the solution is sparse; by choosing <span class="arithmatex">\(p=1\)</span>, one can estimate the sparsest model that at the same time matches the data.</li>
<li>Preconditioning: <span class="arithmatex">\(J = ||\mathbf{d}^{obs}-g(p(\mathbf{z}))||_p^p + \lambda ||\mathbf{z}||_p^p\)</span>
  where by performing a change of variable (<span class="arithmatex">\(\mathbf{m}=p(\mathbf{z})\)</span>) the inverse problem is now solved in a transformed domain, and 
  <span class="arithmatex">\(p\)</span> is a function that filters the solution <span class="arithmatex">\(\mathbf{z}\)</span> in such a way that favourable features of the model are enhanced. As an example,
  a smoothing operator can be used to produce smooth solution (note how this differs from the previous approach where smooth solutions
  could be constructed by penalizing roughness in the solution by means of second derivatives). </li>
</ul>
<p>A common feature of these two families of approaches is that we as user are requested to select the regularizer or preconditioner for
the problem at hand. This could be a difficult task and usually requires a lot of trial-and-error before a good choice is made for a
specific problem. Alternatively, one could define a projection that reduces the dimensionality of the space in which we wish to find the solution (i.e., <span class="arithmatex">\(\mathbf{x} \in \mathbb{R}^{N_x}, 
\mathbf{z} \in \mathbb{R}^{N_z}\)</span> with <span class="arithmatex">\(N_z &lt;&lt; N_x\)</span>). This approach reminds us of the dimensionality reduction techniques discusses in this <a href="../13_dimred/">lecture</a>
and the choice of the method used to identify a representative latent space can be arbitrary (i.e., a simple linear transformation like PCA or a complex nonlinear
transformation like that induced by an Autoencoder or a GAN). A clear advantage of such an approach is that the user is not required to define
a transform upfront. Provided availability of training dataset in the form of a representative set of solutions <span class="arithmatex">\(M = (\mathbf{m}^{&lt;1&gt;}, 
\mathbf{m}^{&lt;2&gt;}, ..., \mathbf{m}^{&lt;N&gt;})\)</span>, the best data-driven transformation can be identified that suits the problem at hand. </p>
<p>Before we get more into the details of such an approach, it is important to make a few remarks. This approach lies in between
classical approaches in inverse problem theory and supervised learning approaches in that:</p>
<ul>
<li>classical inverse problems: only the modelling operator <span class="arithmatex">\(g/\mathbf{G}\)</span> and <em>one instance</em> of data <span class="arithmatex">\(\mathbf{d}^{obs}\)</span> are available. Prior information comes from our
  knowledge of the expected solution (or its probability distribution), but no set of solutions are available when solving the problem;</li>
<li>supervised learning: pairs of models and associated observations <span class="arithmatex">\((\mathbf{m}^{&lt;i&gt;}, \mathbf{d}^{obs,&lt;i&gt;})\)</span> are available upfront
  (or a set of models <span class="arithmatex">\((\mathbf{m}^{&lt;i&gt;}\)</span> from which the associated observations can be synthetically created via the modelling operator). A data-driven model (e.g., a NN)
  is then trained to find the mapping between data and models. Note that the modelling operator is not actively used in the training process;</li>
<li>learned regularization: a set of models <span class="arithmatex">\((\mathbf{m}^{&lt;i&gt;}\)</span> is available upfront, which are used to find a latent representation. The inverse problem is subsequently solved for <em>one instance</em> of data <span class="arithmatex">\(\mathbf{d}^{obs}\)</span>
  using the learned regularizer (or preconditioner) and the physical modelling operator.</li>
</ul>
<p>The key idea of solving inverse problems with learned regularizers is therefore to split the problem into two subsequent tasks, where the first is concerned with the prior
and the latter with the modelling operator (this is different from the supervised learning approach where the two are learned together):</p>
<ul>
<li>
<p>Learning process: a nonlinear model is trained to identify a representative latent space for the set of available solutions. Such model can be an AE (or VAE) network:</p>
<div class="arithmatex">\[
\underset{\mathbf{e}_\theta, \mathbf{d}_\phi} {\mathrm{argmin}} \; \frac{1}{N_s}\sum_i \mathscr{L}(\mathbf{m}^{(i)}, d_\phi(e_\theta(\mathbf{m}^{(i)}))) 
\]</div>
<p>or a GAN network</p>
<div class="arithmatex">\[
arg \; \underset{g_\theta} {\mathrm{min}} \; \underset{d_\phi} {\mathrm{max}} \; \frac{1}{N_s}\sum_i \mathscr{L}_{adv}(\mathbf{m}^{(i)})
\]</div>
</li>
<li>
<p>Inversion: Once the training process is finalized, the decoder (or generator) is used as a nonlinear preconditioner to the solution of the inverse problem as follows:</p>
<div class="arithmatex">\[
AE: \mathbf{m} = d_\phi(\mathbf{z}) = p(\mathbf{z}) \quad GAN: \mathbf{m} = g_\theta(\mathbf{z}) = p(\mathbf{z}) 
\]</div>
<p>such that the inverse problem becomes:</p>
<div class="arithmatex">\[
J = ||\mathbf{d}^{obs}-g(p(\mathbf{z}))||_p^p + \lambda ||\mathbf{z}||_p^p
\]</div>
<p>This problem can be now solved using a nonlinear solver of choice, where the gradient can be easily computed using the same set of tools that we employed in the
training process of neural networks, namely backpropagation:</p>
<div class="arithmatex">\[
\frac{\partial J}{\partial \mathbf{z}} = \frac{\partial J}{\partial g} \frac{\partial g}{\partial p} \frac{\partial p}{\partial \mathbf{z}}
\]</div>
<p>where <span class="arithmatex">\(\partial J / \partial g\)</span> is the derivative of the loss function over the predicted data, <span class="arithmatex">\(\partial g / \partial p\)</span> is the derivative of the physical modelling
operator, and <span class="arithmatex">\(\partial p / \partial \mathbf{z}\)</span> is the derivative of the decoder of the pretrained AE (or that of the generator of the pretrained GAN) over the input.</p>
</li>
</ul>
<p>Finally, it is worth noting that when an autoencoder is used to find a representative latent space, alternatively a regularized problem of this form can be solved:</p>
<div class="arithmatex">\[
J = ||\mathbf{d}^{obs}-g(\mathbf{m})||_p^p + \lambda ||\mathbf{m} - d_\phi(e_\theta(\mathbf{m})) ||_p^p
\]</div>
<p>where the regularization terms ensures that the autoencoder can recreate the estimated model. This ensures that the solution lies in the manifold of the set of plausible solutions used
to train the AE network.</p>
<h2 id="learned-solvers">Learned solvers</h2>
<p>In the previous section we have discussed the solution of linear (or nonlinear) inverse problems from a high-level perspective. In fact, we purposely decided to avoid any discussion
regarding the numerical aspects of solving any of the cost functions <span class="arithmatex">\(J\)</span>. In practice, real-life problems may target model spaces that contain millions (or even billions) of variables and the
same usually applies for the observation vector. Under these conditions, iterative solvers similar to those presented <a href="../03_gradopt/">here</a> and <a href="../08_gradopt1/">here</a> are
therefore the only viable option.</p>
<p>An iterative solver can be loosely expressed as a nonlinear function <span class="arithmatex">\(\mathcal{F}\)</span> of this form:</p>
<div class="arithmatex">\[
\hat{\mathbf{m}} = \mathcal{F}(\mathbf{d}^{obs}, \mathbf{m}_0, g/\mathbf{G})
\]</div>
<p>where <span class="arithmatex">\(\mathbf{m}_0\)</span> is an initial guess. The vanilla gradient descent algorithm can be more explicitly described by the following update rule:</p>
<div class="arithmatex">\[
\mathbf{m}_{i+1} = \mathbf{m}_i - \alpha \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_i} (\mathbf{d}^{obs}, \mathbf{m}, g/\mathbf{G})
\]</div>
<p>which we can <em>unroll</em> for a number of iterations and write as:</p>
<div class="arithmatex">\[
\mathbf{m}_{2} = \mathbf{m}_0 - \alpha \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_0} - \alpha \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_1}
\]</div>
<p>This expression clearly shows that the solution of an iterative solver at a given iteration is a simple weighted summation of the intermediate gradients that are subtracted from
the initial guess $\mathbf{m}_0 $. Similarly, more advanced solvers like the linear or nonlinear conjugate gradient algorithm take into account the past gradients at each iteration,
however they still apply simple linear scalings to the gradients to produce the final solution.</p>
<p>The mathematical community has recently started to investigate a new family of iterative solvers, called learned solvers. The key idea lies in the fact that a linear combination of gradients may not be the 
best choice (both in terms of convergence speed and ultimate quality of the solution). An alternative update rule of this form</p>
<div class="arithmatex">\[
\mathbf{m}_{i+1} = \mathbf{m}_i - f_\theta \left( \frac{\partial J}{\partial \mathbf{m}} | _ {\mathbf{m}=\mathbf{m}_i}\right)
\]</div>
<p>may represent a better choice. However, a question may arise at this point: how do we choose the nonlinear project <span class="arithmatex">\(f_\theta\)</span> that we are going to apply to the gradients at each step? 
Learned iterative solvers, as the name implies, learn this mapping. More specifically, assuming availability of pairs of models and associated observations <span class="arithmatex">\((\mathbf{m}^{&lt;i&gt;}, \mathbf{d}^{obs,&lt;i&gt;})\)</span>,
a supervised learning process is setup such that an iterative solver with <span class="arithmatex">\(N_it\)</span> iterations is tasked to learn the mapping from data to models. Let's take a look at the schematic below
to better understand how this works:</p>
<p><img alt="LEARNEDSOLV" src="../figs/learnedsolv.png" /></p>
<p>A learned iterative solver can be seen as an unrolled neural network where each element takes as input the current model estimate and its gradient and produces an updated version
of the model. To keep the model capacity low, each unit shares weights like in classical RNN and each update can be compactly written as:</p>
<div class="arithmatex">\[
\mathbf{m}_i = f_\theta(\mathbf{x}_i), \qquad \mathbf{x}_i = \mathbf{m}_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}}
\]</div>
<p>where <span class="arithmatex">\(\oplus\)</span> indicates concatenation over the channel axis (assuming that model and gradient are N-dimensional tensors). Depending on the problem and type of data
<span class="arithmatex">\(f_\theta\)</span> can be chosen to be any network architecture, from a simple FF block, to a stack of FF blocks, or even a convolutional neural network.
Moreover, given that we have access to the solution, the loss function is set up as follows:</p>
<div class="arithmatex">\[
\underset{f_\theta} {\mathrm{arg min}} \; \frac{1}{N_s}\sum_{i=1}^{N_s} \sum_{j=1}^{N_{it}} w_j \mathscr{L}(\mathbf{m}_j^{(i)}, \mathbf{m})
\]</div>
<p>where each estimate is compared to the true model. Since early iterations may be worse, an exponentially increasing weight may be used to downweight their contributions over the estimates
as later iterations of the unrolled solver. Finally, once the learning process is finalized, inference can be simply performed by evaluation a single forward pass of the network for <em>one instance</em> of data <span class="arithmatex">\(\mathbf{d}^{obs}\)</span> and 
a chosen initial guess.</p>
<p>To conclude, it is important to answer the following question: why learned solvers are better than pure vanilla supervised learning? </p>
<p>The key difference between these two approaches lies in how they decide to use the knowledge of the modelling operator <span class="arithmatex">\(g/\mathbf{G}\)</span>. Whilst traditional supervised
learning approaches may use the modelling operator in the process of generating training data whilst ignoring it during training, learned iterative solvers integrate the 
modelling operator in the learning process. Two benefits may arise from this choice: generalization of the trained network over unseen modelling operator and 
increased robustness to noise in the data.</p>
<h3 id="variants-of-learned-solvers">Variants of learned solvers</h3>
<p>The structure of the learned solver discussed above closely resembles the method proposed by <a href="https://iopscience.iop.org/article/10.1088/1361-6420/aa9581/meta">Adler and Öktem</a> in 2017. 
A number of variants have been suggested in the literature in the following years:</p>
<p><strong>Learned solver with memory</strong></p>
<p>Adler and Öktem further propose to include a memory variable <span class="arithmatex">\(\mathbf{s}\)</span>. This takes inspiration from conventional solvers that use past gradients (or memory) to obtain more informed update directions. </p>
<p><img alt="LEARNEDSOLV1" src="../figs/learnedsolv1.png" /></p>
<p>The model update can be therefore written as follows:</p>
<div class="arithmatex">\[
\mathbf{y}_i = f_\theta(\mathbf{x}_i), \qquad \mathbf{x}_i = \mathbf{m}_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}} \oplus \mathbf{s}_{i-1} 
\qquad \mathbf{y}_i = \mathbf{m}_i \oplus \mathbf{s}_i
\]</div>
<p><strong>Recurrent Inference Machines (RIMs)</strong></p>
<p>RIMs closely resemble the second learned solver of Adler and Öktem. They however differ in the design on the network block and the fact that similarly to RNNs two set of parameters
are used instead of one, <span class="arithmatex">\(f_\theta\)</span> and <span class="arithmatex">\(f'_\phi\)</span>.</p>
<p>The model update can be therefore written as follows:</p>
<div class="arithmatex">\[
\begin{aligned}
\mathbf{s}_i &amp;= f'_\phi (\mathbf{z}_i) , \qquad \mathbf{z}_i = \boldsymbol \eta_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}} \oplus \mathbf{s}_{i-1} \\
\boldsymbol \eta_i &amp;= \boldsymbol \eta_{i-1} + f_\theta(\mathbf{x}_i), \qquad \mathbf{x}_i = \boldsymbol \eta_{i-1} \oplus \frac{\partial J}{\partial \mathbf{m}} \oplus \mathbf{s}_i
\end{aligned}
\]</div>
<p>where a new variable <span class="arithmatex">\(\boldsymbol \eta\)</span> has been introduced. This is the unscaled output and is connected to the model via a nonlinear activation function <span class="arithmatex">\(\sigma\)</span> that is in change of
defining a range of allowed values: <span class="arithmatex">\(\mathbf{z} = \sigma ( \boldsymbol \eta)\)</span>.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
    
    <nav class="md-footer__inner md-grid" aria-label="Footer" >
      
        
        <a href="../16_pinns/" class="md-footer__link md-footer__link--prev" aria-label="Previous: Scientific Machine Learning and Physics-informed Neural Networks" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Scientific Machine Learning and Physics-informed Neural Networks
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.ecf98df9.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version.title": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.d691e9de.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>