
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../02_linalg/">
      
      
        <link rel="next" href="../03_gradopt/">
      
      <link rel="icon" href="../../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.4.2, mkdocs-material-9.1.4">
    
    
      
        <title>Probability refresher - ErSE 222 - Machine Learning in Geoscience</title>
      
    
    
      <link rel="stylesheet" href="../../assets/stylesheets/main.240905d7.min.css">
      
        
        <link rel="stylesheet" href="../../assets/stylesheets/palette.a0c5b2b5.min.css">
      
      

    
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../..",location),__md_hash=e=>[...e].reduce((e,_)=>(e<<5)-e+_.charCodeAt(0),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="teal" data-md-color-accent="teal">
  
    
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#probability-refresher" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-header__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2Z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            ErSE 222 - Machine Learning in Geoscience
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Probability refresher
            
          </span>
        </div>
      </div>
    </div>
    
    
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
      </label>
      <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5Z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12Z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41Z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../.." title="ErSE 222 - Machine Learning in Geoscience" class="md-nav__button md-logo" aria-label="ErSE 222 - Machine Learning in Geoscience" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54Z"/></svg>

    </a>
    ErSE 222 - Machine Learning in Geoscience
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../.." class="md-nav__link">
        Homepage
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../gradind/" class="md-nav__link">
        Grading system
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../../schedule/" class="md-nav__link">
        Schedule
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
      
      
      <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_4" checked>
      
      
      
        <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0">
          Lectures
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_4_label" aria-expanded="true">
        <label class="md-nav__title" for="__nav_4">
          <span class="md-nav__icon md-icon"></span>
          Lectures
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../01_intro/" class="md-nav__link">
        Introduction to Machine Learning
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../02_linalg/" class="md-nav__link">
        Linear Algebra refresher
      </a>
    </li>
  

            
          
            
              
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Probability refresher
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Probability refresher
      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probability" class="md-nav__link">
    Probability
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#information-theory" class="md-nav__link">
    Information theory
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../03_gradopt/" class="md-nav__link">
        Gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../04_linreg/" class="md-nav__link">
        Linear and Logistic Regression
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../05_nn/" class="md-nav__link">
        Basics of Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../06_nn/" class="md-nav__link">
        More on Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../07_bestpractice/" class="md-nav__link">
        Best practices in the training of Machine Learning models
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../08_gradopt1/" class="md-nav__link">
        More on gradient-based optimization
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../09_mdn/" class="md-nav__link">
        Uncertainty Quantification in Neural Networks and Mixture Density Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../10_cnn/" class="md-nav__link">
        Convolutional Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../11_cnnarch/" class="md-nav__link">
        CNNs Popular Architectures
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../12_seqmod/" class="md-nav__link">
        Sequence modelling
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../13_dimred/" class="md-nav__link">
        Dimensionality reduction
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../14_vae/" class="md-nav__link">
        Generative Modelling and Variational AutoEncoders
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../15_gans/" class="md-nav__link">
        Generative Adversarial Networks (GANs)
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../16_pinns/" class="md-nav__link">
        Scientific Machine Learning and Physics-informed Neural Networks
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../17_deepinv/" class="md-nav__link">
        Deep learning for Inverse Problems
      </a>
    </li>
  

            
          
            
              
  
  
  
    <li class="md-nav__item">
      <a href="../18_INN/" class="md-nav__link">
        Invertible Neural Networks
      </a>
    </li>
  

            
          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#probability" class="md-nav__link">
    Probability
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#information-theory" class="md-nav__link">
    Information theory
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  


<h1 id="probability-refresher">Probability refresher</h1>
<p>Another set of fundamental mathematical tools required to develop various machine learning algorithms 
(especially towards the end of the course when we will focus on generative modelling)</p>
<p>In order to develop various machine learning algorithms (especially towards the end of the 
course when we will focus on generative modelling) we need to be familiarized with some basic concepts of:
mathematical tools from:</p>
<ul>
<li><strong>Probability</strong>: mathematical framework to handle uncertain statements;</li>
<li><strong>Information Theory</strong>: scientific field focused on the quantification of amount of uncertainty in a probability distribution.</li>
</ul>
<h2 id="probability">Probability</h2>
<p><strong>Random Variable</strong>: a variable whose value is unknown, all we know is that it can take on different 
values with a given probability. It is generally defined by an uppercase letter <span class="arithmatex">\(X\)</span>, whilst the values 
it can take are in lowercase letter <span class="arithmatex">\(x\)</span>. (Note: Actually, random variable is not really a variable. To be exact, random variable is actually a function that maps from sample space to the probability space.)</p>
<p><strong>Probability distribution</strong>: description of how likely a variable <span class="arithmatex">\(x\)</span> is, <span class="arithmatex">\(P(x)\)</span> (or <span class="arithmatex">\(p(x)\)</span>). 
Depending on the type of variable we have:</p>
<ul>
<li>
<p><em>Discrete distributions</em>: <span class="arithmatex">\(P(X)\)</span> called Probability Mass Function (PMF) and <span class="arithmatex">\(X\)</span> can take on a discrete number of states N.
A classical example is represented by a coin where N=2 and <span class="arithmatex">\(X={0,1}\)</span>. For a fair coin, <span class="arithmatex">\(P(X=0)=0.5\)</span> and <span class="arithmatex">\(P(X=1)=0.5\)</span>.</p>
</li>
<li>
<p><em>Continuous distributions</em>: <span class="arithmatex">\(p(X)\)</span> called Probability Density Function (PDF) and <span class="arithmatex">\(X\)</span> can take on any value from a continuous space 
  (e.g., <span class="arithmatex">\(\mathbb{R}\)</span>). A classical example is represented by the gaussian distribution where <span class="arithmatex">\(x \in (-\infty, \infty)\)</span>.</p>
</li>
</ul>
<p>A probability distribution must satisfy the following conditions:</p>
<ul>
<li>
<p>each of the possible states must have probability bounded between 0 (no occurrance) and 1 (certainty of occurcence):
  <span class="arithmatex">\(\forall x \in X, \; 0 \leq P(x) \leq 1\)</span> (or <span class="arithmatex">\(p(x) \geq 0\)</span>, where the upper bound is removed because of the 
  fact that the integration step <span class="arithmatex">\(\delta x\)</span> in the second condition can be smaller than 1: <span class="arithmatex">\(p(X=x) \delta x &lt;=1\)</span>);</p>
</li>
<li>
<p>the sum of the probabilities of all possible states must equal to 1: <span class="arithmatex">\(\sum_x P(X=x)=1\)</span> (or <span class="arithmatex">\(\int p(X=x)dx=1\)</span>).</p>
</li>
</ul>
<p><strong>Joint and Marginal Probabilities</strong>: assuming we have a probability distribution acting over a set of variables (e.g., <span class="arithmatex">\(X\)</span> and <span class="arithmatex">\(Y\)</span>) 
we can define</p>
<ul>
<li>
<p><em>Joint distribution</em>: <span class="arithmatex">\(P(X=x, Y=y)\)</span> (or <span class="arithmatex">\(p(X=x, Y=y)\)</span>);</p>
</li>
<li>
<p><em>Marginal distribution</em>: <span class="arithmatex">\(P(X=x) = \sum_{y \in Y} P(X=x, Y=y)\)</span> (or <span class="arithmatex">\(p(X=x) = \int P(X=x, Y=y) dy\)</span>), 
  which is the probability spanning one or a subset of the original variables;</p>
</li>
</ul>
<p><strong>Conditional Probability</strong>: provides us with the probability of an event given the knowledge 
that another event has already occurred</p>
<div class="arithmatex">\[
P(Y=y | X=x) = \frac{P(X=x, Y=y)}{P(X=x)}
\]</div>
<p>This formula can be used recursively to define the joint probability of N variables as product of conditional
probabilities (so-called <em>Chain Rule of Probability</em>)</p>
<div class="arithmatex">\[
P(x_1, x_2, ..., x_N) = P(x_1) \prod_{i=2}^N P(x_i | x_1, x_2, x_{i-1})
\]</div>
<p><strong>Independence and Conditional Independence</strong>: Two variables X and Y are said to be independent if</p>
<div class="arithmatex">\[
P(X=x, Y=y) = P(X=x) P(Y=y)
\]</div>
<p>If both variables are conditioned on a third variable Z (i.e., P(X=x, Y=y | Z=z)), they are said to be conditionally
independent if</p>
<div class="arithmatex">\[
P(X=x, Y=y | Z=z) = P(X=x | Z=z) P(Y=y| Z=z)
\]</div>
<p><strong>Bayes Rule</strong>: probabilistic way to update our knowledge of a certain phenomenon (called prior) based on a new piece of evidence
(called likelihood):</p>
<div class="arithmatex">\[
P(x | y) = \frac{P(y|x) P(x)}{P(y)}
\]</div>
<p>where <span class="arithmatex">\(P(y) = \sum_x P(x, y) = \sum_x P(y |x) P(x)\)</span> is called the evidence. In practice, it is infeasible to compute this
quantity as it would require evaluating <span class="arithmatex">\(y\)</span> for all possible combination of <span class="arithmatex">\(x\)</span> (we will see later how it is possible to 
devise methods for which <span class="arithmatex">\(P(y)\)</span> can be ignored).</p>
<p><strong>Mean (or Expectation)</strong>: Given a function <span class="arithmatex">\(f(x)\)</span> where <span class="arithmatex">\(x\)</span> is a random variable with probability <span class="arithmatex">\(P(x)\)</span>, its average
or mean value is defined as follows for the discrete case:</p>
<div class="arithmatex">\[
\mu = E_{x \sim P} [f(x)] = \sum_x P(x) f(x)
\]</div>
<p>and for the continuous case</p>
<div class="arithmatex">\[
\mu = E_{x \sim p} [f(x)] = \int p(x) f(x) dx
\]</div>
<p>In most Machine Learning applications, we do not have knowledge of the full distribution to evaluate the mean, rather we 
have access to N equi-probable samples that we assume are drawn from the underlying distribution. We can approximate the mean
via the <em>Sample Mean</em>:</p>
<div class="arithmatex">\[
\mu \approx \sum_i \frac{1}{N} f(x_i)
\]</div>
<p><strong>Variance (and Covariance)</strong>: Given a function <span class="arithmatex">\(f(x)\)</span> where <span class="arithmatex">\(x\)</span> is a random variable with probability <span class="arithmatex">\(P(x)\)</span>,
it represents a measure of how much the values of the function vary from the mean:</p>
<div class="arithmatex">\[
\sigma^2 = E_{x \sim p} [(f(x)-\mu)^2]
\]</div>
<p>Covariance is the extension of the variance to two or more variables, and it tells 
how much these variables are related to each other:</p>
<div class="arithmatex">\[
Cov(f(x), g(y)) =  E_{x,y \sim p} [(f(x)-\mu_x)(f(y)-\mu_y)]
\]</div>
<p>Here, <span class="arithmatex">\(Cov \rightarrow 0\)</span> indicates no correlation between the variables, <span class="arithmatex">\(Cov &gt; 0\)</span> denotes positive correlation and
<span class="arithmatex">\(Cov &lt; 0\)</span> denotes negative correlation. It is worth remembering that covariance is linked to correlation via:</p>
<div class="arithmatex">\[
Corr_{x,y} =  \frac{Cov_{x,y}}{\sigma_x \sigma_y}
\]</div>
<p>Finally, the covariance of a multidimensional vector <span class="arithmatex">\(\textbf{x} \in \mathbb{R}^n\)</span> is defined as:</p>
<div class="arithmatex">\[
Cov_{i,j} = Cov(x_i, x_j), \qquad Cov_{i,i} = \sigma^2_i
\]</div>
<p><strong>Distributions</strong>: some of the most used probability distributions in Machine Learning are listed in the following.</p>
<p><em>1. Bernoulli</em>: single binary variable <span class="arithmatex">\(x \in \{0,1\}\)</span> (commonly used to describe the toss of a coin). It is defined as</p>
<div class="arithmatex">\[
P(x=1)=\phi, \; P(x=0)=1-\phi, \; \phi \in [0,1]
\]</div>
<p>with probability:</p>
<div class="arithmatex">\[
P(x)=\phi^x(1-\phi)^{1-x} = \phi x + (1-\phi)(1-x)
\]</div>
<p>and momentum equal to:</p>
<div class="arithmatex">\[
E[x] = 1, \; \sigma^2 = \phi (1-\phi)
\]</div>
<p><em>2. Multinoulli (or categorical)</em>: extension of Bernoulli distribution to K different states</p>
<div class="arithmatex">\[
\textbf{P} \in [0,1]^{K-1}; \; P_k = 1- \textbf{1}^T\textbf{P}, \; \textbf{1}^T\textbf{P} \leq 1
\]</div>
<p><em>3. Gaussian</em>: most popular choice for continuous random variables (most distributions are close to a normal distribution
  and the central limit theorem states that any sum of independent variables is approximately normal)</p>
<div class="arithmatex">\[
x \sim \mathcal{N}(\mu, \sigma^2) \rightarrow p(x) = \frac{1}{\sqrt{2 \pi} \sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} = \sqrt{\frac{\beta}{2 \pi}} e^{-\frac{\beta(x-\mu)^2}{2}}
\]</div>
<p>where the second definition uses the precision <span class="arithmatex">\(\beta=\frac{1}{\sigma^2} \in (0, \infty)\)</span> to avoid possible division by zero. 
A third way to parametrize the gaussian probability uses <span class="arithmatex">\(2 \delta = log \sigma^2 \in (-\infty, \infty)\)</span> which has the further
benefit to be unbounded and can be easily optimized for during training.
which is unbounded (compared to the variance that must be positive)</p>
<p><em>4. Multivariate Gaussian</em>: extension of Gaussian distribution to a multidimensional vector <span class="arithmatex">\(\textbf{x} \in \mathbb{R}^n\)</span></p>
<div class="arithmatex">\[
\textbf{x} \sim \mathcal{N}(\boldsymbol\mu, \boldsymbol\Sigma) \rightarrow p(\textbf{x}) = 
\sqrt{\frac{1}{(2 \pi)^n det \boldsymbol\Sigma}} e^{-\frac{1}{2}(\textbf{x}- \boldsymbol\mu)^T\boldsymbol\Sigma^{-1}(\textbf{x}- \boldsymbol\mu)}=
\sqrt{\frac{det \boldsymbol\beta}{(2 \pi)^n}} e^{-\frac{1}{2}(\textbf{x}- \boldsymbol\mu)^T\boldsymbol\beta(\textbf{x}- \boldsymbol\mu)}
\]</div>
<p>where again <span class="arithmatex">\(\boldsymbol\beta =\boldsymbol\Sigma^{-1}\)</span>. In ML applications, <span class="arithmatex">\(\boldsymbol\beta\)</span> is generally assumed diagonal
(mean-field approximation) or even isotropic ($\boldsymbol\beta = \beta \textbf{I}_n)</p>
<p><em>5. Mixture of distributions</em>: any smooth probability density function can be expressed as a weighted sum of simpler distributions</p>
<div class="arithmatex">\[
P(x) = \sum_i P(c=i) P(x | c=i)
\]</div>
<p>where <span class="arithmatex">\(c\)</span> is a categorical variable with Multinoulli distribution and plays the role of a <em>latent variable</em>, a variable that 
cannot be directly observed but is related to <span class="arithmatex">\(x\)</span> via the joint distribution:</p>
<div class="arithmatex">\[
P(x,c) = P(x | c) P(c), \; P(x) = \sum_c P(x|c)P(c)
\]</div>
<p>A special case is the so-called <em>Gaussian Mixture</em> where each probability <span class="arithmatex">\(P(x|c=i) \sim \mathcal{N}(\mu_i, \sigma_i^2)\)</span>.</p>
<h2 id="information-theory">Information theory</h2>
<p>In Machine Learning, we are sometimes interested to quantify how much information is contained in a signal or how much two
signals (or probability distributions) differ from each other.</p>
<p>A large body of literature exists in the context of telecommunications, where it is necessary to study how to transmit signals
for a discrete alphabet over a noisy channel. More specifically, a code must be designed so to allow sending the least amount
of bits for the most amount of useful information. Extension of such theory to continuous variables is also available and more 
commonly used in the context of ML systems.</p>
<p><strong>Self-information</strong>: a measure of information in such a way that likely events have low information content, less 
likely events have higher information content and independent events have additive information:</p>
<div class="arithmatex">\[
I(x) = - log_eP(x)
\]</div>
<p>such that for <span class="arithmatex">\(P(x) \rightarrow 0\)</span> (unlikely event), <span class="arithmatex">\(I \rightarrow \infty\)</span> and for <span class="arithmatex">\(P(x) \rightarrow 1\)</span> (likely event), <span class="arithmatex">\(I \rightarrow 0\)</span>.</p>
<p><strong>Shannon entropy</strong>: extension of self-information to continuous variables, representing the expected amount of information in an event <span class="arithmatex">\(x\)</span> drawn from a probability $P:</p>
<div class="arithmatex">\[
H(x) = E_{x \sim P} [I(x)] = - E_{x \sim P} [log_eP(x)]
\]</div>
<p><strong>Kullback-Leibler divergence</strong>: extension of entropy to 2 variables with probability <span class="arithmatex">\(P\)</span> and <span class="arithmatex">\(Q\)</span>, respectively. It is used to measure their distance</p>
<div class="arithmatex">\[
D_{KL}(P||Q)  = E_{x \sim P} [log\frac{P(x)}{Q(x)}] = 
E_{x \sim P} [logP(x)-logQ(x)]  = E_{x \sim P} [logP(x)] -E_{x \sim P}[logQ(x)] 
\]</div>
<p>which is <span class="arithmatex">\(D_{KL}(P||Q)=0\)</span> only when <span class="arithmatex">\(P=Q\)</span> and grows the further away the two probabilities are. 
Finally, note that this is not a real distance in that <span class="arithmatex">\(D_{KL}(P||Q) \neq D_{KL}(Q|| P)\)</span> (non-symmetric), therefore
the direction matter and it must be chosen wisely when devising optimization schemes with KL divergence in the loss function as
we will discuss in more details later.</p>





                
              </article>
            </div>
          
          
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    <script id="__config" type="application/json">{"base": "../..", "features": [], "search": "../../assets/javascripts/workers/search.208ed371.min.js", "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}}</script>
    
    
      <script src="../../assets/javascripts/bundle.19047be9.min.js"></script>
      
        <script src="../../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>